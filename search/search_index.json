{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Unlocking Cyclone: A Comprehensive Guide to HPC Workflows 1. Purpose of the Tutorial Series The \"Unlocking Cyclone: A Comprehensive Guide to HPC Workflows\" tutorial series is designed to equip users with the knowledge and skills necessary to access, navigate, and optimize the use of the Cyclone HPC system. Whether users are researchers running computational simulations, Python developers scaling their data analysis workflows, or scientists utilizing pre-installed software packages like GROMACS or OpenFOAM, this series provides practical guidance tailored to diverse needs. The tutorials respond to recurring challenges identified during training events and aim to demystify complex processes in HPC usage. By addressing fundamental concepts, workflow setups, and advanced optimization techniques, the series ensures users can effectively harness Cyclone\u2019s capabilities for their research and applications. 2. Goals Enhance Accessibility: Simplify access and navigation processes to help users familiarize themselves with Cyclone\u2019s architecture and tools. Promote Best Practices: Foster a culture of fair and efficient resource use, data management, and compliance with HPC policies. Build Proficiency: Develop user competency in job scheduling, software module management, and workflow optimization. Enable Scalability: Support users in scaling their applications using Cyclone\u2019s compute power, whether through parallel programming or package optimization. Facilitate Troubleshooting: Provide solutions to common issues, reducing barriers to productive HPC use. 3. Anticipated Outcomes By implementing the \"Unlocking Cyclone: A Comprehensive Guide to HPC Workflows\" series, the NCC aims to: Improve user confidence and competence in HPC usage. Enhance the productivity of Cyclone\u2019s user base. Establish Cyclone as a model of user-centric HPC systems, supporting researchers in tackling complex computational challenges. 4. Tutorials The tutorials are structured to guide users through various aspects of using the Cyclone system: Introduction to HPC Systems: This tutorial introduces Cyclone\u2019s architecture, including compute nodes, file systems, modules and the SLURM job scheduler. It sets the foundation for understanding the system\u2019s components and resource management, ensuring participants are prepared for more advanced sessions. Accessing and Navigating Cyclone: Users will learn to securely access Cyclone via SSH, transfer data using tools like scp and rsync, and navigate the file system with basic Linux commands. The focus is on practical skills for accessing and managing data across Cyclone\u2019s file systems. Setting Up and Using Development Tools: This session covers setting up tools like VS Code and MobaXTerm for remote development, managing software environments with modules, and extending functionality using tools like Conda or Python\u2019s venv. Scaling Python Workloads on HPC: This tutorial addresses how to run Python scripts on Cyclone using SLURM, focusing on scaling Python workflows using examples with PyTorch across CPUs and GPUs, and troubleshooting common issues. Compiling and Running C/C++ Code on Cyclone with SLURM: Participants will learn to compile and run C/C++ programs, covering serial, multi-threaded, GPU-accelerated, and distributed programs using SLURM. Interactive Computing on Cyclone with Jupyter Notebooks: This tutorial demonstrates how to set up and use Jupyter Notebooks on Cyclone\u2019s compute nodes, emphasizing resource allocation optimization, SSH tunneling, and best practices for interactive computing.","title":"Home"},{"location":"#unlocking-cyclone-a-comprehensive-guide-to-hpc-workflows","text":"","title":"Unlocking Cyclone: A Comprehensive Guide to HPC Workflows"},{"location":"#1-purpose-of-the-tutorial-series","text":"The \"Unlocking Cyclone: A Comprehensive Guide to HPC Workflows\" tutorial series is designed to equip users with the knowledge and skills necessary to access, navigate, and optimize the use of the Cyclone HPC system. Whether users are researchers running computational simulations, Python developers scaling their data analysis workflows, or scientists utilizing pre-installed software packages like GROMACS or OpenFOAM, this series provides practical guidance tailored to diverse needs. The tutorials respond to recurring challenges identified during training events and aim to demystify complex processes in HPC usage. By addressing fundamental concepts, workflow setups, and advanced optimization techniques, the series ensures users can effectively harness Cyclone\u2019s capabilities for their research and applications.","title":"1. Purpose of the Tutorial Series"},{"location":"#2-goals","text":"Enhance Accessibility: Simplify access and navigation processes to help users familiarize themselves with Cyclone\u2019s architecture and tools. Promote Best Practices: Foster a culture of fair and efficient resource use, data management, and compliance with HPC policies. Build Proficiency: Develop user competency in job scheduling, software module management, and workflow optimization. Enable Scalability: Support users in scaling their applications using Cyclone\u2019s compute power, whether through parallel programming or package optimization. Facilitate Troubleshooting: Provide solutions to common issues, reducing barriers to productive HPC use.","title":"2. Goals"},{"location":"#3-anticipated-outcomes","text":"By implementing the \"Unlocking Cyclone: A Comprehensive Guide to HPC Workflows\" series, the NCC aims to: Improve user confidence and competence in HPC usage. Enhance the productivity of Cyclone\u2019s user base. Establish Cyclone as a model of user-centric HPC systems, supporting researchers in tackling complex computational challenges.","title":"3. Anticipated Outcomes"},{"location":"#4-tutorials","text":"The tutorials are structured to guide users through various aspects of using the Cyclone system: Introduction to HPC Systems: This tutorial introduces Cyclone\u2019s architecture, including compute nodes, file systems, modules and the SLURM job scheduler. It sets the foundation for understanding the system\u2019s components and resource management, ensuring participants are prepared for more advanced sessions. Accessing and Navigating Cyclone: Users will learn to securely access Cyclone via SSH, transfer data using tools like scp and rsync, and navigate the file system with basic Linux commands. The focus is on practical skills for accessing and managing data across Cyclone\u2019s file systems. Setting Up and Using Development Tools: This session covers setting up tools like VS Code and MobaXTerm for remote development, managing software environments with modules, and extending functionality using tools like Conda or Python\u2019s venv. Scaling Python Workloads on HPC: This tutorial addresses how to run Python scripts on Cyclone using SLURM, focusing on scaling Python workflows using examples with PyTorch across CPUs and GPUs, and troubleshooting common issues. Compiling and Running C/C++ Code on Cyclone with SLURM: Participants will learn to compile and run C/C++ programs, covering serial, multi-threaded, GPU-accelerated, and distributed programs using SLURM. Interactive Computing on Cyclone with Jupyter Notebooks: This tutorial demonstrates how to set up and use Jupyter Notebooks on Cyclone\u2019s compute nodes, emphasizing resource allocation optimization, SSH tunneling, and best practices for interactive computing.","title":"4. Tutorials"},{"location":"about/","text":"EuroCC-2 project EuroCC 2 will work to identify and address the skills gaps in the European High Performance Computing (HPC) ecosystem and coordinate cooperation across Europe to ensure a consistent skills base. The role of EuroCC 2 is to establish and run a network of more than 30 NCCs across the EuroHPC Participating States. The NCCs act as single points of access in each country between stakeholders and national and EuroHPC systems. They operate on a regional and national level to liaise with local communities, in particular SMEs, map HPC competencies and facilitate access to European HPC resources for users from the private and public sector. EuroCC 2 delivers training, interacts with industry, develops competence mapping and communication materials and activities, and supports the adoption of HPC services in other related fields, such as quantum computing, artificial intelligence (AI), high performance data analytics (HPDA) to expand the HPC user base. EuroCC Cyprus The Computation-based Science and Technology Research Center (CaSToRC) of the Cyprus Institute (CyI) is designated by the government of Cyprus as the National Competence Center (NCC) in High Performance Computing (HPC) and related technologies of Artificial Intelligence (AI), Machine Learning (ML) and Data Science (DS). This activity is part of the EuroCC projects.","title":"About"},{"location":"about/#eurocc-2-project","text":"EuroCC 2 will work to identify and address the skills gaps in the European High Performance Computing (HPC) ecosystem and coordinate cooperation across Europe to ensure a consistent skills base. The role of EuroCC 2 is to establish and run a network of more than 30 NCCs across the EuroHPC Participating States. The NCCs act as single points of access in each country between stakeholders and national and EuroHPC systems. They operate on a regional and national level to liaise with local communities, in particular SMEs, map HPC competencies and facilitate access to European HPC resources for users from the private and public sector. EuroCC 2 delivers training, interacts with industry, develops competence mapping and communication materials and activities, and supports the adoption of HPC services in other related fields, such as quantum computing, artificial intelligence (AI), high performance data analytics (HPDA) to expand the HPC user base.","title":"EuroCC-2 project"},{"location":"about/#eurocc-cyprus","text":"The Computation-based Science and Technology Research Center (CaSToRC) of the Cyprus Institute (CyI) is designated by the government of Cyprus as the National Competence Center (NCC) in High Performance Computing (HPC) and related technologies of Artificial Intelligence (AI), Machine Learning (ML) and Data Science (DS). This activity is part of the EuroCC projects.","title":"EuroCC Cyprus"},{"location":"tutorials/t01_introduction_to_hpc_systems/","text":"1. Introduction to HPC Systems 1.1. Overview This tutorial provides a high-level introduction to High-Performance Computing (HPC) systems, with a focus on Cyclone's architecture and operational principles. Participants will explore the fundamental components of an HPC system, including compute nodes and file systems, while gaining insight into data management policies. Additionally, the tutorial introduces key concepts such as software management through modules, and job scheduling using SLURM, setting the stage for deeper exploration in subsequent tutorials. 1.2. Learning Objectives By the end of this tutorial, participants will be able to: Describe the architecture of an HPC system, including Cyclone\u2019s compute nodes and interconnects. Identify and understand the use cases of Cyclone\u2019s file systems (home, scratch, shared directories). Identify when to use an HPC system or alternative solutions such as Cloud systems or High-end Workstations. Recognize the role of modules in managing software environments and how they simplify system use. Understand the purpose of job scheduling and the function of SLURM in resource management. 1.3. Overview of HPC Architectures An HPC system is typically composed of multiple interconnected compute nodes which work in parallel to solve large-scale tasks. Each compute node consists of CPUs and GPUs (in some cases) designed for performing the computations. These nodes are interconnected through high-speed networks, which enable them to share data efficiently and work in parallel. Key components of an HPC system: Login Nodes: These are the entry points for users to connect to the system. Users prepare their jobs, access resources, and manage data from the login nodes. However, compute jobs are not executed here. Compute Nodes: These nodes execute user jobs. They often come with specialized hardware, like GPUs, to accelerate computational workloads. Interconnects: These are high-speed networks that link the compute nodes together, ensuring fast data transfer between them. The interconnects are critical for achieving low latency and high bandwidth communication essential for parallel computing. Storage Systems: These systems are used to manage the data. They include both local storage for temporary data and shared storage for datasets that are accessible across all nodes in the system, through the available filesystem. The architecture ensures that large computational tasks are broken down into smaller sub-tasks, which are processed simultaneously across the nodes. 1.4. Overview of Cyclone Cyclone is the National HPC system hosted at The Cyprus Institute . It is designed to support a range of disciplines, including physics, climate modeling, bioinformatics, AI and more. Cyclone consists of login and compute nodes, high-speed interconnects and a file system to manage data storage across different types of files and directories. 1.4.1 Architecture Configuration Compute Nodes: These are the main engines that run the jobs and computations. Cyclone has two types of compute nodes: CPU Nodes: There are 17 nodes, each with 40 cores . These are used for general-purpose computing tasks like simulations, data processing, and more. GPU Nodes: Cyclone has 16 nodes with 40 cores and each node is also equipped with 4 NVIDIA V100 GPUs . These specialized nodes are perfect for tasks that require faster computations, such as deep learning (AI), large-scale simulations, or tasks that benefit from the extra power of GPUs. Node Details: Each node has 2 CPUs , each with 20 cores (so, 40 cores total). These CPUs are Intel Xeon Gold 6248 , which are very powerful and efficient for handling parallel tasks. Each node also has 192 GB of memory (RAM). This is where the data is temporarily stored and processed while jobs are running. Storage: Cyclone has 135 TB of fast NVMe storage for the Scratch and Home directories. These directories are used to store files and data that you\u2019re working on during a job. The Scratch directory is for temporary files, and the Home directory is for your personal files and scripts. Additionally, there is 3.2 PB (Petabytes) of Shared Disk Storage . This is a large space used for collaboration, allowing multiple users or teams to access and share project data. Interconnect: The system has an HDR 100 Node-to-Node interconnect , which is a high-speed network (up to 100GB/s) that allows nodes to communicate with each other very quickly. This is especially important for tasks that involve a lot of data moving between nodes, such as simulations and large data analysis. Operating System: Cyclone uses Rocky Linux 8.6 , an operating system that\u2019s optimized for high-performance computing tasks. This system is designed to support demanding tasks, like scientific simulations, machine learning, and data analysis, providing the necessary computing power, fast data transfer, and shared storage to store large datasets. 1.4.2. File System Overview When working on Cyclone, your files will be stored in specific directories tailored to different purposes. Each directory has unique performance characteristics and retention policies. It is important to note that no backups are performed on any of these directories \u2014it is solely the user's responsibility to maintain their own backups. Directory Home Directory Scratch Directory Shared Data Directory Path /nvme/h/your_username/ /nvme/scratch/your_username/ /onyx/data/project_name/ Description Personal space on Cyclone for storing configuration files, scripts, and small, critical files. High-performance, temporary storage for active computations and intermediate files. Shared project directory available to multiple users for collaborative datasets or resources. Performance Moderate I/O performance. High I/O performance optimized for compute-intensive workloads. Moderate to low I/O performance. Retention Persistent storage with no automatic cleanup, but limited in size. Monitor usage to avoid exceeding your quota. Temporary storage. Files may be deleted after a set period or when the system requires space. Persistent storage, but subject to project-specific quotas and policies. Usage Tips - Store SSH keys, environment setup files, and small codebases. - Avoid storing large datasets or temporary files here. - The home path can also be displayed by using the $HOME variable. - Use for large datasets or files generated during computations. - Regularly move important results to your home directory or a local backup to prevent data loss. - The user Scratch path can also be located at ~/scratch . - Collaborate with team members by storing shared input data and results. - Ensure file organization and naming conventions are clear for effective collaboration. 1.4.3. Important Notes and Best Practises No Backups: None of these directories are backed up by Cyclone. You must regularly back up your important data to a secure location. Data Responsibility: It is the user's sole responsibility to maintain copies of critical files. Loss of data in these directories due to system failure or cleanup policies is irreversible. Store active job data in the Scratch directory. Keep your source code and executables in the Home directory. Store large shared data in Shared directories for collaboration. Always back up important results from the Scratch directory to the Home or external storage to avoid data loss. 1.5. HPC Systems vs Cloud Systems vs High-End Workstations Both HPC systems , cloud platforms , and high-end workstations offer powerful computational resources but are optimized for different tasks and use cases. Here's a comparison to help understand which system is more suitable for specific applications: Feature HPC Systems Cloud Systems High-End Workstations Purpose Optimized for large-scale, parallel, and intensive computations like simulations, AI, and complex calculations. Designed for scalability and flexibility, best for small-scale tasks, web applications, or quick scalability. Suitable for single-machine tasks like 3D rendering, video editing, and software development, where extreme parallelism is not required. Hardware Specialized CPUs, GPUs, and high-speed interconnects optimized for performance. General-purpose hardware, flexible virtual machines, varying capabilities. Single high-performance CPU and GPU, moderate memory capacity (64GB to 128GB), optimized for individual tasks. Resource Allocation Fixed, controlled environment with job schedulers like SLURM to allocate resources efficiently. On-demand, pay-as-you-go provisioning with flexible scaling. Limited scalability, bound by local hardware limitations. When to Use Ideal for large-scale simulations (climate modeling, molecular dynamics), intensive data processing (genomics, weather prediction), or AI tasks requiring parallel computing (large scale training, hyperparameter optimisation). Best for web applications, small to medium workloads, or cost-effective solutions for short-term or ad-hoc tasks. Best for personal or small-team use, with high power needed for design, simulations, or smaller AI workloads. 1.6. Introduction to Modules In a High-Performance Computing (HPC) system like Cyclone, users often need to work with specialized software and libraries that are required for their research or computational tasks. Modules help manage these software environments in a way that simplifies the process and ensures compatibility across different users and applications. 1.6.1. What are Modules? A module is a tool that allows users to dynamically load, unload, and switch between different software environments without having to manually configure system paths or dependencies. Modules are used to make software easier to access on an HPC system, so you don\u2019t need to worry about installation or environment conflicts. For example, when you want to use a specific version of Python, you can load the Python module for that version, and the system will automatically configure the necessary settings for you. 1.6.2. Why Use Modules? Using modules has several key benefits: No Root Access: On HPC Systems, users do not have administrative priviledges in order to be able to install software in the same way as in traditional Linux systems (e.g., using sudo apt-get ). Simplifies Environment Management: You don\u2019t need to worry about setting up complicated software environments. The system automatically adjusts paths and settings when you load a module. Prevents Software Conflicts: Many HPC systems, including Cyclone, host multiple versions of software. Modules ensure that you can use the right version for your task without causing conflicts with other users. Saves Time: Instead of manually installing or configuring software, you can simply load the required module and begin working right away. Ensures Reproducibility: By using modules, you ensure that your environment is consistent across different sessions and that your work can be replicated by others. 1.6.3. How Modules Work Modules work by modifying the environment variables (like PATH , LD_LIBRARY_PATH , and others) to point to the correct version of the software. These environment variables tell the system where to find executables, libraries, and other resources required for the software to run properly. For example, if you load the Python 3.10 module, the system will automatically adjust the environment to use the Python 3.10 binary and related libraries without affecting other users who may be using a different version of Python. 1.6.4. Common Module Commands Cheat Sheet Below is a table summarizing some of the most commonly used module commands to manage your software environment on Cyclone. Command Description Example List Available Modules View all available software modules. module avail Load a Module Load a specific module to set up the software environment. module load Python/3.8.5 Unload a Module Unload a currently loaded module when it is no longer needed. module unload Python/3.8.5 Check Loaded Modules View a list of all currently loaded modules in your environment. module list Switch Between Modules Switch from one version of a module to another (e.g., switch Python versions). module switch Python/3.8.5 Python/3.9.1 1.7. Introduction to Job Scheduling and SLURM In systems like Cyclone, multiple users often share the same resources (e.g., CPUs, memory, GPUs). To ensure that everyone gets fair access to the system\u2019s resources, HPC systems use job scheduling . SLURM ( Simple Linux Utility for Resource Management ) is the job scheduler used on Cyclone to manage and allocate resources for running computational tasks. 1.7.1. What is Job Scheduling? Job scheduling is a process where the system manages the allocation of computational resources for running tasks (jobs). Instead of users running tasks directly on the system, SLURM queues up jobs, decides when and where they should run, and allocates resources such as CPUs, memory, and GPUs to those jobs. This is especially important in a multi-user environment like Cyclone, where many tasks might need to run at the same time. SLURM helps manage and prioritize these tasks, ensuring that resources are used efficiently and fairly. 1.7.2. Why is Job Scheduling Important? Job scheduling is important because: Fair Resource Allocation: It ensures that no single user monopolizes the system and that resources are shared equitably among all users. Efficient Use of Resources: SLURM makes sure that the system\u2019s resources are used optimally, by deciding the best time and place to run each job. Queue Management: SLURM organizes jobs into queues and allows jobs to wait in line until resources become available. 1.7.3. How Does SLURM Work? SLURM divides resources into partitions ( groups of compute nodes ) based on their hardware and intended use. A user submits a job request, and SLURM assigns the job to the most appropriate node, based on the job\u2019s resource requirements (e.g., number of cores, memory, GPU). SLURM provides several commands to interact with the job scheduler and manage jobs. Below are the key SLURM commands that you will use to submit and manage jobs. 1.7.4. Key SLURM Commands Cheat Sheet Category Command Description Job Submission sbatch <script> Submit a job using a submission script. Cancel Job scancel <job_id> Cancel a specific job using its job ID. Hold Job scontrol hold <job_id> Place a job on hold, preventing it from starting. Release Job scontrol release <job_id> Release a held job, allowing it to start when resources become available. Queue Overview squeue View the status of all jobs in the queue. My Queued Jobs squeue -u <your_username> View jobs specific to your user account. Detailed Job Info scontrol show job <job_id> Show detailed information about a specific job. Job History sacct -j <job_id> View historical job statistics and performance for a specific job. Partition Info sinfo View available partitions, their nodes, and current states. Node Details sinfo -N Display detailed node information for all partitions. Resources Per Node scontrol show node <node_name> Display detailed resource availability for a specific node. 1.7.5. Job Status Symbols SLURM uses the following symbols to indicate the current state of a job: Symbol Status Description PD Pending Job is waiting for resources or dependencies to become available. R Running Job is currently executing. CG Completing Job is completing, cleaning up resources. CF Configuring Job is configuring, setting up resources. CD Completed Job has finished successfully. F Failed Job failed to execute successfully. TO Timeout Job exceeded the allocated time limit. CA Canceled Job was canceled by the user or administrator. NF Node Failure Job failed due to a node failure. ST Stopped Job has been stopped. 1.7.6. Submitting a Job with sbatch To submit a job to SLURM, you will typically create a job script (a text file with the required commands) and submit it using the sbatch command. The job script defines the resources your job needs (such as the number of CPUs, memory, and time), along with the command to run your application. Example Job Script my_job_script.sh : #!/bin/bash #SBATCH --job-name=my_job #SBATCH --output=output.txt #SBATCH --error=error.txt #SBATCH --time=01:00:00 # Set the maximum runtime (hh:mm:ss) #SBATCH --ntasks=1 # Number of tasks (CPUs) to run #SBATCH --mem=4G # Memory required for the job #SBATCH --partition=cpu # Which partition to run the job on # Your command(s) to run the job # e.g., a Python script or an executable python my_script.py In this script: #SBATCH lines are SLURM directives that define resource requirements . After defining the resources, the script contains the command to run the job, in this case, a Python script ( python my_script.py ). Note that instead of a single command, it could have been a series of commands. Once the job script is ready, you can submit it with the following command: sbatch my_job_script.sh 1.7.7. Monitoring Jobs Monitoring your jobs on Cyclone is essential to ensure they are running smoothly. Use the squeue command to check the status of your active jobs, including information such as job ID, partition, and node allocation. If a job needs to be canceled, you can use the scancel command with the job ID. Once a job completes, the sacct command allows you to view detailed statistics, such as resource usage and job duration. Checking Job Status Use squeue to view active jobs: squeue -u <your_username> Output example: JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 12345 cpu test_job user1 R 00:05:12 1 cn01 Canceling Jobs Cancel a running or queued job: scancel <job_id> Viewing Completed Jobs Use sacct to see statistics of completed jobs: sacct -j <job_id> 1.7.8. Resource Allocation When using an HPC system like Cyclone, it\u2019s crucial to request the right resources to run your jobs efficiently and avoid wasting system capacity. Below useful specifications and best practises are described. Useful Specifications Resource Description Job Specification Nodes Number of Nodes --nodes CPU tasks Number of CPU tasks per node --ntasks-per-node CPU threads Number of CPU threads (cores) per task --cpus-per-task Memory Amount of RAM per Job --mem GPUs Request for GPUs if needed --gres System Partition Either use the CPU or GPU part of the system --partition Best Practices Request Only What You Need: Avoid over-requesting resources. Only ask for the CPU cores, memory, and time your job actually needs. Use the Appropriate Partition: Submit jobs to the correct partition (e.g., CPU for general tasks, GPU for tasks requiring GPU acceleration). Specify the Right Number of Cores: Match the number of CPU cores to the needs of your job (e.g., single-core for small tasks, multi-core for parallel tasks). Limit Job Runtime: Set a realistic time limit to prevent wasting resources. Avoid setting excessive or very short time limits. Use Job Arrays for Multiple Jobs: For repetitive tasks (e.g., simulations), use job arrays to submit many jobs efficiently. Avoid Overloading the System: Be mindful of the system load and avoid excessive resource requests, especially during peak usage times. Monitor Job Performance: Use commands like squeue and sacct to check job status and resource usage. Use Interactive Jobs for Debugging: For testing and debugging, run jobs interactively to better understand and optimize resource requirements. Don't run on the login nodes of the system. 1.8. Useful Resources SLURM Best Practises Guide: This guide, created by EuroCC Spain , provides comprehensive information on using SLURM. It covers essential topics such as resource allocation, job initiation, and monitoring. The document highlights best practices for memory management, efficient parallelism, and handling large numbers of jobs. Tips on managing job arrays, wall time, and CPU usage are also included to ensure optimized performance. This guide is particularly valuable for users working with SLURM in high-performance computing environments, offering practical advice on system usage and resource allocation. High-Performance Computing - Why & How: This document serves as an introduction to high-performance computing (HPC), explaining its relevance for research and computational tasks. It details the importance of HPC systems in fields like AI, data-intensive research, and simulations. The guide also emphasizes the need for proper training and access to systems like Cyclone at the Cyprus Institute. It provides an overview of various HPC resources, software, and how they enable more efficient and scalable computations. This guide is an excellent resource for those looking to understand HPC's capabilities and practical applications in the Cypriot ecosystem. 1.9. Recap and Next Steps To conclude, this tutorial provided an overview of Cyclone's architecture, focusing on key components such as compute nodes, storage systems, and high-speed interconnects, as well as the role of file systems like Home Scratch and Shared directories. We also explored the importance of resource management using SLURM , including job scheduling, job submission, and monitoring, as well as the use of modules to manage software environments. With this foundational knowledge, you are now ready to proceed to the next tutorials, where you will gain hands-on experience on accessing the system, with job submission, resource allocation, and utilizing modules, which are crucial for efficient any HPC system, including Cyclone.","title":"Introduction to HPC Systems"},{"location":"tutorials/t01_introduction_to_hpc_systems/#1-introduction-to-hpc-systems","text":"","title":"1. Introduction to HPC Systems"},{"location":"tutorials/t01_introduction_to_hpc_systems/#11-overview","text":"This tutorial provides a high-level introduction to High-Performance Computing (HPC) systems, with a focus on Cyclone's architecture and operational principles. Participants will explore the fundamental components of an HPC system, including compute nodes and file systems, while gaining insight into data management policies. Additionally, the tutorial introduces key concepts such as software management through modules, and job scheduling using SLURM, setting the stage for deeper exploration in subsequent tutorials.","title":"1.1. Overview"},{"location":"tutorials/t01_introduction_to_hpc_systems/#12-learning-objectives","text":"By the end of this tutorial, participants will be able to: Describe the architecture of an HPC system, including Cyclone\u2019s compute nodes and interconnects. Identify and understand the use cases of Cyclone\u2019s file systems (home, scratch, shared directories). Identify when to use an HPC system or alternative solutions such as Cloud systems or High-end Workstations. Recognize the role of modules in managing software environments and how they simplify system use. Understand the purpose of job scheduling and the function of SLURM in resource management.","title":"1.2. Learning Objectives"},{"location":"tutorials/t01_introduction_to_hpc_systems/#13-overview-of-hpc-architectures","text":"An HPC system is typically composed of multiple interconnected compute nodes which work in parallel to solve large-scale tasks. Each compute node consists of CPUs and GPUs (in some cases) designed for performing the computations. These nodes are interconnected through high-speed networks, which enable them to share data efficiently and work in parallel. Key components of an HPC system: Login Nodes: These are the entry points for users to connect to the system. Users prepare their jobs, access resources, and manage data from the login nodes. However, compute jobs are not executed here. Compute Nodes: These nodes execute user jobs. They often come with specialized hardware, like GPUs, to accelerate computational workloads. Interconnects: These are high-speed networks that link the compute nodes together, ensuring fast data transfer between them. The interconnects are critical for achieving low latency and high bandwidth communication essential for parallel computing. Storage Systems: These systems are used to manage the data. They include both local storage for temporary data and shared storage for datasets that are accessible across all nodes in the system, through the available filesystem. The architecture ensures that large computational tasks are broken down into smaller sub-tasks, which are processed simultaneously across the nodes.","title":"1.3. Overview of HPC Architectures"},{"location":"tutorials/t01_introduction_to_hpc_systems/#14-overview-of-cyclone","text":"Cyclone is the National HPC system hosted at The Cyprus Institute . It is designed to support a range of disciplines, including physics, climate modeling, bioinformatics, AI and more. Cyclone consists of login and compute nodes, high-speed interconnects and a file system to manage data storage across different types of files and directories.","title":"1.4. Overview of Cyclone"},{"location":"tutorials/t01_introduction_to_hpc_systems/#141-architecture-configuration","text":"Compute Nodes: These are the main engines that run the jobs and computations. Cyclone has two types of compute nodes: CPU Nodes: There are 17 nodes, each with 40 cores . These are used for general-purpose computing tasks like simulations, data processing, and more. GPU Nodes: Cyclone has 16 nodes with 40 cores and each node is also equipped with 4 NVIDIA V100 GPUs . These specialized nodes are perfect for tasks that require faster computations, such as deep learning (AI), large-scale simulations, or tasks that benefit from the extra power of GPUs. Node Details: Each node has 2 CPUs , each with 20 cores (so, 40 cores total). These CPUs are Intel Xeon Gold 6248 , which are very powerful and efficient for handling parallel tasks. Each node also has 192 GB of memory (RAM). This is where the data is temporarily stored and processed while jobs are running. Storage: Cyclone has 135 TB of fast NVMe storage for the Scratch and Home directories. These directories are used to store files and data that you\u2019re working on during a job. The Scratch directory is for temporary files, and the Home directory is for your personal files and scripts. Additionally, there is 3.2 PB (Petabytes) of Shared Disk Storage . This is a large space used for collaboration, allowing multiple users or teams to access and share project data. Interconnect: The system has an HDR 100 Node-to-Node interconnect , which is a high-speed network (up to 100GB/s) that allows nodes to communicate with each other very quickly. This is especially important for tasks that involve a lot of data moving between nodes, such as simulations and large data analysis. Operating System: Cyclone uses Rocky Linux 8.6 , an operating system that\u2019s optimized for high-performance computing tasks. This system is designed to support demanding tasks, like scientific simulations, machine learning, and data analysis, providing the necessary computing power, fast data transfer, and shared storage to store large datasets.","title":"1.4.1 Architecture Configuration"},{"location":"tutorials/t01_introduction_to_hpc_systems/#142-file-system-overview","text":"When working on Cyclone, your files will be stored in specific directories tailored to different purposes. Each directory has unique performance characteristics and retention policies. It is important to note that no backups are performed on any of these directories \u2014it is solely the user's responsibility to maintain their own backups. Directory Home Directory Scratch Directory Shared Data Directory Path /nvme/h/your_username/ /nvme/scratch/your_username/ /onyx/data/project_name/ Description Personal space on Cyclone for storing configuration files, scripts, and small, critical files. High-performance, temporary storage for active computations and intermediate files. Shared project directory available to multiple users for collaborative datasets or resources. Performance Moderate I/O performance. High I/O performance optimized for compute-intensive workloads. Moderate to low I/O performance. Retention Persistent storage with no automatic cleanup, but limited in size. Monitor usage to avoid exceeding your quota. Temporary storage. Files may be deleted after a set period or when the system requires space. Persistent storage, but subject to project-specific quotas and policies. Usage Tips - Store SSH keys, environment setup files, and small codebases. - Avoid storing large datasets or temporary files here. - The home path can also be displayed by using the $HOME variable. - Use for large datasets or files generated during computations. - Regularly move important results to your home directory or a local backup to prevent data loss. - The user Scratch path can also be located at ~/scratch . - Collaborate with team members by storing shared input data and results. - Ensure file organization and naming conventions are clear for effective collaboration.","title":"1.4.2. File System Overview"},{"location":"tutorials/t01_introduction_to_hpc_systems/#143-important-notes-and-best-practises","text":"No Backups: None of these directories are backed up by Cyclone. You must regularly back up your important data to a secure location. Data Responsibility: It is the user's sole responsibility to maintain copies of critical files. Loss of data in these directories due to system failure or cleanup policies is irreversible. Store active job data in the Scratch directory. Keep your source code and executables in the Home directory. Store large shared data in Shared directories for collaboration. Always back up important results from the Scratch directory to the Home or external storage to avoid data loss.","title":"1.4.3. Important Notes and Best Practises"},{"location":"tutorials/t01_introduction_to_hpc_systems/#15-hpc-systems-vs-cloud-systems-vs-high-end-workstations","text":"Both HPC systems , cloud platforms , and high-end workstations offer powerful computational resources but are optimized for different tasks and use cases. Here's a comparison to help understand which system is more suitable for specific applications: Feature HPC Systems Cloud Systems High-End Workstations Purpose Optimized for large-scale, parallel, and intensive computations like simulations, AI, and complex calculations. Designed for scalability and flexibility, best for small-scale tasks, web applications, or quick scalability. Suitable for single-machine tasks like 3D rendering, video editing, and software development, where extreme parallelism is not required. Hardware Specialized CPUs, GPUs, and high-speed interconnects optimized for performance. General-purpose hardware, flexible virtual machines, varying capabilities. Single high-performance CPU and GPU, moderate memory capacity (64GB to 128GB), optimized for individual tasks. Resource Allocation Fixed, controlled environment with job schedulers like SLURM to allocate resources efficiently. On-demand, pay-as-you-go provisioning with flexible scaling. Limited scalability, bound by local hardware limitations. When to Use Ideal for large-scale simulations (climate modeling, molecular dynamics), intensive data processing (genomics, weather prediction), or AI tasks requiring parallel computing (large scale training, hyperparameter optimisation). Best for web applications, small to medium workloads, or cost-effective solutions for short-term or ad-hoc tasks. Best for personal or small-team use, with high power needed for design, simulations, or smaller AI workloads.","title":"1.5. HPC Systems vs Cloud Systems vs High-End Workstations"},{"location":"tutorials/t01_introduction_to_hpc_systems/#16-introduction-to-modules","text":"In a High-Performance Computing (HPC) system like Cyclone, users often need to work with specialized software and libraries that are required for their research or computational tasks. Modules help manage these software environments in a way that simplifies the process and ensures compatibility across different users and applications.","title":"1.6. Introduction to Modules"},{"location":"tutorials/t01_introduction_to_hpc_systems/#161-what-are-modules","text":"A module is a tool that allows users to dynamically load, unload, and switch between different software environments without having to manually configure system paths or dependencies. Modules are used to make software easier to access on an HPC system, so you don\u2019t need to worry about installation or environment conflicts. For example, when you want to use a specific version of Python, you can load the Python module for that version, and the system will automatically configure the necessary settings for you.","title":"1.6.1. What are Modules?"},{"location":"tutorials/t01_introduction_to_hpc_systems/#162-why-use-modules","text":"Using modules has several key benefits: No Root Access: On HPC Systems, users do not have administrative priviledges in order to be able to install software in the same way as in traditional Linux systems (e.g., using sudo apt-get ). Simplifies Environment Management: You don\u2019t need to worry about setting up complicated software environments. The system automatically adjusts paths and settings when you load a module. Prevents Software Conflicts: Many HPC systems, including Cyclone, host multiple versions of software. Modules ensure that you can use the right version for your task without causing conflicts with other users. Saves Time: Instead of manually installing or configuring software, you can simply load the required module and begin working right away. Ensures Reproducibility: By using modules, you ensure that your environment is consistent across different sessions and that your work can be replicated by others.","title":"1.6.2. Why Use Modules?"},{"location":"tutorials/t01_introduction_to_hpc_systems/#163-how-modules-work","text":"Modules work by modifying the environment variables (like PATH , LD_LIBRARY_PATH , and others) to point to the correct version of the software. These environment variables tell the system where to find executables, libraries, and other resources required for the software to run properly. For example, if you load the Python 3.10 module, the system will automatically adjust the environment to use the Python 3.10 binary and related libraries without affecting other users who may be using a different version of Python.","title":"1.6.3. How Modules Work"},{"location":"tutorials/t01_introduction_to_hpc_systems/#164-common-module-commands-cheat-sheet","text":"Below is a table summarizing some of the most commonly used module commands to manage your software environment on Cyclone. Command Description Example List Available Modules View all available software modules. module avail Load a Module Load a specific module to set up the software environment. module load Python/3.8.5 Unload a Module Unload a currently loaded module when it is no longer needed. module unload Python/3.8.5 Check Loaded Modules View a list of all currently loaded modules in your environment. module list Switch Between Modules Switch from one version of a module to another (e.g., switch Python versions). module switch Python/3.8.5 Python/3.9.1","title":"1.6.4. Common Module Commands Cheat Sheet"},{"location":"tutorials/t01_introduction_to_hpc_systems/#17-introduction-to-job-scheduling-and-slurm","text":"In systems like Cyclone, multiple users often share the same resources (e.g., CPUs, memory, GPUs). To ensure that everyone gets fair access to the system\u2019s resources, HPC systems use job scheduling . SLURM ( Simple Linux Utility for Resource Management ) is the job scheduler used on Cyclone to manage and allocate resources for running computational tasks.","title":"1.7. Introduction to Job Scheduling and SLURM"},{"location":"tutorials/t01_introduction_to_hpc_systems/#171-what-is-job-scheduling","text":"Job scheduling is a process where the system manages the allocation of computational resources for running tasks (jobs). Instead of users running tasks directly on the system, SLURM queues up jobs, decides when and where they should run, and allocates resources such as CPUs, memory, and GPUs to those jobs. This is especially important in a multi-user environment like Cyclone, where many tasks might need to run at the same time. SLURM helps manage and prioritize these tasks, ensuring that resources are used efficiently and fairly.","title":"1.7.1. What is Job Scheduling?"},{"location":"tutorials/t01_introduction_to_hpc_systems/#172-why-is-job-scheduling-important","text":"Job scheduling is important because: Fair Resource Allocation: It ensures that no single user monopolizes the system and that resources are shared equitably among all users. Efficient Use of Resources: SLURM makes sure that the system\u2019s resources are used optimally, by deciding the best time and place to run each job. Queue Management: SLURM organizes jobs into queues and allows jobs to wait in line until resources become available.","title":"1.7.2. Why is Job Scheduling Important?"},{"location":"tutorials/t01_introduction_to_hpc_systems/#173-how-does-slurm-work","text":"SLURM divides resources into partitions ( groups of compute nodes ) based on their hardware and intended use. A user submits a job request, and SLURM assigns the job to the most appropriate node, based on the job\u2019s resource requirements (e.g., number of cores, memory, GPU). SLURM provides several commands to interact with the job scheduler and manage jobs. Below are the key SLURM commands that you will use to submit and manage jobs.","title":"1.7.3. How Does SLURM Work?"},{"location":"tutorials/t01_introduction_to_hpc_systems/#174-key-slurm-commands-cheat-sheet","text":"Category Command Description Job Submission sbatch <script> Submit a job using a submission script. Cancel Job scancel <job_id> Cancel a specific job using its job ID. Hold Job scontrol hold <job_id> Place a job on hold, preventing it from starting. Release Job scontrol release <job_id> Release a held job, allowing it to start when resources become available. Queue Overview squeue View the status of all jobs in the queue. My Queued Jobs squeue -u <your_username> View jobs specific to your user account. Detailed Job Info scontrol show job <job_id> Show detailed information about a specific job. Job History sacct -j <job_id> View historical job statistics and performance for a specific job. Partition Info sinfo View available partitions, their nodes, and current states. Node Details sinfo -N Display detailed node information for all partitions. Resources Per Node scontrol show node <node_name> Display detailed resource availability for a specific node.","title":"1.7.4. Key SLURM Commands Cheat Sheet"},{"location":"tutorials/t01_introduction_to_hpc_systems/#175-job-status-symbols","text":"SLURM uses the following symbols to indicate the current state of a job: Symbol Status Description PD Pending Job is waiting for resources or dependencies to become available. R Running Job is currently executing. CG Completing Job is completing, cleaning up resources. CF Configuring Job is configuring, setting up resources. CD Completed Job has finished successfully. F Failed Job failed to execute successfully. TO Timeout Job exceeded the allocated time limit. CA Canceled Job was canceled by the user or administrator. NF Node Failure Job failed due to a node failure. ST Stopped Job has been stopped.","title":"1.7.5. Job Status Symbols"},{"location":"tutorials/t01_introduction_to_hpc_systems/#176-submitting-a-job-with-sbatch","text":"To submit a job to SLURM, you will typically create a job script (a text file with the required commands) and submit it using the sbatch command. The job script defines the resources your job needs (such as the number of CPUs, memory, and time), along with the command to run your application.","title":"1.7.6. Submitting a Job with sbatch"},{"location":"tutorials/t01_introduction_to_hpc_systems/#example-job-script-my_job_scriptsh","text":"#!/bin/bash #SBATCH --job-name=my_job #SBATCH --output=output.txt #SBATCH --error=error.txt #SBATCH --time=01:00:00 # Set the maximum runtime (hh:mm:ss) #SBATCH --ntasks=1 # Number of tasks (CPUs) to run #SBATCH --mem=4G # Memory required for the job #SBATCH --partition=cpu # Which partition to run the job on # Your command(s) to run the job # e.g., a Python script or an executable python my_script.py In this script: #SBATCH lines are SLURM directives that define resource requirements . After defining the resources, the script contains the command to run the job, in this case, a Python script ( python my_script.py ). Note that instead of a single command, it could have been a series of commands. Once the job script is ready, you can submit it with the following command: sbatch my_job_script.sh","title":"Example Job Script my_job_script.sh:"},{"location":"tutorials/t01_introduction_to_hpc_systems/#177-monitoring-jobs","text":"Monitoring your jobs on Cyclone is essential to ensure they are running smoothly. Use the squeue command to check the status of your active jobs, including information such as job ID, partition, and node allocation. If a job needs to be canceled, you can use the scancel command with the job ID. Once a job completes, the sacct command allows you to view detailed statistics, such as resource usage and job duration.","title":"1.7.7. Monitoring Jobs"},{"location":"tutorials/t01_introduction_to_hpc_systems/#checking-job-status","text":"Use squeue to view active jobs: squeue -u <your_username> Output example: JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 12345 cpu test_job user1 R 00:05:12 1 cn01","title":"Checking Job Status"},{"location":"tutorials/t01_introduction_to_hpc_systems/#canceling-jobs","text":"Cancel a running or queued job: scancel <job_id>","title":"Canceling Jobs"},{"location":"tutorials/t01_introduction_to_hpc_systems/#viewing-completed-jobs","text":"Use sacct to see statistics of completed jobs: sacct -j <job_id>","title":"Viewing Completed Jobs"},{"location":"tutorials/t01_introduction_to_hpc_systems/#178-resource-allocation","text":"When using an HPC system like Cyclone, it\u2019s crucial to request the right resources to run your jobs efficiently and avoid wasting system capacity. Below useful specifications and best practises are described.","title":"1.7.8. Resource Allocation"},{"location":"tutorials/t01_introduction_to_hpc_systems/#useful-specifications","text":"Resource Description Job Specification Nodes Number of Nodes --nodes CPU tasks Number of CPU tasks per node --ntasks-per-node CPU threads Number of CPU threads (cores) per task --cpus-per-task Memory Amount of RAM per Job --mem GPUs Request for GPUs if needed --gres System Partition Either use the CPU or GPU part of the system --partition","title":"Useful Specifications"},{"location":"tutorials/t01_introduction_to_hpc_systems/#best-practices","text":"Request Only What You Need: Avoid over-requesting resources. Only ask for the CPU cores, memory, and time your job actually needs. Use the Appropriate Partition: Submit jobs to the correct partition (e.g., CPU for general tasks, GPU for tasks requiring GPU acceleration). Specify the Right Number of Cores: Match the number of CPU cores to the needs of your job (e.g., single-core for small tasks, multi-core for parallel tasks). Limit Job Runtime: Set a realistic time limit to prevent wasting resources. Avoid setting excessive or very short time limits. Use Job Arrays for Multiple Jobs: For repetitive tasks (e.g., simulations), use job arrays to submit many jobs efficiently. Avoid Overloading the System: Be mindful of the system load and avoid excessive resource requests, especially during peak usage times. Monitor Job Performance: Use commands like squeue and sacct to check job status and resource usage. Use Interactive Jobs for Debugging: For testing and debugging, run jobs interactively to better understand and optimize resource requirements. Don't run on the login nodes of the system.","title":"Best Practices"},{"location":"tutorials/t01_introduction_to_hpc_systems/#18-useful-resources","text":"SLURM Best Practises Guide: This guide, created by EuroCC Spain , provides comprehensive information on using SLURM. It covers essential topics such as resource allocation, job initiation, and monitoring. The document highlights best practices for memory management, efficient parallelism, and handling large numbers of jobs. Tips on managing job arrays, wall time, and CPU usage are also included to ensure optimized performance. This guide is particularly valuable for users working with SLURM in high-performance computing environments, offering practical advice on system usage and resource allocation. High-Performance Computing - Why & How: This document serves as an introduction to high-performance computing (HPC), explaining its relevance for research and computational tasks. It details the importance of HPC systems in fields like AI, data-intensive research, and simulations. The guide also emphasizes the need for proper training and access to systems like Cyclone at the Cyprus Institute. It provides an overview of various HPC resources, software, and how they enable more efficient and scalable computations. This guide is an excellent resource for those looking to understand HPC's capabilities and practical applications in the Cypriot ecosystem.","title":"1.8. Useful Resources"},{"location":"tutorials/t01_introduction_to_hpc_systems/#19-recap-and-next-steps","text":"To conclude, this tutorial provided an overview of Cyclone's architecture, focusing on key components such as compute nodes, storage systems, and high-speed interconnects, as well as the role of file systems like Home Scratch and Shared directories. We also explored the importance of resource management using SLURM , including job scheduling, job submission, and monitoring, as well as the use of modules to manage software environments. With this foundational knowledge, you are now ready to proceed to the next tutorials, where you will gain hands-on experience on accessing the system, with job submission, resource allocation, and utilizing modules, which are crucial for efficient any HPC system, including Cyclone.","title":"1.9. Recap and Next Steps"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/","text":"2. Accessing and Navigating Cyclone 2.1. Overview This tutorial focuses on providing participants with the practical skills needed to access and navigate the Cyclone HPC system. It covers secure system access across different platforms, transferring files to and from Cyclone, and managing data within its file systems. Participants will also learn fundamental Linux commands to navigate directories and organize data effectively. By the end of this tutorial, users will have connected to Cyclone, transferred files, and explored the directory structure using basic Linux commands. 2.2. Learning Objectives By the end of this tutorial, participants will be able to: Securely access Cyclone using SSH, with tailored solutions for different operating systems (Linux, macOS, Windows). Transfer data to and from Cyclone using tools like scp, rsync, and FileZilla. Navigate Cyclone\u2019s directory structure and manage files effectively using basic Linux commands. Understand best practices for data organization and storage across Cyclone\u2019s file systems, including strategies for long-term data management. 2.3. Prerequisites A Cyclone account (contact your system administrator if you don't have one) A computer with internet connection Administrator rights or permission to install software (for some setup options) 2.4. Getting Started 2.4.1. What is SSH and Why is it Important? SSH, or Secure Shell, is a secure way to access and manage remote systems, such as High-Performance Computing (HPC) resources, over a network. It encrypts all communication, protecting sensitive information from being intercepted by unauthorized users. SSH is essential because it provides a safe and efficient way to connect to powerful remote systems for tasks like running simulations, managing files, and analyzing data. Instead of using vulnerable passwords, SSH often uses a system called public-key cryptography to verify your identity. Here\u2019s how it works: SSH relies on a pair of keys\u2014a public key and a private key. The public key is shared with the remote system (the server), acting like a lock, while the private key stays safely on your computer, working as the unique key that can open that lock. When you try to connect, the server sends a challenge that only your private key can solve. If it\u2019s solved correctly, the server knows it\u2019s you, and the connection is established securely. This approach ensures that even if someone intercepts the communication, they can\u2019t access your data or impersonate you. SSH combines simplicity and robust security, making it an indispensable tool for accessing and using HPC systems effectively. 2.4.2. For MacOS and Linux Users Open the Terminal application On MacOS: Use Spotlight (Command + Space) and type \"Terminal\" On Linux: Use your system's application launcher and search for \"Terminal\" Keep this Terminal window open throughout the setup process 2.4.3. For Windows Users Choose your terminal: If using PowerShell: Search for \"PowerShell\" in the Start menu and run as Administrator If using Git Bash (recommended): Launch Git Bash from the Start menu 2.5. Setting Up SSH and your keys \u26a0\ufe0f Remember to replace username with your actual Cyclone username in all examples. 2.5.1. MacOS and Linux Using SSH keys is more secure and convenient than password authentication. Here's how to set them up: Managing SSH Keys Start the SSH agent: eval \"$(ssh-agent -s)\" Add your SSH key to the agent: If you used another filename or directory to store your ssh key you generated, make sure to change it ssh-add ~/.ssh/id_rsa SSH Config File Setup Navigate to your SSH directory: cd ~/.ssh/ To simplify connections create or edit ~/.ssh/config with nano: nano config In the nano editor, add these lines: Host cyclone HostName cyclone.hpcf.cyi.ac.cy User your_username IdentityFile ~/.ssh/id_rsa If you're using macOS, add this to your ~/.ssh/config file to make the key persistent: Host * UseKeychain yes AddKeysToAgent yes IdentityFile ~/.ssh/id_rsa To save the file: Press Ctrl + X Press Y to confirm Press Enter to save Once you save the file, return to your Terminal window to test the connection: ssh cyclone 2.5.2. Windows Option 1: Using PowerShell 1. Start the SSH Agent Open PowerShell as Administrator. Then check if the SSH agent service is running: Get-Service ssh-agent If the service is stopped, enable and start it: # Set the service to manual startup Set-Service ssh-agent -StartupType Manual # Start the service Start-Service ssh-agent 2. Add Your SSH Key Add your private key to the SSH agent: ssh-add $env:USERPROFILE\\.ssh\\id_rsa To verify if the key was added: ssh-add -l 3. Connect to Cyclone ssh username@cyclone.hpcf.cyi.ac.cy Replace username with your Cyclone username. Option 2: Using Git Bash (Recommended) Using git bash doesn't require administrator permissions . Therefore this option will work even if Administrator restrictions apply on your computer. 1. Start Git Bash Download Git Bash from https://git-scm.com/downloads Install Git Bash accepting the default options Open Git Bash from your start menu 2. Add Your SSH Key ssh-add ~/.ssh/id_rsa 3. Create SSH Config (Optional but Recommended) Create or edit ~/.ssh/config : nano ~/.ssh/config Add these lines: Host cyclone HostName cyclone.hpcf.cyi.ac.cy User your_username IdentityFile ~/.ssh/id_rsa Replace your_username with your Cyclone username. Save and close the file (Ctrl+X, then Y, then Enter) 4. Connect to Cyclone If using SSH config: ssh cyclone Without SSH config: ssh username@cyclone.hpcf.cyi.ac.cy When you successfully ssh/login, you'll be greeted with this message: If you are having trouble ssh/logging, refer to the troubleshooting steps at the end or try again the process from the beginning. 2.6. Managing Data and Directories on Cyclone 2.6.1. Home Directory Structure When you log in, your home directory ( /nvme/h/ ) typically contains: # Project data links data_p166/ -> /onyx/data/p166 # Shared project storage data_p184/ -> /onyx/data/p184 # Another project data_p213/ -> /onyx/data/p213 # And so on... # Scratch space link scratch/ -> /nvme/scratch/<username> # Personal scratch space # Scratch space link for event edu26/ -> /nvme/scratch/edu26 # Shared scratch space for events While these project directories (like data_p166/ ) will appear to be in your home directory, they are actually symbolic links (shortcuts) pointing to their real location on the /onyx/data/ storage system. This is why you'll see them listed when you run ls in your home directory, even though they're physically stored elsewhere. For more information on Cyclones FileSystem please look at Tutorial 01 . To see the contents of your own directory you can use the ls command when you login: ls -a # prints all files/directories, including hidden ones ls -l # prints visible directories in a list including important information like ownership, permissions, last date of modification ls -la # you can combine flags, this creates a list with all files/directories To see your current working directory: pwd 2.6.2. Directory Organization Best Practices Project Data Organization # In your project directory (e.g., ~/data_p166/) data_p166/ \u251c\u2500\u2500 datasets/ # Shared input data \u251c\u2500\u2500 results/ # Project results \u2502 \u251c\u2500\u2500 experiment1/ \u2502 \u2514\u2500\u2500 experiment2/ \u2514\u2500\u2500 shared_scripts/ # Project-specific scripts # In your scratch space scratch/ \u251c\u2500\u2500 job_outputs/ # Temporary job results \u251c\u2500\u2500 temp_data/ # Temporary processing \u2514\u2500\u2500 checkpoints/ # Job checkpoints Personal Organization # In your home directory scripts/ # Personal script collection \u251c\u2500\u2500 job_templates/ # Slurm job templates \u251c\u2500\u2500 analysis/ # Analysis scripts \u2514\u2500\u2500 utils/ # Utility scripts These directories aren't created by default, except the scratch and project specific parent directories 2.6.3. Data Management Best Practices Setting up a New Project Space Using the cd command to change directories: # Navigate to your project directory cd ~/data_p166 The ~/ we use in our commands points to the home directory Using the mkdir command to make a directory. # Create standard project structure mkdir -p datasets # For input data mkdir -p results # For processed results mkdir -p shared_scripts # For project-specific scripts mkdir -p documentation # For project documentation The -p makes sure that any parent directories that don't already exist are created. Working with Project Data # Create a workspace in scratch for processing mkdir -p ~/scratch/myanalysis cd ~/scratch/myanalysis # Copy input data to scratch for processing cp ~/data_p166/datasets/input.dat ./ # After processing is complete, save important results cp -r ./results ~/data_p166/results/analysis_20240319 # Clean up scratch space cd ~ rm -rf ~/scratch/myanalysis Space Management Using the du command you can view storage usage. To monitor storage: # Check project space usage du -h /onyx/data/p166 \u2139\ufe0f The -h flag makes the sizes 'human readable' meaning it's converting them from bytes to MB/GB/TB # Check scratch usage du -hs ~/scratch \u2139\ufe0f The -s flag summarizes the storage usage of the hole directory. So if you want file by file usage, remove s . Data Safety Keep important data in project directories Use scratch for temporary processing only Regularly clean scratch space Document data organization for team members 2.7. Best Practices Summary Project Organization : Keep project data organized in project directories Use consistent structure across projects Data Management : Store shared data in project directories Use scratch for temporary processing Clean up scratch regularly Document organization for team members Job Workflow : Read input from project directories Process in scratch space Save results back to project directories Clean up scratch after job completion Collaboration : Use project directories for sharing Maintain consistent directory structure Document data organization Communicate changes with team members \u2139\ufe0f Remember that your project memberships determine both your compute resource allocation (via Slurm) and your access to shared storage spaces. Always specify the correct project ID in your Slurm jobs and organize your data accordingly. 2.8. Transferring Files 2.8.1. Before starting Open a terminal on your local machine (not on Cyclone) Make sure you know: The full path of the file/directory on your local machine Where you want it to go on Cyclone (or vice versa) All commands below should be run from your local machine's terminal 2.8.2. Using SCP (Secure Copy): Transferring FROM your local machine TO Cyclone: # Run this command on your local machine's terminal scp /path/on/your/local/machine/localfile.txt cyclone:~/destination/on/cyclone/ Transferring FROM Cyclone TO your local machine: # Run this command on your local machine's terminal scp cyclone:~/path/on/cyclone/remotefile.txt /path/on/your/local/machine/ \ud83d\udca1 If you have not created an ssh config file, replace cyclone with username@cyclone.hpcf.cyi.ac.cy This is what it should look like: 2.8.3. Using Rsync (Recommended for Large Transfers) Copying FROM your local machine TO Cyclone: # Run this on your local machine's terminal # The ./ refers to the current directory on your local machine rsync -avz ./local_directory/ cyclone:~/remote_directory/ Copying FROM Cyclone TO your local machine: # Run this on your local machine's terminal rsync -avz cyclone:~/remote_directory/ ./local_directory/ \ud83d\udca1 Note: All file transfer commands should be run from your local machine's terminal, not from within Cyclone. The paths before the colon (\\:) refer to your local machine, while paths after the colon refer to locations on Cyclone. 2.8.4. Using FileZilla (Graphical Interface) Download and install FileZilla. Then, go to Edit \u2192 Settings Go to SFTP Add your key If your key is in OpenSSH format, you'll be prompted to convert your key. Press yes: Then input your passphrase: And then save the converted key. It's a good idea to keep it at the same place as the original in case you go looking for it in the future. Set up connection: Host: sftp://cyclone.hpcf.cyi.ac.cy Username: your_username Port: 22 Quick connect Once you're connected you'll see your local directory on the left and cyclone on the right. You can now just drag and drop between the two and the transfer wil happen automatically! 2.9. Security Best Practices Use different keys for different services Regularly rotate keys (yearly) Always use strong passphrases Back up your private keys securely Never share private keys 2.10. Notes and Troubleshooting 2.10.1 Installing Required Tools If you don\u2019t have OpenSSH, WSL, or Git Bash installed, refer to the Installation Guide . 2.10.2. ~/.ssh directory does not exist on Windows Option 1: Using Powershell Open PowerShell. Press Windows Key > Search \"Windows PowerShell\" > Enter Navigate to your Home directory cd ~ This will take you to your home directory, typically something like C:\\Users\\<YourUsername> . Create the .ssh directory: mkdir .ssh Verify that the directory was created: ls .ssh If the .ssh folder exists, the command will list its contents (it may be empty if just created). Option 2: Using File Explorer Open File Explorer. Navigate to your home directory: C:\\Users\\<YourUsername> . Create a new folder named .ssh : Right-click and choose New > Folder . Name it .ssh (include the period). Confirm if prompted about using a name that starts with a period. 2.10.3 Changing and Removing File Extensions (Windows) File extensions (like .txt , .png , .exe ) are often hidden by default in Windows, so you'll first need to make extensions visible before removing or changing them. Open File Explorer . Press Win + E or click the folder icon in the taskbar. Access View Options: Windows 10 : Click on the View tab in the toolbar at the top. Windows 11 : Click on the three dots ( ... ) in the toolbar at the top and choose Options . Click on the tab View , go to \"Advanced settings\" and uncheck the checkbox \"Hide extensions for known file types\" if already checked. (Now that the extension is visible) Rename the file: Right-click the file and choose Rename . Remove or modify the extension as needed. Confirm the change when prompted. 2.10.4. Show/Unhide .ssh directory (Windows) File extensions (like .txt , .png , .exe ) are often hidden by default in Windows, so you'll first need to make extensions visible before removing or changing them. 1. Open File Explorer . - Press Win + E or click the folder icon in the taskbar. 2. Access View Options: - Windows 10 : Click on the View tab in the toolbar at the top. - Windows 11 : Click on the three dots ( ... ) in the toolbar at the top and choose Options . 3. Click on the tab View , go to \"Advanced settings\" . 4. Scroll down to \"Hidden files and folders\" and select the option Show hidden files, folders, and drives . 2.10.5. SSH agent issues (MacOS/Linux/Git Bash) To set Up SSH Agent Automatically, first open or create the ~/.bashrc file: nano ~/.bashrc Add these lines to the file: # Start SSH agent if not running if [ -z \"$SSH_AUTH_SOCK\" ] ; then eval `ssh-agent -s` > /dev/null fi Save, close the file (Ctrl+X, then Y, then Enter) and then reload the configuration: source ~/.bashrc 2.10.6. Connection issues (MacOS/Linux/Git Bash) First, check the SSH agent: ssh-add -l #This will list all of your added keys Then verify the relevant files and directories have the correct permissions: chmod 700 ~/.ssh chmod 600 ~/.ssh/id_rsa chmod 644 ~/.ssh/id_rsa.pub This makes sure your private/public key and the directory they are in have the correct read write and execute permissions for the ssh client to allow a connection. Finally, test connection with verbose output: ssh -v cyclone # Adding the `-v` flag prints debuging information 2.10.7 SSH Permission Issues (Powershell) If your key doesn't have the correct permissions: icacls <path-to-your-id_rsa> /inheritance:r /grant:r \"$($env:USERNAME):(F)\" \u26a0\ufe0f Remember to replace username with your actual Cyclone username in all examples.","title":"Accessing and Navigating Cyclone"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#2-accessing-and-navigating-cyclone","text":"","title":"2. Accessing and Navigating Cyclone"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#21-overview","text":"This tutorial focuses on providing participants with the practical skills needed to access and navigate the Cyclone HPC system. It covers secure system access across different platforms, transferring files to and from Cyclone, and managing data within its file systems. Participants will also learn fundamental Linux commands to navigate directories and organize data effectively. By the end of this tutorial, users will have connected to Cyclone, transferred files, and explored the directory structure using basic Linux commands.","title":"2.1. Overview"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#22-learning-objectives","text":"By the end of this tutorial, participants will be able to: Securely access Cyclone using SSH, with tailored solutions for different operating systems (Linux, macOS, Windows). Transfer data to and from Cyclone using tools like scp, rsync, and FileZilla. Navigate Cyclone\u2019s directory structure and manage files effectively using basic Linux commands. Understand best practices for data organization and storage across Cyclone\u2019s file systems, including strategies for long-term data management.","title":"2.2. Learning Objectives"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#23-prerequisites","text":"A Cyclone account (contact your system administrator if you don't have one) A computer with internet connection Administrator rights or permission to install software (for some setup options)","title":"2.3. Prerequisites"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#24-getting-started","text":"","title":"2.4. Getting Started"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#241-what-is-ssh-and-why-is-it-important","text":"SSH, or Secure Shell, is a secure way to access and manage remote systems, such as High-Performance Computing (HPC) resources, over a network. It encrypts all communication, protecting sensitive information from being intercepted by unauthorized users. SSH is essential because it provides a safe and efficient way to connect to powerful remote systems for tasks like running simulations, managing files, and analyzing data. Instead of using vulnerable passwords, SSH often uses a system called public-key cryptography to verify your identity. Here\u2019s how it works: SSH relies on a pair of keys\u2014a public key and a private key. The public key is shared with the remote system (the server), acting like a lock, while the private key stays safely on your computer, working as the unique key that can open that lock. When you try to connect, the server sends a challenge that only your private key can solve. If it\u2019s solved correctly, the server knows it\u2019s you, and the connection is established securely. This approach ensures that even if someone intercepts the communication, they can\u2019t access your data or impersonate you. SSH combines simplicity and robust security, making it an indispensable tool for accessing and using HPC systems effectively.","title":"2.4.1. What is SSH and Why is it Important?"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#242-for-macos-and-linux-users","text":"Open the Terminal application On MacOS: Use Spotlight (Command + Space) and type \"Terminal\" On Linux: Use your system's application launcher and search for \"Terminal\" Keep this Terminal window open throughout the setup process","title":"2.4.2. For MacOS and Linux Users"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#243-for-windows-users","text":"Choose your terminal: If using PowerShell: Search for \"PowerShell\" in the Start menu and run as Administrator If using Git Bash (recommended): Launch Git Bash from the Start menu","title":"2.4.3. For Windows Users"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#25-setting-up-ssh-and-your-keys","text":"\u26a0\ufe0f Remember to replace username with your actual Cyclone username in all examples.","title":"2.5. Setting Up SSH and your keys"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#251-macos-and-linux","text":"Using SSH keys is more secure and convenient than password authentication. Here's how to set them up:","title":"2.5.1. MacOS and Linux"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#managing-ssh-keys","text":"Start the SSH agent: eval \"$(ssh-agent -s)\" Add your SSH key to the agent: If you used another filename or directory to store your ssh key you generated, make sure to change it ssh-add ~/.ssh/id_rsa","title":"Managing SSH Keys"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#ssh-config-file-setup","text":"Navigate to your SSH directory: cd ~/.ssh/ To simplify connections create or edit ~/.ssh/config with nano: nano config In the nano editor, add these lines: Host cyclone HostName cyclone.hpcf.cyi.ac.cy User your_username IdentityFile ~/.ssh/id_rsa If you're using macOS, add this to your ~/.ssh/config file to make the key persistent: Host * UseKeychain yes AddKeysToAgent yes IdentityFile ~/.ssh/id_rsa To save the file: Press Ctrl + X Press Y to confirm Press Enter to save Once you save the file, return to your Terminal window to test the connection: ssh cyclone","title":"SSH Config File Setup"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#252-windows","text":"","title":"2.5.2. Windows"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#option-1-using-powershell","text":"","title":"Option 1: Using PowerShell"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#1-start-the-ssh-agent","text":"Open PowerShell as Administrator. Then check if the SSH agent service is running: Get-Service ssh-agent If the service is stopped, enable and start it: # Set the service to manual startup Set-Service ssh-agent -StartupType Manual # Start the service Start-Service ssh-agent","title":"1. Start the SSH Agent"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#2-add-your-ssh-key","text":"Add your private key to the SSH agent: ssh-add $env:USERPROFILE\\.ssh\\id_rsa To verify if the key was added: ssh-add -l","title":"2. Add Your SSH Key"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#3-connect-to-cyclone","text":"ssh username@cyclone.hpcf.cyi.ac.cy Replace username with your Cyclone username.","title":"3. Connect to Cyclone"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#option-2-using-git-bash-recommended","text":"Using git bash doesn't require administrator permissions . Therefore this option will work even if Administrator restrictions apply on your computer.","title":"Option 2: Using Git Bash (Recommended)"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#1-start-git-bash","text":"Download Git Bash from https://git-scm.com/downloads Install Git Bash accepting the default options Open Git Bash from your start menu","title":"1. Start Git Bash"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#2-add-your-ssh-key_1","text":"ssh-add ~/.ssh/id_rsa","title":"2. Add Your SSH Key"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#3-create-ssh-config-optional-but-recommended","text":"Create or edit ~/.ssh/config : nano ~/.ssh/config Add these lines: Host cyclone HostName cyclone.hpcf.cyi.ac.cy User your_username IdentityFile ~/.ssh/id_rsa Replace your_username with your Cyclone username. Save and close the file (Ctrl+X, then Y, then Enter)","title":"3. Create SSH Config (Optional but Recommended)"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#4-connect-to-cyclone","text":"If using SSH config: ssh cyclone Without SSH config: ssh username@cyclone.hpcf.cyi.ac.cy When you successfully ssh/login, you'll be greeted with this message: If you are having trouble ssh/logging, refer to the troubleshooting steps at the end or try again the process from the beginning.","title":"4. Connect to Cyclone"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#26-managing-data-and-directories-on-cyclone","text":"","title":"2.6. Managing Data and Directories on Cyclone"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#261-home-directory-structure","text":"When you log in, your home directory ( /nvme/h/ ) typically contains: # Project data links data_p166/ -> /onyx/data/p166 # Shared project storage data_p184/ -> /onyx/data/p184 # Another project data_p213/ -> /onyx/data/p213 # And so on... # Scratch space link scratch/ -> /nvme/scratch/<username> # Personal scratch space # Scratch space link for event edu26/ -> /nvme/scratch/edu26 # Shared scratch space for events While these project directories (like data_p166/ ) will appear to be in your home directory, they are actually symbolic links (shortcuts) pointing to their real location on the /onyx/data/ storage system. This is why you'll see them listed when you run ls in your home directory, even though they're physically stored elsewhere. For more information on Cyclones FileSystem please look at Tutorial 01 . To see the contents of your own directory you can use the ls command when you login: ls -a # prints all files/directories, including hidden ones ls -l # prints visible directories in a list including important information like ownership, permissions, last date of modification ls -la # you can combine flags, this creates a list with all files/directories To see your current working directory: pwd","title":"2.6.1. Home Directory Structure"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#262-directory-organization-best-practices","text":"","title":"2.6.2. Directory Organization Best Practices"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#project-data-organization","text":"# In your project directory (e.g., ~/data_p166/) data_p166/ \u251c\u2500\u2500 datasets/ # Shared input data \u251c\u2500\u2500 results/ # Project results \u2502 \u251c\u2500\u2500 experiment1/ \u2502 \u2514\u2500\u2500 experiment2/ \u2514\u2500\u2500 shared_scripts/ # Project-specific scripts # In your scratch space scratch/ \u251c\u2500\u2500 job_outputs/ # Temporary job results \u251c\u2500\u2500 temp_data/ # Temporary processing \u2514\u2500\u2500 checkpoints/ # Job checkpoints","title":"Project Data Organization"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#personal-organization","text":"# In your home directory scripts/ # Personal script collection \u251c\u2500\u2500 job_templates/ # Slurm job templates \u251c\u2500\u2500 analysis/ # Analysis scripts \u2514\u2500\u2500 utils/ # Utility scripts These directories aren't created by default, except the scratch and project specific parent directories","title":"Personal Organization"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#263-data-management-best-practices","text":"","title":"2.6.3. Data Management Best Practices"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#setting-up-a-new-project-space","text":"Using the cd command to change directories: # Navigate to your project directory cd ~/data_p166 The ~/ we use in our commands points to the home directory Using the mkdir command to make a directory. # Create standard project structure mkdir -p datasets # For input data mkdir -p results # For processed results mkdir -p shared_scripts # For project-specific scripts mkdir -p documentation # For project documentation The -p makes sure that any parent directories that don't already exist are created.","title":"Setting up a New Project Space"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#working-with-project-data","text":"# Create a workspace in scratch for processing mkdir -p ~/scratch/myanalysis cd ~/scratch/myanalysis # Copy input data to scratch for processing cp ~/data_p166/datasets/input.dat ./ # After processing is complete, save important results cp -r ./results ~/data_p166/results/analysis_20240319 # Clean up scratch space cd ~ rm -rf ~/scratch/myanalysis","title":"Working with Project Data"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#space-management","text":"Using the du command you can view storage usage. To monitor storage: # Check project space usage du -h /onyx/data/p166 \u2139\ufe0f The -h flag makes the sizes 'human readable' meaning it's converting them from bytes to MB/GB/TB # Check scratch usage du -hs ~/scratch \u2139\ufe0f The -s flag summarizes the storage usage of the hole directory. So if you want file by file usage, remove s .","title":"Space Management"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#data-safety","text":"Keep important data in project directories Use scratch for temporary processing only Regularly clean scratch space Document data organization for team members","title":"Data Safety"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#27-best-practices-summary","text":"Project Organization : Keep project data organized in project directories Use consistent structure across projects Data Management : Store shared data in project directories Use scratch for temporary processing Clean up scratch regularly Document organization for team members Job Workflow : Read input from project directories Process in scratch space Save results back to project directories Clean up scratch after job completion Collaboration : Use project directories for sharing Maintain consistent directory structure Document data organization Communicate changes with team members \u2139\ufe0f Remember that your project memberships determine both your compute resource allocation (via Slurm) and your access to shared storage spaces. Always specify the correct project ID in your Slurm jobs and organize your data accordingly.","title":"2.7. Best Practices Summary"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#28-transferring-files","text":"","title":"2.8. Transferring Files"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#281-before-starting","text":"Open a terminal on your local machine (not on Cyclone) Make sure you know: The full path of the file/directory on your local machine Where you want it to go on Cyclone (or vice versa) All commands below should be run from your local machine's terminal","title":"2.8.1. Before starting"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#282-using-scp-secure-copy","text":"","title":"2.8.2. Using SCP (Secure Copy):"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#transferring-from-your-local-machine-to-cyclone","text":"# Run this command on your local machine's terminal scp /path/on/your/local/machine/localfile.txt cyclone:~/destination/on/cyclone/","title":"Transferring FROM your local machine TO Cyclone:"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#transferring-from-cyclone-to-your-local-machine","text":"# Run this command on your local machine's terminal scp cyclone:~/path/on/cyclone/remotefile.txt /path/on/your/local/machine/ \ud83d\udca1 If you have not created an ssh config file, replace cyclone with username@cyclone.hpcf.cyi.ac.cy This is what it should look like:","title":"Transferring FROM Cyclone TO your local machine:"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#283-using-rsync-recommended-for-large-transfers","text":"","title":"2.8.3. Using Rsync (Recommended for Large Transfers)"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#copying-from-your-local-machine-to-cyclone","text":"# Run this on your local machine's terminal # The ./ refers to the current directory on your local machine rsync -avz ./local_directory/ cyclone:~/remote_directory/","title":"Copying FROM your local machine TO Cyclone:"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#copying-from-cyclone-to-your-local-machine","text":"# Run this on your local machine's terminal rsync -avz cyclone:~/remote_directory/ ./local_directory/ \ud83d\udca1 Note: All file transfer commands should be run from your local machine's terminal, not from within Cyclone. The paths before the colon (\\:) refer to your local machine, while paths after the colon refer to locations on Cyclone.","title":"Copying FROM Cyclone TO your local machine:"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#284-using-filezilla-graphical-interface","text":"Download and install FileZilla. Then, go to Edit \u2192 Settings Go to SFTP Add your key If your key is in OpenSSH format, you'll be prompted to convert your key. Press yes: Then input your passphrase: And then save the converted key. It's a good idea to keep it at the same place as the original in case you go looking for it in the future. Set up connection: Host: sftp://cyclone.hpcf.cyi.ac.cy Username: your_username Port: 22 Quick connect Once you're connected you'll see your local directory on the left and cyclone on the right. You can now just drag and drop between the two and the transfer wil happen automatically!","title":"2.8.4. Using FileZilla (Graphical Interface)"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#29-security-best-practices","text":"Use different keys for different services Regularly rotate keys (yearly) Always use strong passphrases Back up your private keys securely Never share private keys","title":"2.9. Security Best Practices"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#210-notes-and-troubleshooting","text":"","title":"2.10. Notes and Troubleshooting"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#2101-installing-required-tools","text":"If you don\u2019t have OpenSSH, WSL, or Git Bash installed, refer to the Installation Guide .","title":"2.10.1 Installing Required Tools"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#2102-ssh-directory-does-not-exist-on-windows","text":"","title":"2.10.2. ~/.ssh directory does not exist on Windows"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#option-1-using-powershell_1","text":"Open PowerShell. Press Windows Key > Search \"Windows PowerShell\" > Enter Navigate to your Home directory cd ~ This will take you to your home directory, typically something like C:\\Users\\<YourUsername> . Create the .ssh directory: mkdir .ssh Verify that the directory was created: ls .ssh If the .ssh folder exists, the command will list its contents (it may be empty if just created).","title":"Option 1: Using Powershell"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#option-2-using-file-explorer","text":"Open File Explorer. Navigate to your home directory: C:\\Users\\<YourUsername> . Create a new folder named .ssh : Right-click and choose New > Folder . Name it .ssh (include the period). Confirm if prompted about using a name that starts with a period.","title":"Option 2: Using File Explorer"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#2103-changing-and-removing-file-extensions-windows","text":"File extensions (like .txt , .png , .exe ) are often hidden by default in Windows, so you'll first need to make extensions visible before removing or changing them. Open File Explorer . Press Win + E or click the folder icon in the taskbar. Access View Options: Windows 10 : Click on the View tab in the toolbar at the top. Windows 11 : Click on the three dots ( ... ) in the toolbar at the top and choose Options . Click on the tab View , go to \"Advanced settings\" and uncheck the checkbox \"Hide extensions for known file types\" if already checked. (Now that the extension is visible) Rename the file: Right-click the file and choose Rename . Remove or modify the extension as needed. Confirm the change when prompted.","title":"2.10.3 Changing and Removing File Extensions (Windows)"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#2104-showunhide-ssh-directory-windows","text":"File extensions (like .txt , .png , .exe ) are often hidden by default in Windows, so you'll first need to make extensions visible before removing or changing them. 1. Open File Explorer . - Press Win + E or click the folder icon in the taskbar. 2. Access View Options: - Windows 10 : Click on the View tab in the toolbar at the top. - Windows 11 : Click on the three dots ( ... ) in the toolbar at the top and choose Options . 3. Click on the tab View , go to \"Advanced settings\" . 4. Scroll down to \"Hidden files and folders\" and select the option Show hidden files, folders, and drives .","title":"2.10.4. Show/Unhide .ssh directory (Windows)"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#2105-ssh-agent-issues-macoslinuxgit-bash","text":"To set Up SSH Agent Automatically, first open or create the ~/.bashrc file: nano ~/.bashrc Add these lines to the file: # Start SSH agent if not running if [ -z \"$SSH_AUTH_SOCK\" ] ; then eval `ssh-agent -s` > /dev/null fi Save, close the file (Ctrl+X, then Y, then Enter) and then reload the configuration: source ~/.bashrc","title":"2.10.5. SSH agent issues (MacOS/Linux/Git Bash)"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#2106-connection-issues-macoslinuxgit-bash","text":"First, check the SSH agent: ssh-add -l #This will list all of your added keys Then verify the relevant files and directories have the correct permissions: chmod 700 ~/.ssh chmod 600 ~/.ssh/id_rsa chmod 644 ~/.ssh/id_rsa.pub This makes sure your private/public key and the directory they are in have the correct read write and execute permissions for the ssh client to allow a connection. Finally, test connection with verbose output: ssh -v cyclone # Adding the `-v` flag prints debuging information","title":"2.10.6. Connection issues (MacOS/Linux/Git Bash)"},{"location":"tutorials/t02_accessing_and_navigating_cyclone/#2107-ssh-permission-issues-powershell","text":"If your key doesn't have the correct permissions: icacls <path-to-your-id_rsa> /inheritance:r /grant:r \"$($env:USERNAME):(F)\" \u26a0\ufe0f Remember to replace username with your actual Cyclone username in all examples.","title":"2.10.7 SSH Permission Issues (Powershell)"},{"location":"tutorials/t03_setting_up_and_using_development_tools/","text":"3. Setting up and Using Development Tools 3.1. Overview This tutorial introduces users to modern development tools and workflows that simplify working on HPC systems like Cyclone. Participants will learn to set up and configure tools like VS Code and MobaXTerm for remote file editing and code management directly on Cyclone. The session also covers how to use the module system to load software environments and how to extend functionality with tools like Conda or virtual environments (venv). By adopting these workflows, users will enhance their productivity and streamline their interactions with Cyclone. 3.2. Learning Objectives By the end of this tutorial, participants will be able to: Set up and configure modern development tools, such as VS Code (for all platforms) or MobaXTerm (for Windows), to remotely access Cyclone and edit files directly on the system. Understand how to use Cyclone\u2019s module system to load and manage software environments. Create and manage custom environments using tools like Conda or virtual environments (venv) to extend functionality and accommodate specific project needs. 3.3. Prerequisites T01 - Introduction to HPC Systems : This tutorial will give you some basic knowledge on HPC systems and basic terminologies. T02 - Accessing and Navigating Cyclone: This tutorial will give you some basic knowledge on how to connect, copy files and navigate the HPC system. 3.2. Tools for development In this section we will cover how to setup VS Code and MobaXTerm, so the user can connect to Cyclone,and start developing. 3.2.1. VS Code Visual Studio Code (VS Code) is a versatile code editor widely used for software development. With the Remote - SSH extension, it enables seamless connection to Cyclone, allowing users to edit, debug, and manage code directly on the HPC system. This eliminates the need for constant file transfers and provides a familiar development environment. By using VS Code, developers can streamline workflows and enhance productivity on Cyclone. Setting up VS Code Users should follow this link , and download the appropriate VS Code installer based on their OS. Then, follow the on-screen instuctions during the installation. Once the installation is finished, it is time to install some basic extensions. Depending on your code-base, you might want to install some code extensions such as Python . Furthermore, there are various extensions that provide extended support for code predictions or auto-completions. Feel free to browse the Extension Marketplace and download the ones you need! The only extension that is 100% mandatory for this tutorial is the Remote-SSH extension . Remote-SSH enables you to use any remote machine with an SSH server as your development environment. Go ahead and search Remote - SSH in the Extension Marketplace and install it. After you install the extension, you will see an extra button on the left side-panel and on the bottom left. If you don't, restart VS Code. When you click the extension's button, you might see on the top right of the panel a drop-down menu. Remote-ssh lets you connect to other systems as well, such as docker. For our use-case, we need to select the Remotes (Tunnels/SSH) option if it's not already selected. Connecting to Cyclone Now that we have Remote-SSH installed, it's time to set it up so it can establish a connection on Cyclone. If you followed the tutorials up until this point, you should have a private ssh key and a config file that lets you connect onto Cyclone through your terminal. We are going to use both to let VS Code connect onto Cyclone as well. \u26a0\ufe0f If not, please refer to Tutorial 02 for instructions on how to set this up. Let's break it into steps: Step 1: Go and add the following lines to your ssh config file. The config file is located at: Windows: %userprofile%/.ssh/ Linux & Mac: ~/.ssh/ Your SSH config file should get picked up by VS Code. You should see an option called cyclone on the left panel when you press the extension's button. When you hover over that option, you will see two buttons on the right: The first button will establish a connection on your current VS Code window The second button will open a new VS Code window and establish a connection on that Go ahead and click the first button. \u2139\ufe0f You might get a prompt to select the hosts (Cyclone) operating system. Go ahead and select Linux. After that, if everything is setup correctly, you will get a prompt asking for a passphrase . Go ahead and type your passphrase and press enter. That's it - you are now connected on Cyclone with VS Code!! Note: Do not click Open Folder yet. If you followed this tutorial series up to this point, you should be familiar on how Cyclone's file system is structured. Just to remind you, Cyclone has 3 different directory types: $HOME ( /nvme/h/<username> ): is the home directory of a user. Users should store their source code and build executables here. The home directory is limited in size. $DATA_projectid ( /nvme/h/<username>/data_<projectid> )): is the data directory of a user within the project shared directory. Users must change to this directory in their batch scripts to run their jobs. The data directory has a total maximum quota as allocated to each project. $SCRATCH ( /nvme/scratch/<username> ): is a temporary storage directory for data to reside. Month old contents of scratch directory are purged during the monthly maintenance windows. At the end of this tutorial, we are going to have a hands-on session, where we will use everything that we will learn. For this purpose, we are going to create a folder for our dummy project in our $HOME directory. Open VS Code's terminal or your prefered one. Then go into your $HOME directory and create a folder by typing: cd $HOME mkdir tutorial_03_project ls -l The ls -l command should show our newly created folder. Now click Open Folder , and select the folder that we just created. You will be prompted to insert your passphrase once more. After that, you will be able to see that folder on the left panel, when you click the remote-ssh extension button: From now on, you will be able to establish a remote-ssh connection straight to that folder. 3.2.2. MobaXTerm MobaXTerm is an all-in-one remote desktop and terminal solution designed for developers and system administrators. It provides a user-friendly interface for SSH connections, enabling easy access to Cyclone's HPC environment. With features like remote file editing, a built-in SFTP browser, and X11 forwarding, users can manage files, run graphical applications, and execute commands efficiently. MobaXTerm simplifies remote workflows, making it an excellent tool for working on Cyclone. Setting up MobaXTerm (Windows Only) Go to this URL, and download MobaXTerm. After you download the zip folder, extract it, and run the .msi file located inside the folder. If you download the portable edition, then just double click the downloaded file and it will open the application straight away. Follow the installer's steps until completion. Keep in mind, Administrative permission is required to install this software. Go ahead and launch the software. You might be prompted to allow MobaXTerm to access private networks, click allow. Launching an SSH session on MobaXTerm There's a button on the top left corner called Session . Click that, and then select the first option that reads SSH . Afterwards, fill the Remote Host ( cyclone.hpcf.cyi.ac.cy ), click Specify username and type your username . Lastly, we have to setup our SSH key. Click on Advanced SSH settings , then click Use Private Key , and then write the path of your private key for Cyclone. Click OK , you will then be prompted to insert your Passphrase . After that you are done! You have an established connection to Cyclone with MobaXTerm. This tool is very versitile and has a lot of functionality. Please visit their documentation page to read more about it. 3.3. Using Cyclone's Module System The OS of Cyclone is a minimal Linux installation . Software applications installed on the Cyclone, are available through the Module system. For basic overview of the Module system please refer at T01 - Introduction to HPC Systems . 3.3.1. Finding a module There are two ways to search for modules: module avail module spider The module avail command returns the names of all available modules. If you are a new user, it is recommended to use this command. Note: Some modules have the (D) next to them. This means they are the default module to be loaded when a version is not specified [marconstantinou@front02 ~]$ module avail ---------------------------------------------------------------------- /eb/modules/all ---------------------------------------------------------------------- 4ti2/1.6.10-GCC-13.2.0 XZ/5.4.5-GCCcore-13.3.0 (D) ABAQUS/2024 Xerces-C++/3.2.4-GCCcore-12.3.0 ANTLR/2.7.7-GCCcore-8.3.0-Java-11 Xvfb/21.1.3-GCCcore-11.3.0 ASE/3.22.1-foss-2022a YACS/0.1.8-GCCcore-11.3.0 ATK/2.38.0-GCCcore-11.3.0 Yasm/1.3.0-GCCcore-11.2.0 Abseil/20230125.3-GCCcore-12.3.0 Yasm/1.3.0-GCCcore-11.3.0 AmberTools/22.3-foss-2021b Yasm/1.3.0-GCCcore-12.3.0 AmberTools/22.3-foss-2022a (D) Yasm/1.3.0-GCCcore-13.2.0 (D) Anaconda3/2021.11 Z3/4.10.2-GCCcore-11.3.0 Anaconda3/2023.03-1 (D) Z3/4.12.2-GCCcore-12.3.0 (D) ... XZ/5.2.5-GCCcore-11.2.0 zstd/1.5.0-GCCcore-11.2.0 XZ/5.2.5-GCCcore-11.3.0 (L) zstd/1.5.2-GCCcore-11.3.0 XZ/5.2.7-GCCcore-12.2.0 zstd/1.5.2-GCCcore-12.2.0 XZ/5.4.2-GCCcore-12.3.0 zstd/1.5.5-GCCcore-12.3.0 XZ/5.4.4-GCCcore-13.2.0 zstd/1.5.5-GCCcore-13.2.0 (D) Where: Aliases: Aliases exist: foo/1.2.3 (1.2) means that \"module load foo/1.2\" will load foo/1.2.3 D: Default Module L: Module is loaded If the avail list is too long consider trying: \"module --default avail\" or \"ml -d av\" to just list the default modules. \"module overview\" or \"ml ov\" to display the number of modules for each name. Use \"module spider\" to find all possible modules and extensions. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\". The module spider command shows a lot more information about the modules. Users can use the avail command to find the exact name of the module they are looking for, and then use the spider command on the exact name, to find more information about it [marconstantinou@front02 ~]$ module spider --------------------------------------------------------------------------------------------------------------------------------------------------------- The following is a list of the modules and extensions currently available: --------------------------------------------------------------------------------------------------------------------------------------------------------- 4ti2: 4ti2/1.6.10-GCC-13.2.0 A software package for algebraic, geometric and combinatorial problems on linear spaces ABAQUS: ABAQUS/2024 Finite Element Analysis software for modeling, visualization and best-in-class implicit and explicit dynamics FEA. ANTLR: ANTLR/2.7.7-GCCcore-8.3.0-Java-11 ANTLR, ANother Tool for Language Recognition, (formerly PCCTS) is a language tool that provides a framework for constructing recognizers, compilers, and translators from grammatical descriptions containing Java, C#, C++, or Python actions. ASE: ASE/3.22.1-foss-2022a ASE is a python package providing an open source Atomic Simulation Environment in the Python scripting language. From version 3.20.1 we also include the ase-ext package, it contains optional reimplementations in C of functions in ASE. ASE uses it automatically when installed. ATK: ATK/2.38.0-GCCcore-11.3.0 ATK provides the set of accessibility interfaces that are implemented by other toolkits and applications. Using the ATK interfaces, accessibility tools have full access to view and control running applications. ... zlib: zlib/1.2.8, zlib/1.2.11-GCCcore-8.3.0, zlib/1.2.11-GCCcore-10.2.0, zlib/1.2.11-GCCcore-11.2.0, zlib/1.2.11, zlib/1.2.12-GCCcore-11.3.0, ... zlib is designed to be a free, general-purpose, legally unencumbered -- that is, not covered by any patents -- lossless data-compression library for use on virtually any computer hardware and operating system. zstd: zstd/1.5.0-GCCcore-11.2.0, zstd/1.5.2-GCCcore-11.3.0, zstd/1.5.2-GCCcore-12.2.0, zstd/1.5.5-GCCcore-12.3.0, zstd/1.5.5-GCCcore-13.2.0 Zstandard is a real-time compression algorithm, providing high compression ratios. It offers a very wide range of compression/speed trade-off, while being backed by a very fast decoder. It also offers a special mode for small data, called dictionary compression, and can create dictionaries from any sample set. --------------------------------------------------------------------------------------------------------------------------------------------------------- To learn more about a package execute: $ module spider Foo where \"Foo\" is the name of a module. To find detailed information about a particular package you must specify the version if there is more than one version: $ module spider Foo/11.1 --------------------------------------------------------------------------------------------------------------------------------------------------------- If we want to search more details about a specific package, such as Python v.3.10.8 , then we can do so as follows: [marconstantinou@front02 ~]$ module spider Python/3.10.8-GCCcore-12.2.0 --------------------------------------------------------------------------------------------------------------------------------------------------------- Python: Python/3.10.8-GCCcore-12.2.0 --------------------------------------------------------------------------------------------------------------------------------------------------------- Description: Python is a programming language that lets you work more quickly and integrate your systems more effectively. This module can be loaded directly: module load Python/3.10.8-GCCcore-12.2.0 Help: Description =========== Python is a programming language that lets you work more quickly and integrate your systems more effectively. More information ================ - Homepage: https://python.org/ Included extensions =================== alabaster-0.7.12, appdirs-1.4.4, asn1crypto-1.5.1, atomicwrites-1.4.1, attrs-22.1.0, Babel-2.11.0, backports.entry-points-selectable-1.2.0, backports.functools_lru_cache-1.6.4, bcrypt-4.0.1, bitstring-3.1.9, blist-1.3.6, CacheControl-0.12.11, cachy-0.3.0, certifi-2022.9.24, cffi-1.15.1, chardet-5.0.0, charset-normalizer-2.1.1, cleo-1.0.0a5, click-8.1.3, clikit-0.6.2, colorama-0.4.6, crashtest-0.3.1, cryptography-38.0.3, Cython-0.29.32, decorator-5.1.1, distlib-0.3.6, docopt-0.6.2, docutils-0.19, dulwich-0.20.50, ecdsa-0.18.0, editables-0.3, exceptiongroup-1.0.1, filelock-3.8.0, flit-3.8.0, flit_core-3.8.0, flit_scm-1.7.0, fsspec-2022.11.0, future-0.18.2, glob2-0.7, hatch_fancy_pypi_readme-22.8.0, hatch_vcs-0.2.0, hatchling-1.11.1, html5lib-1.1, idna-3.4, imagesize-1.4.1, importlib_metadata-5.0.0, importlib_resources-5.10.0, iniconfig-1.1.1, intervaltree-3.1.0, intreehooks-1.0, ipaddress-1.0.23, jaraco.classes-3.2.3, jeepney-0.8.0, Jinja2-3.1.2, joblib-1.2.0, jsonschema-4.17.0, keyring-23.11.0, keyrings.alt-4.2.0, liac-arff-2.5.0, lockfile-0.12.2, MarkupSafe-2.1.1, mock-4.0.3, more-itertools-9.0.0, msgpack-1.0.4, netaddr-0.8.0, netifaces-0.11.0, packaging-21.3, paramiko-2.12.0, pastel-0.2.1, pathlib2-2.3.7.post1, pathspec-0.10.1, pbr-5.11.0, pexpect-4.8.0, pip-22.3.1, pkginfo-1.8.3, platformdirs-2.5.3, pluggy-1.0.0, poetry-1.2.2, poetry- core-1.3.2, poetry_plugin_export-1.2.0, psutil-5.9.4, ptyprocess-0.7.0, py-1.11.0, py_expression_eval-0.3.14, pyasn1-0.4.8, pycparser-2.21, pycrypto-2.6.1, Pygments-2.13.0, pylev-1.4.0, PyNaCl-1.5.0, pyparsing-3.0.9, pyrsistent-0.19.2, pytest-7.2.0, python-dateutil-2.8.2, pytoml-0.1.21, pytz-2022.6, regex-2022.10.31, requests-2.28.1, requests-toolbelt-0.9.1, scandir-1.10.0, SecretStorage-3.3.3, semantic_version-2.10.0, setuptools-63.4.3, setuptools-rust-1.5.2, setuptools_scm-7.0.5, shellingham-1.5.0, simplegeneric-0.8.1, simplejson-3.17.6, six-1.16.0, snowballstemmer-2.2.0, sortedcontainers-2.4.0, Sphinx-5.3.0, sphinx-bootstrap- theme-0.8.1, sphinxcontrib-applehelp-1.0.2, sphinxcontrib-devhelp-1.0.2, sphinxcontrib-htmlhelp-2.0.0, sphinxcontrib-jsmath-1.0.1, sphinxcontrib- qthelp-1.0.3, sphinxcontrib-serializinghtml-1.1.5, sphinxcontrib- websupport-1.2.4, tabulate-0.9.0, threadpoolctl-3.1.0, toml-0.10.2, tomli-2.0.1, tomli_w-1.0.0, tomlkit-0.11.6, typing_extensions-4.4.0, ujson-5.5.0, urllib3-1.26.12, virtualenv-20.16.6, wcwidth-0.2.5, webencodings-0.5.1, wheel-0.38.4, xlrd-2.0.1, zipfile36-0.1.3, zipp-3.10.0 3.3.2. Managing modules Loading a module By using module load $MODULE_NAME you can load and use $MODULE_NAME . Example: Using Python v3.10.4 instead of system default ( v3.10.13 ) [marconstantinou@front02 ~]$ python -V # Check if Python is loaded Python 3.10.13 # Python v3.10.13 is loaded by default [marconstantinou@front02 ~]$ module avail python # This will print all available python modules ------------------------------- /eb/modules/all -------------------------------- ... Python/2.7.16-GCCcore-8.3.0 Python/2.7.18-GCCcore-11.2.0-bare Python/2.7.18-GCCcore-11.2.0 Python/3.7.4-GCCcore-8.3.0 Python/3.8.6-GCCcore-10.2.0 Python/3.9.6-GCCcore-11.2.0-bare Python/3.9.6-GCCcore-11.2.0 Python/3.10.4-GCCcore-11.3.0-bare Python/3.10.4-GCCcore-11.3.0 Python/3.10.8-GCCcore-12.2.0-bare Python/3.10.8-GCCcore-12.2.0 Python/3.11.3-GCCcore-12.3.0 Python/3.11.5-GCCcore-13.2.0 Python/3.12.3-GCCcore-13.3.0 (D) ... Where: D: Default Module If the avail list is too long consider trying: \"module --default avail\" or \"ml -d av\" to just list the default modules. \"module overview\" or \"ml ov\" to display the number of modules for each name. Use \"module spider\" to find all possible modules and extensions. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\". [marconstantinou@front02 ~]$ module load Python/3.10.4-GCCcore-11.3.0 # Load Python 3.10.4 [marconstantinou@front02 ~]$ python -V Python 3.10.4 Checking loaded modules To see what modules are already loaded and used in your environment, we can type: module list [marconstantinou@front02 ~]$ module list Currently Loaded Modules: 1) GCCcore/11.3.0 5) ncurses/6.3-GCCcore-11.3.0 9) XZ/5.2.5-GCCcore-11.3.0 13) Python/3.10.4-GCCcore-11.3.0 2) zlib/1.2.12-GCCcore-11.3.0 6) libreadline/8.1.2-GCCcore-11.3.0 10) GMP/6.2.1-GCCcore-11.3.0 3) binutils/2.38-GCCcore-11.3.0 7) Tcl/8.6.12-GCCcore-11.3.0 11) libffi/3.4.2-GCCcore-11.3.0 4) bzip2/1.0.8-GCCcore-11.3.0 8) SQLite/3.38.3-GCCcore-11.3.0 12) OpenSSL/1.1 This will show the currently loaded modules. Note here we also see Python/3.10.4-GCCcore-11.3.0 being available (along with any dependencies), following the previous example. Unload a module Following the previous example, to unload the loaded Python version, we can do so by using the module unload command. This will remove the specified module from the loaded modules of your environment. Example: Removing Python 3.10.4 From the Environment [marconstantinou@front02 ~]$ module unload Python/3.10.4-GCCcore-11.3.0 [marconstantinou@front02 ~]$ module list Currently Loaded Modules: 1) GCCcore/11.3.0 4) bzip2/1.0.8-GCCcore-11.3.0 7) Tcl/8.6.12-GCCcore-11.3.0 10) GMP/6.2.1-GCCcore-11.3.0 2) zlib/1.2.12-GCCcore-11.3.0 5) ncurses/6.3-GCCcore-11.3.0 8) SQLite/3.38.3-GCCcore-11.3.0 11) libffi/3.4.2-GCCcore-11.3.0 3) binutils/2.38-GCCcore-11.3.0 6) libreadline/8.1.2-GCCcore-11.3.0 9) XZ/5.2.5-GCCcore-11.3.0 12) OpenSSL/1.1 [marconstantinou@front02 ~]$ python -V Python 3.10.13 You can also switch from one version to another in one go via module swap : [marconstantinou@front02 ~]$ module swap Python/3.10.4-GCCcore-11.3.0 Python/3.12.3-GCCcore-13.3.0 The following have been reloaded with a version change: 1) GCCcore/11.3.0 => GCCcore/13.3.0 7) binutils/2.38-GCCcore-11.3.0 => binutils/2.42-GCCcore-13.3.0 2) OpenSSL/1.1 => OpenSSL/3 8) bzip2/1.0.8-GCCcore-11.3.0 => bzip2/1.0.8-GCCcore-13.3.0 3) Python/3.10.4-GCCcore-11.3.0 => Python/3.12.3-GCCcore-13.3.0 9) libffi/3.4.2-GCCcore-11.3.0 => libffi/3.4.5-GCCcore-13.3.0 4) SQLite/3.38.3-GCCcore-11.3.0 => SQLite/3.45.3-GCCcore-13.3.0 10) libreadline/8.1.2-GCCcore-11.3.0 => libreadline/8.2-GCCcore-13.3.0 5) Tcl/8.6.12-GCCcore-11.3.0 => Tcl/8.6.14-GCCcore-13.3.0 11) ncurses/6.3-GCCcore-11.3.0 => ncurses/6.5-GCCcore-13.3.0 6) XZ/5.2.5-GCCcore-11.3.0 => XZ/5.4.5-GCCcore-13.3.0 12) zlib/1.2.12-GCCcore-11.3.0 => zlib/1.3.1-GCCcore-13.3.0 Note: The module system takes care of any relevant dependencies to the requested module we are trying to load. Therefore, switching from one version to another, the correct version of each dependency is reloaded automatically. Reset loaded modules To start from a clean environemnt we can use the purge command. module purge This will unload all the modules: [marconstantinou@front02 ~]$ module list Currently Loaded Modules: 1) GMP/6.2.1-GCCcore-11.3.0 5) bzip2/1.0.8-GCCcore-13.3.0 9) SQLite/3.45.3-GCCcore-13.3.0 13) Python/3.12.3-GCCcore-13.3.0 2) GCCcore/13.3.0 6) ncurses/6.5-GCCcore-13.3.0 10) XZ/5.4.5-GCCcore-13.3.0 3) zlib/1.3.1-GCCcore-13.3.0 7) libreadline/8.2-GCCcore-13.3.0 11) libffi/3.4.5-GCCcore-13.3.0 4) binutils/2.42-GCCcore-13.3.0 8) Tcl/8.6.14-GCCcore-13.3.0 12) OpenSSL/3 [marconstantinou@front02 ~]$ module purge [marconstantinou@front02 ~]$ module list No modules loaded Note: It is recommended to use before loading any modules, because modules loaded on your login environment are curried over to your job environment. This is a good way to make sure that there are no left over modules. 3.4. Using conda to Manage Python Virtual Environments conda is an open-source package management and environment management system widely used in data science, machine learning, and software development. It simplifies the installation, updating, and management of software packages and their dependencies across various programming languages, including Python , R , and C++ . conda also enables users to create isolated environments, allowing them to work on multiple projects with different dependencies without conflicts . It supports a wide range of operating systems and can manage libraries for scientific computing, data analysis, and machine learning efficiently. The official documentation for Anaconda can be found here . 3.4.1. Creating an environment You can create an empty environment by running: # Replace <ENV_NAME> with a name for your environment [marconstantinou@front02 ~]$ conda create -n <ENV_NAME> or if you want to create an environment with Python and other packages, run this: # Replace <ENV_NAME> with a name for your environment # Replace <PACKAGE> with your desired package # Replace <VERSION> with your desired version of Python [marconstantinou@front02 ~]$ conda create -n <ENV_NAME> python=<VERSION> <PACKAGE>=<VERSION> Example: Creating an environment with Python 3.11 and other specific libraries conda create -n myenv python=3.11 beautifulsoup4 docutils jinja2=3.1.4 wheel 3.4.2. Activating an environment To activate your environment, simply run: # Replace <ENV_NAME> with the name of the environment you want to activate [marconstantinou@front02 ~]$ conda activate <ENV_NAME> (ENV_NAME)[marconstantinou@front02 ~]$ You should now be able to see (ENV_NAME) next to your user name. 3.4.3. Switching between environments If you want to switch to a different environment you can run these commands: # Show all available environments (base)[marconstantinou@front02 ~]$ conda info --envs # conda environments: # base * /nvme/h/buildsets/eb_cyclone_rl/software/Anaconda3/2023.03-1 juplab /nvme/h/marconstantinou/.conda/envs/juplab myenv /nvme/h/marconstantinou/.conda/envs/myenv # Replace <ENV_NAME> with the name of the environment you want to switch to (base)[marconstantinou@front02 ~]$ conda activate <ENV_NAME> (ENV_NAME)[marconstantinou@front02 ~]$ Note: Activating a different environment will deactivate your current one. 3.4.4. Deactivating an environment To deactivate your current environment, simply type: (ENV_NAME)[marconstantinou@front02 ~]$ conda deactivate (base)[marconstantinou@front02 ~]$ Note: Upon deactivation, the environment will switch to the base acting as the default environment. If you want to deactivate conda completely, repeat conda deactivate . 3.4.5. Exporting an environment Do not attempt to copy the environment's files to a different machine. It will not recreate the environment. We must export the environment and install it again. To export the environment, simply run: # Replace <ENV_NAME> with the name of the environment you want exported (base)[marconstantinou@front02 ~]$ conda activate <ENV_NAME> (ENV_NAME)[marconstantinou@front02 ~]$ conda env export > environment.yml Note: This will handle both conda and pip 's packages. In your current directory you should see a file called environment.yml . You can take this file to the machine you want to export the environment and run: (base)[marconstantinou@front02 ~]$ conda env create -f environment.yml After this, you can activate the environment and use it as is. 3.5. Using venv to manage Python virtual environments The venv module in Python is a tool to create isolated environments for your projects. This ensures that dependencies for one project don\u2019t interfere with those of another. Here's how to get started with venv . venv and conda both create isolated environments for Python projects, but they differ in scope and functionality. venv is a lightweight, Python-specific tool for isolating packages installed via pip , relying on the system's Python interpreter. In contrast, conda is a cross-language package and environment manager that can handle both Python and non-Python dependencies, including system libraries, and comes with its own Python interpreter. While venv is simple and built into Python, conda is more feature-rich, making it ideal for data science and projects with complex dependencies. 3.5.1. Creating a virtual environment Navigate to your project's directory: [marconstantinou@front02 ~]$ cd /path/to/your/project Create a venv environment: # REPLACE venv_name with the name you want to give to your environment [marconstantinou@front02 ~]$ python -m venv venv_name 3.5.2. Activating a virtual environment To start using the virtual environment, you need to activate it via: [marconstantinou@front02 ~]$ source venv_name/bin/activate (venv_name)[marconstantinou@front02 ~]$ 3.5.3. Installing packages Once you activate the environment you can start installing packages via simply running: pip install package_name Example: Installing numpy [marconstantinou@front02 ~]$ pip install numpy (venv_name) [marconstantinou@front02 ~]$ pip install numpy Collecting numpy Downloading numpy-2.2.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB) \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 62.0/62.0 kB 1.2 MB/s eta 0:00:00 Downloading numpy-2.2.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.1 MB) \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 16.1/16.1 MB 22.2 MB/s eta 0:00:00 Installing collected packages: numpy Successfully installed numpy-2.2.2 You can see what packages you have installed by typing: (venv_name) [marconstantinou@front02 ~]$ pip list Package Version ------- ------- numpy 2.2.2 pip 24.0 3.5.4. Deleting a virtual environment To delete the environment simply type: (venv_name) [marconstantinou@front02 ~]$ rm -rf venv_name 3.6. Hands-on Exercise In this exercise, we will: connect to Cyclone using VS Code (you can use whatever interface you prefer) create a directory for our small project load some modules create a virtual environment for our python scripts and finally we will run a small python file. 3.6.1. Setup our project's directory Open up VS Code and connect to Cyclone. Once connected, go to the terminal and into your $HOME dir. Note: Please keep in mind that you can do all these by using the graphical interface of VS Code - if you prefer that way, then go ahead and do that. [marconstantinou@front02 ~]$ pwd /nvme/h/marconstantinou If you are not in your $HOME directory, type: [marconstantinou@front02 ~]$ cd $HOME To create our hands-on directory, type: [marconstantinou@front02 ~]$ mkdir tutorial_03_project/ If you followed all the steps until now, you should see a tutorial_03_project directory when you type this command: [marconstantinou@front02 ~]$ ls -l total 2 lrwxrwxrwx 1 marconstantinou p232 15 Nov 4 10:20 data_p232 -> /onyx/data/p232 lrwxrwxrwx 1 marconstantinou p232 19 Nov 4 11:22 edu24 -> /onyx/data/edu24 lrwxrwxrwx 1 marconstantinou p232 29 Nov 4 10:20 scratch -> /nvme/scratch/marconstantinou drwxr-xr-x 2 marconstantinou p232 0 Dec 10 15:26 tutorial_03_project Go inside that directory: [marconstantinou@front02 ~]$ cd tutorial_03_project/ Then create a main.py file: [marconstantinou@front02 ~]$ touch main.py [marconstantinou@front02 ~]$ ls -l -rw-r--r-- 1 marconstantinou p232 0 Dec 19 11:40 main.py Now that we have our directory and script ready, lets load some module and install some packages. 3.6.2. Load CUDA module Again, in your terminal connected to Cyclone, type: [marconstantinou@front02 ~]$ module avail CUDA ------------------------------------------- /eb/modules/all -------------------------------------------- CUDA/10.1.243 CUDA/11.4.1 CUDA/11.7.0 CUDA/11.8.0 CUDA/12.0.0 CUDA/12.1.0 CUDA/12.1.1 CUDA/12.6.0 (D) ... Where: D: Default Module If the avail list is too long consider trying: \"module --default avail\" or \"ml -d av\" to just list the default modules. \"module overview\" or \"ml ov\" to display the number of modules for each name. Use \"module spider\" to find all possible modules and extensions. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\". Let's load CUDA/12.1.1 : [marconstantinou@front02 ~]$ module load CUDA/12.1.1 3.6.3. Create a conda environment for our project and install some libraries We are going to create a conda virtual environment, and install PyTorch which is a Deep Learning framework. First we need to load Anaconda using the module system: [marconstantinou@front02 ~]$ module load Anaconda3/2023.03-1 Then run: [marconstantinou@front02 ~]$ conda init Now restart your terminal. When you open a new terminal and connect to Cyclone, you should be able to create an environment and activate it. To do this, type: (base)[marconstantinou@front02 ~]$ conda create --name dummy_proj pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia Note: This will take some time to install, be patient. After everything is installed, go ahead and request a single GPU for 30 minutes on GPU partition of Cyclone by typing: # Start a 30 minute interactive session on a GPU node (base)[marconstantinou@front02 ~]$ srun --partition=gpu --gres=gpu:1 --time=00:30:00 --pty bash You might see something like this: srun: job 1037413 queued and waiting for resources This mean that cyclone is being utilized, and that our job is in queue until resourcers are found. Please give SLURM some time to allocate some resources to our request. Once the allocation of resources is successful, the front02 hostname shown on the terminal next to your username will be changed to the allocated node. In this case, the allocated node is gpu01 (Note it might differ from yours!). Then we want to activate our environment: (base)[marconstantinou@gpu01 ~]$ conda activate dummy_proj You can check which libraries are installed in your conda environment by typing (dummy_proj)[marconstantinou@gpu01 ~]$ conda list 3.6.4. Select the correct Python Interpreter in VS Code On the top of VS Code, click and type the following: > Python: Select Interpreter Click enter and then select the newly created conda environment: 3.6.5. Running a python script on a GPU Node Copy and paste the following code snippet inside the main.py file that we created earlier. This file should be inside the directory tutorial_03_project import sys import torch def check_pytorch_installation(): try: # Check if PyTorch is installed print(f\"PyTorch version: {torch.__version__}\") except ImportError: print(\"PyTorch is not installed.\") sys.exit(1) def check_cuda_support(): if torch.cuda.is_available(): print(\"CUDA is available!\") print(f\"CUDA version: {torch.version.cuda}\") print(f\"Number of GPUs available: {torch.cuda.device_count()}\") print(f\"GPU Name: {torch.cuda.get_device_name(0)}\") else: print(\"CUDA is not available. Please ensure that your system has a compatible GPU and CUDA setup.\") def main(): print(\"Checking PyTorch and CUDA installation...\") check_pytorch_installation() check_cuda_support() print(\"Verification complete.\") if __name__ == \"__main__\": main() Now save it, and inside your terminal run: (dummy_proj)[marconstantinou@gpu01 ~]$ python main.py Note: The above command assumes you are in the directory of the main.py file. If you are in a different directory, you will have to edit the above command. You should see the following: Checking PyTorch and CUDA installation... PyTorch version: 2.5.1 CUDA is available! CUDA version: 12.1 Number of GPUs available: 1 GPU Name: Tesla V100-SXM2-32GB Verification complete. And that's all! You created a directory for your project, loaded some modules, created a conda environment with some libraries, and then you run some python code on a GPU Node. 3.6.6. Closing our interactive job session Inside your terminal go ahead and type: (dummy_proj)[marconstantinou@gpu01 ~]$ squeue --me You should see 1 job with a JOBID , copy that JOBID and type: # Replace JOBID with the ID of your job that you just copied (dummy_proj)[marconstantinou@gpu01 ~]$ scancel JOBID # Example (dummy_proj)[marconstantinou@gpu01 ~]$ scancel 1037152","title":"Setting up and Using Development tools"},{"location":"tutorials/t03_setting_up_and_using_development_tools/#3-setting-up-and-using-development-tools","text":"","title":"3. Setting up and Using Development Tools"},{"location":"tutorials/t03_setting_up_and_using_development_tools/#31-overview","text":"This tutorial introduces users to modern development tools and workflows that simplify working on HPC systems like Cyclone. Participants will learn to set up and configure tools like VS Code and MobaXTerm for remote file editing and code management directly on Cyclone. The session also covers how to use the module system to load software environments and how to extend functionality with tools like Conda or virtual environments (venv). By adopting these workflows, users will enhance their productivity and streamline their interactions with Cyclone.","title":"3.1. Overview"},{"location":"tutorials/t03_setting_up_and_using_development_tools/#32-learning-objectives","text":"By the end of this tutorial, participants will be able to: Set up and configure modern development tools, such as VS Code (for all platforms) or MobaXTerm (for Windows), to remotely access Cyclone and edit files directly on the system. Understand how to use Cyclone\u2019s module system to load and manage software environments. Create and manage custom environments using tools like Conda or virtual environments (venv) to extend functionality and accommodate specific project needs.","title":"3.2. Learning Objectives"},{"location":"tutorials/t03_setting_up_and_using_development_tools/#33-prerequisites","text":"T01 - Introduction to HPC Systems : This tutorial will give you some basic knowledge on HPC systems and basic terminologies. T02 - Accessing and Navigating Cyclone: This tutorial will give you some basic knowledge on how to connect, copy files and navigate the HPC system.","title":"3.3. Prerequisites"},{"location":"tutorials/t03_setting_up_and_using_development_tools/#32-tools-for-development","text":"In this section we will cover how to setup VS Code and MobaXTerm, so the user can connect to Cyclone,and start developing.","title":"3.2. Tools for development"},{"location":"tutorials/t03_setting_up_and_using_development_tools/#321-vs-code","text":"Visual Studio Code (VS Code) is a versatile code editor widely used for software development. With the Remote - SSH extension, it enables seamless connection to Cyclone, allowing users to edit, debug, and manage code directly on the HPC system. This eliminates the need for constant file transfers and provides a familiar development environment. By using VS Code, developers can streamline workflows and enhance productivity on Cyclone.","title":"3.2.1. VS Code"},{"location":"tutorials/t03_setting_up_and_using_development_tools/#setting-up-vs-code","text":"Users should follow this link , and download the appropriate VS Code installer based on their OS. Then, follow the on-screen instuctions during the installation. Once the installation is finished, it is time to install some basic extensions. Depending on your code-base, you might want to install some code extensions such as Python . Furthermore, there are various extensions that provide extended support for code predictions or auto-completions. Feel free to browse the Extension Marketplace and download the ones you need! The only extension that is 100% mandatory for this tutorial is the Remote-SSH extension . Remote-SSH enables you to use any remote machine with an SSH server as your development environment. Go ahead and search Remote - SSH in the Extension Marketplace and install it. After you install the extension, you will see an extra button on the left side-panel and on the bottom left. If you don't, restart VS Code. When you click the extension's button, you might see on the top right of the panel a drop-down menu. Remote-ssh lets you connect to other systems as well, such as docker. For our use-case, we need to select the Remotes (Tunnels/SSH) option if it's not already selected.","title":"Setting up VS Code"},{"location":"tutorials/t03_setting_up_and_using_development_tools/#connecting-to-cyclone","text":"Now that we have Remote-SSH installed, it's time to set it up so it can establish a connection on Cyclone. If you followed the tutorials up until this point, you should have a private ssh key and a config file that lets you connect onto Cyclone through your terminal. We are going to use both to let VS Code connect onto Cyclone as well. \u26a0\ufe0f If not, please refer to Tutorial 02 for instructions on how to set this up. Let's break it into steps:","title":"Connecting to Cyclone"},{"location":"tutorials/t03_setting_up_and_using_development_tools/#step-1-go-and-add-the-following-lines-to-your-ssh-config-file","text":"The config file is located at: Windows: %userprofile%/.ssh/ Linux & Mac: ~/.ssh/ Your SSH config file should get picked up by VS Code. You should see an option called cyclone on the left panel when you press the extension's button. When you hover over that option, you will see two buttons on the right: The first button will establish a connection on your current VS Code window The second button will open a new VS Code window and establish a connection on that Go ahead and click the first button. \u2139\ufe0f You might get a prompt to select the hosts (Cyclone) operating system. Go ahead and select Linux. After that, if everything is setup correctly, you will get a prompt asking for a passphrase . Go ahead and type your passphrase and press enter. That's it - you are now connected on Cyclone with VS Code!! Note: Do not click Open Folder yet. If you followed this tutorial series up to this point, you should be familiar on how Cyclone's file system is structured. Just to remind you, Cyclone has 3 different directory types: $HOME ( /nvme/h/<username> ): is the home directory of a user. Users should store their source code and build executables here. The home directory is limited in size. $DATA_projectid ( /nvme/h/<username>/data_<projectid> )): is the data directory of a user within the project shared directory. Users must change to this directory in their batch scripts to run their jobs. The data directory has a total maximum quota as allocated to each project. $SCRATCH ( /nvme/scratch/<username> ): is a temporary storage directory for data to reside. Month old contents of scratch directory are purged during the monthly maintenance windows. At the end of this tutorial, we are going to have a hands-on session, where we will use everything that we will learn. For this purpose, we are going to create a folder for our dummy project in our $HOME directory. Open VS Code's terminal or your prefered one. Then go into your $HOME directory and create a folder by typing: cd $HOME mkdir tutorial_03_project ls -l The ls -l command should show our newly created folder. Now click Open Folder , and select the folder that we just created. You will be prompted to insert your passphrase once more. After that, you will be able to see that folder on the left panel, when you click the remote-ssh extension button: From now on, you will be able to establish a remote-ssh connection straight to that folder.","title":"Step 1: Go and add the following lines to your ssh config file."},{"location":"tutorials/t03_setting_up_and_using_development_tools/#322-mobaxterm","text":"MobaXTerm is an all-in-one remote desktop and terminal solution designed for developers and system administrators. It provides a user-friendly interface for SSH connections, enabling easy access to Cyclone's HPC environment. With features like remote file editing, a built-in SFTP browser, and X11 forwarding, users can manage files, run graphical applications, and execute commands efficiently. MobaXTerm simplifies remote workflows, making it an excellent tool for working on Cyclone.","title":"3.2.2. MobaXTerm"},{"location":"tutorials/t03_setting_up_and_using_development_tools/#setting-up-mobaxterm-windows-only","text":"Go to this URL, and download MobaXTerm. After you download the zip folder, extract it, and run the .msi file located inside the folder. If you download the portable edition, then just double click the downloaded file and it will open the application straight away. Follow the installer's steps until completion. Keep in mind, Administrative permission is required to install this software. Go ahead and launch the software. You might be prompted to allow MobaXTerm to access private networks, click allow.","title":"Setting up MobaXTerm (Windows Only)"},{"location":"tutorials/t03_setting_up_and_using_development_tools/#launching-an-ssh-session-on-mobaxterm","text":"There's a button on the top left corner called Session . Click that, and then select the first option that reads SSH . Afterwards, fill the Remote Host ( cyclone.hpcf.cyi.ac.cy ), click Specify username and type your username . Lastly, we have to setup our SSH key. Click on Advanced SSH settings , then click Use Private Key , and then write the path of your private key for Cyclone. Click OK , you will then be prompted to insert your Passphrase . After that you are done! You have an established connection to Cyclone with MobaXTerm. This tool is very versitile and has a lot of functionality. Please visit their documentation page to read more about it.","title":"Launching an SSH session on MobaXTerm"},{"location":"tutorials/t03_setting_up_and_using_development_tools/#33-using-cyclones-module-system","text":"The OS of Cyclone is a minimal Linux installation . Software applications installed on the Cyclone, are available through the Module system. For basic overview of the Module system please refer at T01 - Introduction to HPC Systems .","title":"3.3. Using Cyclone's Module System"},{"location":"tutorials/t03_setting_up_and_using_development_tools/#331-finding-a-module","text":"There are two ways to search for modules: module avail module spider The module avail command returns the names of all available modules. If you are a new user, it is recommended to use this command. Note: Some modules have the (D) next to them. This means they are the default module to be loaded when a version is not specified [marconstantinou@front02 ~]$ module avail ---------------------------------------------------------------------- /eb/modules/all ---------------------------------------------------------------------- 4ti2/1.6.10-GCC-13.2.0 XZ/5.4.5-GCCcore-13.3.0 (D) ABAQUS/2024 Xerces-C++/3.2.4-GCCcore-12.3.0 ANTLR/2.7.7-GCCcore-8.3.0-Java-11 Xvfb/21.1.3-GCCcore-11.3.0 ASE/3.22.1-foss-2022a YACS/0.1.8-GCCcore-11.3.0 ATK/2.38.0-GCCcore-11.3.0 Yasm/1.3.0-GCCcore-11.2.0 Abseil/20230125.3-GCCcore-12.3.0 Yasm/1.3.0-GCCcore-11.3.0 AmberTools/22.3-foss-2021b Yasm/1.3.0-GCCcore-12.3.0 AmberTools/22.3-foss-2022a (D) Yasm/1.3.0-GCCcore-13.2.0 (D) Anaconda3/2021.11 Z3/4.10.2-GCCcore-11.3.0 Anaconda3/2023.03-1 (D) Z3/4.12.2-GCCcore-12.3.0 (D) ... XZ/5.2.5-GCCcore-11.2.0 zstd/1.5.0-GCCcore-11.2.0 XZ/5.2.5-GCCcore-11.3.0 (L) zstd/1.5.2-GCCcore-11.3.0 XZ/5.2.7-GCCcore-12.2.0 zstd/1.5.2-GCCcore-12.2.0 XZ/5.4.2-GCCcore-12.3.0 zstd/1.5.5-GCCcore-12.3.0 XZ/5.4.4-GCCcore-13.2.0 zstd/1.5.5-GCCcore-13.2.0 (D) Where: Aliases: Aliases exist: foo/1.2.3 (1.2) means that \"module load foo/1.2\" will load foo/1.2.3 D: Default Module L: Module is loaded If the avail list is too long consider trying: \"module --default avail\" or \"ml -d av\" to just list the default modules. \"module overview\" or \"ml ov\" to display the number of modules for each name. Use \"module spider\" to find all possible modules and extensions. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\". The module spider command shows a lot more information about the modules. Users can use the avail command to find the exact name of the module they are looking for, and then use the spider command on the exact name, to find more information about it [marconstantinou@front02 ~]$ module spider --------------------------------------------------------------------------------------------------------------------------------------------------------- The following is a list of the modules and extensions currently available: --------------------------------------------------------------------------------------------------------------------------------------------------------- 4ti2: 4ti2/1.6.10-GCC-13.2.0 A software package for algebraic, geometric and combinatorial problems on linear spaces ABAQUS: ABAQUS/2024 Finite Element Analysis software for modeling, visualization and best-in-class implicit and explicit dynamics FEA. ANTLR: ANTLR/2.7.7-GCCcore-8.3.0-Java-11 ANTLR, ANother Tool for Language Recognition, (formerly PCCTS) is a language tool that provides a framework for constructing recognizers, compilers, and translators from grammatical descriptions containing Java, C#, C++, or Python actions. ASE: ASE/3.22.1-foss-2022a ASE is a python package providing an open source Atomic Simulation Environment in the Python scripting language. From version 3.20.1 we also include the ase-ext package, it contains optional reimplementations in C of functions in ASE. ASE uses it automatically when installed. ATK: ATK/2.38.0-GCCcore-11.3.0 ATK provides the set of accessibility interfaces that are implemented by other toolkits and applications. Using the ATK interfaces, accessibility tools have full access to view and control running applications. ... zlib: zlib/1.2.8, zlib/1.2.11-GCCcore-8.3.0, zlib/1.2.11-GCCcore-10.2.0, zlib/1.2.11-GCCcore-11.2.0, zlib/1.2.11, zlib/1.2.12-GCCcore-11.3.0, ... zlib is designed to be a free, general-purpose, legally unencumbered -- that is, not covered by any patents -- lossless data-compression library for use on virtually any computer hardware and operating system. zstd: zstd/1.5.0-GCCcore-11.2.0, zstd/1.5.2-GCCcore-11.3.0, zstd/1.5.2-GCCcore-12.2.0, zstd/1.5.5-GCCcore-12.3.0, zstd/1.5.5-GCCcore-13.2.0 Zstandard is a real-time compression algorithm, providing high compression ratios. It offers a very wide range of compression/speed trade-off, while being backed by a very fast decoder. It also offers a special mode for small data, called dictionary compression, and can create dictionaries from any sample set. --------------------------------------------------------------------------------------------------------------------------------------------------------- To learn more about a package execute: $ module spider Foo where \"Foo\" is the name of a module. To find detailed information about a particular package you must specify the version if there is more than one version: $ module spider Foo/11.1 --------------------------------------------------------------------------------------------------------------------------------------------------------- If we want to search more details about a specific package, such as Python v.3.10.8 , then we can do so as follows: [marconstantinou@front02 ~]$ module spider Python/3.10.8-GCCcore-12.2.0 --------------------------------------------------------------------------------------------------------------------------------------------------------- Python: Python/3.10.8-GCCcore-12.2.0 --------------------------------------------------------------------------------------------------------------------------------------------------------- Description: Python is a programming language that lets you work more quickly and integrate your systems more effectively. This module can be loaded directly: module load Python/3.10.8-GCCcore-12.2.0 Help: Description =========== Python is a programming language that lets you work more quickly and integrate your systems more effectively. More information ================ - Homepage: https://python.org/ Included extensions =================== alabaster-0.7.12, appdirs-1.4.4, asn1crypto-1.5.1, atomicwrites-1.4.1, attrs-22.1.0, Babel-2.11.0, backports.entry-points-selectable-1.2.0, backports.functools_lru_cache-1.6.4, bcrypt-4.0.1, bitstring-3.1.9, blist-1.3.6, CacheControl-0.12.11, cachy-0.3.0, certifi-2022.9.24, cffi-1.15.1, chardet-5.0.0, charset-normalizer-2.1.1, cleo-1.0.0a5, click-8.1.3, clikit-0.6.2, colorama-0.4.6, crashtest-0.3.1, cryptography-38.0.3, Cython-0.29.32, decorator-5.1.1, distlib-0.3.6, docopt-0.6.2, docutils-0.19, dulwich-0.20.50, ecdsa-0.18.0, editables-0.3, exceptiongroup-1.0.1, filelock-3.8.0, flit-3.8.0, flit_core-3.8.0, flit_scm-1.7.0, fsspec-2022.11.0, future-0.18.2, glob2-0.7, hatch_fancy_pypi_readme-22.8.0, hatch_vcs-0.2.0, hatchling-1.11.1, html5lib-1.1, idna-3.4, imagesize-1.4.1, importlib_metadata-5.0.0, importlib_resources-5.10.0, iniconfig-1.1.1, intervaltree-3.1.0, intreehooks-1.0, ipaddress-1.0.23, jaraco.classes-3.2.3, jeepney-0.8.0, Jinja2-3.1.2, joblib-1.2.0, jsonschema-4.17.0, keyring-23.11.0, keyrings.alt-4.2.0, liac-arff-2.5.0, lockfile-0.12.2, MarkupSafe-2.1.1, mock-4.0.3, more-itertools-9.0.0, msgpack-1.0.4, netaddr-0.8.0, netifaces-0.11.0, packaging-21.3, paramiko-2.12.0, pastel-0.2.1, pathlib2-2.3.7.post1, pathspec-0.10.1, pbr-5.11.0, pexpect-4.8.0, pip-22.3.1, pkginfo-1.8.3, platformdirs-2.5.3, pluggy-1.0.0, poetry-1.2.2, poetry- core-1.3.2, poetry_plugin_export-1.2.0, psutil-5.9.4, ptyprocess-0.7.0, py-1.11.0, py_expression_eval-0.3.14, pyasn1-0.4.8, pycparser-2.21, pycrypto-2.6.1, Pygments-2.13.0, pylev-1.4.0, PyNaCl-1.5.0, pyparsing-3.0.9, pyrsistent-0.19.2, pytest-7.2.0, python-dateutil-2.8.2, pytoml-0.1.21, pytz-2022.6, regex-2022.10.31, requests-2.28.1, requests-toolbelt-0.9.1, scandir-1.10.0, SecretStorage-3.3.3, semantic_version-2.10.0, setuptools-63.4.3, setuptools-rust-1.5.2, setuptools_scm-7.0.5, shellingham-1.5.0, simplegeneric-0.8.1, simplejson-3.17.6, six-1.16.0, snowballstemmer-2.2.0, sortedcontainers-2.4.0, Sphinx-5.3.0, sphinx-bootstrap- theme-0.8.1, sphinxcontrib-applehelp-1.0.2, sphinxcontrib-devhelp-1.0.2, sphinxcontrib-htmlhelp-2.0.0, sphinxcontrib-jsmath-1.0.1, sphinxcontrib- qthelp-1.0.3, sphinxcontrib-serializinghtml-1.1.5, sphinxcontrib- websupport-1.2.4, tabulate-0.9.0, threadpoolctl-3.1.0, toml-0.10.2, tomli-2.0.1, tomli_w-1.0.0, tomlkit-0.11.6, typing_extensions-4.4.0, ujson-5.5.0, urllib3-1.26.12, virtualenv-20.16.6, wcwidth-0.2.5, webencodings-0.5.1, wheel-0.38.4, xlrd-2.0.1, zipfile36-0.1.3, zipp-3.10.0","title":"3.3.1. Finding a module"},{"location":"tutorials/t03_setting_up_and_using_development_tools/#332-managing-modules","text":"","title":"3.3.2. Managing modules"},{"location":"tutorials/t03_setting_up_and_using_development_tools/#loading-a-module","text":"By using module load $MODULE_NAME you can load and use $MODULE_NAME . Example: Using Python v3.10.4 instead of system default ( v3.10.13 ) [marconstantinou@front02 ~]$ python -V # Check if Python is loaded Python 3.10.13 # Python v3.10.13 is loaded by default [marconstantinou@front02 ~]$ module avail python # This will print all available python modules ------------------------------- /eb/modules/all -------------------------------- ... Python/2.7.16-GCCcore-8.3.0 Python/2.7.18-GCCcore-11.2.0-bare Python/2.7.18-GCCcore-11.2.0 Python/3.7.4-GCCcore-8.3.0 Python/3.8.6-GCCcore-10.2.0 Python/3.9.6-GCCcore-11.2.0-bare Python/3.9.6-GCCcore-11.2.0 Python/3.10.4-GCCcore-11.3.0-bare Python/3.10.4-GCCcore-11.3.0 Python/3.10.8-GCCcore-12.2.0-bare Python/3.10.8-GCCcore-12.2.0 Python/3.11.3-GCCcore-12.3.0 Python/3.11.5-GCCcore-13.2.0 Python/3.12.3-GCCcore-13.3.0 (D) ... Where: D: Default Module If the avail list is too long consider trying: \"module --default avail\" or \"ml -d av\" to just list the default modules. \"module overview\" or \"ml ov\" to display the number of modules for each name. Use \"module spider\" to find all possible modules and extensions. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\". [marconstantinou@front02 ~]$ module load Python/3.10.4-GCCcore-11.3.0 # Load Python 3.10.4 [marconstantinou@front02 ~]$ python -V Python 3.10.4","title":"Loading a module"},{"location":"tutorials/t03_setting_up_and_using_development_tools/#checking-loaded-modules","text":"To see what modules are already loaded and used in your environment, we can type: module list [marconstantinou@front02 ~]$ module list Currently Loaded Modules: 1) GCCcore/11.3.0 5) ncurses/6.3-GCCcore-11.3.0 9) XZ/5.2.5-GCCcore-11.3.0 13) Python/3.10.4-GCCcore-11.3.0 2) zlib/1.2.12-GCCcore-11.3.0 6) libreadline/8.1.2-GCCcore-11.3.0 10) GMP/6.2.1-GCCcore-11.3.0 3) binutils/2.38-GCCcore-11.3.0 7) Tcl/8.6.12-GCCcore-11.3.0 11) libffi/3.4.2-GCCcore-11.3.0 4) bzip2/1.0.8-GCCcore-11.3.0 8) SQLite/3.38.3-GCCcore-11.3.0 12) OpenSSL/1.1 This will show the currently loaded modules. Note here we also see Python/3.10.4-GCCcore-11.3.0 being available (along with any dependencies), following the previous example.","title":"Checking loaded modules"},{"location":"tutorials/t03_setting_up_and_using_development_tools/#unload-a-module","text":"Following the previous example, to unload the loaded Python version, we can do so by using the module unload command. This will remove the specified module from the loaded modules of your environment. Example: Removing Python 3.10.4 From the Environment [marconstantinou@front02 ~]$ module unload Python/3.10.4-GCCcore-11.3.0 [marconstantinou@front02 ~]$ module list Currently Loaded Modules: 1) GCCcore/11.3.0 4) bzip2/1.0.8-GCCcore-11.3.0 7) Tcl/8.6.12-GCCcore-11.3.0 10) GMP/6.2.1-GCCcore-11.3.0 2) zlib/1.2.12-GCCcore-11.3.0 5) ncurses/6.3-GCCcore-11.3.0 8) SQLite/3.38.3-GCCcore-11.3.0 11) libffi/3.4.2-GCCcore-11.3.0 3) binutils/2.38-GCCcore-11.3.0 6) libreadline/8.1.2-GCCcore-11.3.0 9) XZ/5.2.5-GCCcore-11.3.0 12) OpenSSL/1.1 [marconstantinou@front02 ~]$ python -V Python 3.10.13 You can also switch from one version to another in one go via module swap : [marconstantinou@front02 ~]$ module swap Python/3.10.4-GCCcore-11.3.0 Python/3.12.3-GCCcore-13.3.0 The following have been reloaded with a version change: 1) GCCcore/11.3.0 => GCCcore/13.3.0 7) binutils/2.38-GCCcore-11.3.0 => binutils/2.42-GCCcore-13.3.0 2) OpenSSL/1.1 => OpenSSL/3 8) bzip2/1.0.8-GCCcore-11.3.0 => bzip2/1.0.8-GCCcore-13.3.0 3) Python/3.10.4-GCCcore-11.3.0 => Python/3.12.3-GCCcore-13.3.0 9) libffi/3.4.2-GCCcore-11.3.0 => libffi/3.4.5-GCCcore-13.3.0 4) SQLite/3.38.3-GCCcore-11.3.0 => SQLite/3.45.3-GCCcore-13.3.0 10) libreadline/8.1.2-GCCcore-11.3.0 => libreadline/8.2-GCCcore-13.3.0 5) Tcl/8.6.12-GCCcore-11.3.0 => Tcl/8.6.14-GCCcore-13.3.0 11) ncurses/6.3-GCCcore-11.3.0 => ncurses/6.5-GCCcore-13.3.0 6) XZ/5.2.5-GCCcore-11.3.0 => XZ/5.4.5-GCCcore-13.3.0 12) zlib/1.2.12-GCCcore-11.3.0 => zlib/1.3.1-GCCcore-13.3.0 Note: The module system takes care of any relevant dependencies to the requested module we are trying to load. Therefore, switching from one version to another, the correct version of each dependency is reloaded automatically.","title":"Unload a module"},{"location":"tutorials/t03_setting_up_and_using_development_tools/#reset-loaded-modules","text":"To start from a clean environemnt we can use the purge command. module purge This will unload all the modules: [marconstantinou@front02 ~]$ module list Currently Loaded Modules: 1) GMP/6.2.1-GCCcore-11.3.0 5) bzip2/1.0.8-GCCcore-13.3.0 9) SQLite/3.45.3-GCCcore-13.3.0 13) Python/3.12.3-GCCcore-13.3.0 2) GCCcore/13.3.0 6) ncurses/6.5-GCCcore-13.3.0 10) XZ/5.4.5-GCCcore-13.3.0 3) zlib/1.3.1-GCCcore-13.3.0 7) libreadline/8.2-GCCcore-13.3.0 11) libffi/3.4.5-GCCcore-13.3.0 4) binutils/2.42-GCCcore-13.3.0 8) Tcl/8.6.14-GCCcore-13.3.0 12) OpenSSL/3 [marconstantinou@front02 ~]$ module purge [marconstantinou@front02 ~]$ module list No modules loaded Note: It is recommended to use before loading any modules, because modules loaded on your login environment are curried over to your job environment. This is a good way to make sure that there are no left over modules.","title":"Reset loaded modules"},{"location":"tutorials/t03_setting_up_and_using_development_tools/#34-using-conda-to-manage-python-virtual-environments","text":"conda is an open-source package management and environment management system widely used in data science, machine learning, and software development. It simplifies the installation, updating, and management of software packages and their dependencies across various programming languages, including Python , R , and C++ . conda also enables users to create isolated environments, allowing them to work on multiple projects with different dependencies without conflicts . It supports a wide range of operating systems and can manage libraries for scientific computing, data analysis, and machine learning efficiently. The official documentation for Anaconda can be found here .","title":"3.4. Using conda to Manage Python Virtual Environments"},{"location":"tutorials/t03_setting_up_and_using_development_tools/#341-creating-an-environment","text":"You can create an empty environment by running: # Replace <ENV_NAME> with a name for your environment [marconstantinou@front02 ~]$ conda create -n <ENV_NAME> or if you want to create an environment with Python and other packages, run this: # Replace <ENV_NAME> with a name for your environment # Replace <PACKAGE> with your desired package # Replace <VERSION> with your desired version of Python [marconstantinou@front02 ~]$ conda create -n <ENV_NAME> python=<VERSION> <PACKAGE>=<VERSION> Example: Creating an environment with Python 3.11 and other specific libraries conda create -n myenv python=3.11 beautifulsoup4 docutils jinja2=3.1.4 wheel","title":"3.4.1. Creating an environment"},{"location":"tutorials/t03_setting_up_and_using_development_tools/#342-activating-an-environment","text":"To activate your environment, simply run: # Replace <ENV_NAME> with the name of the environment you want to activate [marconstantinou@front02 ~]$ conda activate <ENV_NAME> (ENV_NAME)[marconstantinou@front02 ~]$ You should now be able to see (ENV_NAME) next to your user name.","title":"3.4.2. Activating an environment"},{"location":"tutorials/t03_setting_up_and_using_development_tools/#343-switching-between-environments","text":"If you want to switch to a different environment you can run these commands: # Show all available environments (base)[marconstantinou@front02 ~]$ conda info --envs # conda environments: # base * /nvme/h/buildsets/eb_cyclone_rl/software/Anaconda3/2023.03-1 juplab /nvme/h/marconstantinou/.conda/envs/juplab myenv /nvme/h/marconstantinou/.conda/envs/myenv # Replace <ENV_NAME> with the name of the environment you want to switch to (base)[marconstantinou@front02 ~]$ conda activate <ENV_NAME> (ENV_NAME)[marconstantinou@front02 ~]$ Note: Activating a different environment will deactivate your current one.","title":"3.4.3. Switching between environments"},{"location":"tutorials/t03_setting_up_and_using_development_tools/#344-deactivating-an-environment","text":"To deactivate your current environment, simply type: (ENV_NAME)[marconstantinou@front02 ~]$ conda deactivate (base)[marconstantinou@front02 ~]$ Note: Upon deactivation, the environment will switch to the base acting as the default environment. If you want to deactivate conda completely, repeat conda deactivate .","title":"3.4.4. Deactivating an environment"},{"location":"tutorials/t03_setting_up_and_using_development_tools/#345-exporting-an-environment","text":"Do not attempt to copy the environment's files to a different machine. It will not recreate the environment. We must export the environment and install it again. To export the environment, simply run: # Replace <ENV_NAME> with the name of the environment you want exported (base)[marconstantinou@front02 ~]$ conda activate <ENV_NAME> (ENV_NAME)[marconstantinou@front02 ~]$ conda env export > environment.yml Note: This will handle both conda and pip 's packages. In your current directory you should see a file called environment.yml . You can take this file to the machine you want to export the environment and run: (base)[marconstantinou@front02 ~]$ conda env create -f environment.yml After this, you can activate the environment and use it as is.","title":"3.4.5. Exporting an environment"},{"location":"tutorials/t03_setting_up_and_using_development_tools/#35-using-venv-to-manage-python-virtual-environments","text":"The venv module in Python is a tool to create isolated environments for your projects. This ensures that dependencies for one project don\u2019t interfere with those of another. Here's how to get started with venv . venv and conda both create isolated environments for Python projects, but they differ in scope and functionality. venv is a lightweight, Python-specific tool for isolating packages installed via pip , relying on the system's Python interpreter. In contrast, conda is a cross-language package and environment manager that can handle both Python and non-Python dependencies, including system libraries, and comes with its own Python interpreter. While venv is simple and built into Python, conda is more feature-rich, making it ideal for data science and projects with complex dependencies.","title":"3.5. Using venv to manage Python virtual environments"},{"location":"tutorials/t03_setting_up_and_using_development_tools/#351-creating-a-virtual-environment","text":"Navigate to your project's directory: [marconstantinou@front02 ~]$ cd /path/to/your/project Create a venv environment: # REPLACE venv_name with the name you want to give to your environment [marconstantinou@front02 ~]$ python -m venv venv_name","title":"3.5.1. Creating a virtual environment"},{"location":"tutorials/t03_setting_up_and_using_development_tools/#352-activating-a-virtual-environment","text":"To start using the virtual environment, you need to activate it via: [marconstantinou@front02 ~]$ source venv_name/bin/activate (venv_name)[marconstantinou@front02 ~]$","title":"3.5.2. Activating a virtual environment"},{"location":"tutorials/t03_setting_up_and_using_development_tools/#353-installing-packages","text":"Once you activate the environment you can start installing packages via simply running: pip install package_name Example: Installing numpy [marconstantinou@front02 ~]$ pip install numpy (venv_name) [marconstantinou@front02 ~]$ pip install numpy Collecting numpy Downloading numpy-2.2.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB) \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 62.0/62.0 kB 1.2 MB/s eta 0:00:00 Downloading numpy-2.2.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.1 MB) \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 16.1/16.1 MB 22.2 MB/s eta 0:00:00 Installing collected packages: numpy Successfully installed numpy-2.2.2 You can see what packages you have installed by typing: (venv_name) [marconstantinou@front02 ~]$ pip list Package Version ------- ------- numpy 2.2.2 pip 24.0","title":"3.5.3. Installing packages"},{"location":"tutorials/t03_setting_up_and_using_development_tools/#354-deleting-a-virtual-environment","text":"To delete the environment simply type: (venv_name) [marconstantinou@front02 ~]$ rm -rf venv_name","title":"3.5.4. Deleting a virtual environment"},{"location":"tutorials/t03_setting_up_and_using_development_tools/#36-hands-on-exercise","text":"In this exercise, we will: connect to Cyclone using VS Code (you can use whatever interface you prefer) create a directory for our small project load some modules create a virtual environment for our python scripts and finally we will run a small python file.","title":"3.6. Hands-on Exercise"},{"location":"tutorials/t03_setting_up_and_using_development_tools/#361-setup-our-projects-directory","text":"Open up VS Code and connect to Cyclone. Once connected, go to the terminal and into your $HOME dir. Note: Please keep in mind that you can do all these by using the graphical interface of VS Code - if you prefer that way, then go ahead and do that. [marconstantinou@front02 ~]$ pwd /nvme/h/marconstantinou If you are not in your $HOME directory, type: [marconstantinou@front02 ~]$ cd $HOME To create our hands-on directory, type: [marconstantinou@front02 ~]$ mkdir tutorial_03_project/ If you followed all the steps until now, you should see a tutorial_03_project directory when you type this command: [marconstantinou@front02 ~]$ ls -l total 2 lrwxrwxrwx 1 marconstantinou p232 15 Nov 4 10:20 data_p232 -> /onyx/data/p232 lrwxrwxrwx 1 marconstantinou p232 19 Nov 4 11:22 edu24 -> /onyx/data/edu24 lrwxrwxrwx 1 marconstantinou p232 29 Nov 4 10:20 scratch -> /nvme/scratch/marconstantinou drwxr-xr-x 2 marconstantinou p232 0 Dec 10 15:26 tutorial_03_project Go inside that directory: [marconstantinou@front02 ~]$ cd tutorial_03_project/ Then create a main.py file: [marconstantinou@front02 ~]$ touch main.py [marconstantinou@front02 ~]$ ls -l -rw-r--r-- 1 marconstantinou p232 0 Dec 19 11:40 main.py Now that we have our directory and script ready, lets load some module and install some packages.","title":"3.6.1. Setup our project's directory"},{"location":"tutorials/t03_setting_up_and_using_development_tools/#362-load-cuda-module","text":"Again, in your terminal connected to Cyclone, type: [marconstantinou@front02 ~]$ module avail CUDA ------------------------------------------- /eb/modules/all -------------------------------------------- CUDA/10.1.243 CUDA/11.4.1 CUDA/11.7.0 CUDA/11.8.0 CUDA/12.0.0 CUDA/12.1.0 CUDA/12.1.1 CUDA/12.6.0 (D) ... Where: D: Default Module If the avail list is too long consider trying: \"module --default avail\" or \"ml -d av\" to just list the default modules. \"module overview\" or \"ml ov\" to display the number of modules for each name. Use \"module spider\" to find all possible modules and extensions. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\". Let's load CUDA/12.1.1 : [marconstantinou@front02 ~]$ module load CUDA/12.1.1","title":"3.6.2. Load CUDA module"},{"location":"tutorials/t03_setting_up_and_using_development_tools/#363-create-a-conda-environment-for-our-project-and-install-some-libraries","text":"We are going to create a conda virtual environment, and install PyTorch which is a Deep Learning framework. First we need to load Anaconda using the module system: [marconstantinou@front02 ~]$ module load Anaconda3/2023.03-1 Then run: [marconstantinou@front02 ~]$ conda init Now restart your terminal. When you open a new terminal and connect to Cyclone, you should be able to create an environment and activate it. To do this, type: (base)[marconstantinou@front02 ~]$ conda create --name dummy_proj pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia Note: This will take some time to install, be patient. After everything is installed, go ahead and request a single GPU for 30 minutes on GPU partition of Cyclone by typing: # Start a 30 minute interactive session on a GPU node (base)[marconstantinou@front02 ~]$ srun --partition=gpu --gres=gpu:1 --time=00:30:00 --pty bash You might see something like this: srun: job 1037413 queued and waiting for resources This mean that cyclone is being utilized, and that our job is in queue until resourcers are found. Please give SLURM some time to allocate some resources to our request. Once the allocation of resources is successful, the front02 hostname shown on the terminal next to your username will be changed to the allocated node. In this case, the allocated node is gpu01 (Note it might differ from yours!). Then we want to activate our environment: (base)[marconstantinou@gpu01 ~]$ conda activate dummy_proj You can check which libraries are installed in your conda environment by typing (dummy_proj)[marconstantinou@gpu01 ~]$ conda list","title":"3.6.3. Create a conda environment for our project and install some libraries"},{"location":"tutorials/t03_setting_up_and_using_development_tools/#364-select-the-correct-python-interpreter-in-vs-code","text":"On the top of VS Code, click and type the following: > Python: Select Interpreter Click enter and then select the newly created conda environment:","title":"3.6.4. Select the correct Python Interpreter in VS Code"},{"location":"tutorials/t03_setting_up_and_using_development_tools/#365-running-a-python-script-on-a-gpu-node","text":"Copy and paste the following code snippet inside the main.py file that we created earlier. This file should be inside the directory tutorial_03_project import sys import torch def check_pytorch_installation(): try: # Check if PyTorch is installed print(f\"PyTorch version: {torch.__version__}\") except ImportError: print(\"PyTorch is not installed.\") sys.exit(1) def check_cuda_support(): if torch.cuda.is_available(): print(\"CUDA is available!\") print(f\"CUDA version: {torch.version.cuda}\") print(f\"Number of GPUs available: {torch.cuda.device_count()}\") print(f\"GPU Name: {torch.cuda.get_device_name(0)}\") else: print(\"CUDA is not available. Please ensure that your system has a compatible GPU and CUDA setup.\") def main(): print(\"Checking PyTorch and CUDA installation...\") check_pytorch_installation() check_cuda_support() print(\"Verification complete.\") if __name__ == \"__main__\": main() Now save it, and inside your terminal run: (dummy_proj)[marconstantinou@gpu01 ~]$ python main.py Note: The above command assumes you are in the directory of the main.py file. If you are in a different directory, you will have to edit the above command. You should see the following: Checking PyTorch and CUDA installation... PyTorch version: 2.5.1 CUDA is available! CUDA version: 12.1 Number of GPUs available: 1 GPU Name: Tesla V100-SXM2-32GB Verification complete. And that's all! You created a directory for your project, loaded some modules, created a conda environment with some libraries, and then you run some python code on a GPU Node.","title":"3.6.5. Running a python script on a GPU Node"},{"location":"tutorials/t03_setting_up_and_using_development_tools/#366-closing-our-interactive-job-session","text":"Inside your terminal go ahead and type: (dummy_proj)[marconstantinou@gpu01 ~]$ squeue --me You should see 1 job with a JOBID , copy that JOBID and type: # Replace JOBID with the ID of your job that you just copied (dummy_proj)[marconstantinou@gpu01 ~]$ scancel JOBID # Example (dummy_proj)[marconstantinou@gpu01 ~]$ scancel 1037152","title":"3.6.6. Closing our interactive job session"},{"location":"tutorials/t04_scaling_python_workloads/","text":"4. Scaling Python Workloads 4.1. Overview This tutorial provides a comprehensive guide on scaling Python workloads using Cyclone, covering frameworks for deep learning and hardware acceleration. The tutorial begins by executing Python scripts on CPUs, gradually progressing to single GPU, multi GPU, and multi-node distributed training using PyTorch's Distributed Data Parallel framework. Each section includes modifications to SLURM scripts for resource allocation and Python code adaptations to leverage the targeted hardware configurations. The tutorial concludes with troubleshooting tips and a recap of practices mentioned. 4.2. Learning Objectives By the end of this tutorial, participants will be able to: Write and execute SLURM scripts to run Python scripts on Cyclone in different configurations (CPU, single GPU, multi-GPU, multi-node). Understand the resource allocation process using SLURM for Python workloads. Optimize Python workflows for HPC by leveraging distributed computing frameworks like PyTorch DDP. Troubleshoot common issues when running Python jobs on Cyclone. 4.3. Prerequisites T01 - Introduction to HPC Systems : This tutorial will give you some basic knowledge on HPC systems and basic terminologies. T02 - Accessing and Navigating Cyclone: This tutorial will give you some basic knowledge on how to connect, copy files and navigate the HPC system. 4.4. Why Use Cyclone for Python Workloads? Scalability: Using Cyclone, workloads can be scaled across multiple CPUs and GPUs, which enables faster processing of large datasets and complex computations. Performance Gains: Utilizing advanced hardware/software such as GPUs, high-speed interconnects and SLURM, Python workflows can be executed more efficiently. 4.5. Tools and Frameworks SLURM - Used for job scheduling, job monitoring and environment setup PyTorch - Open-source deep learning framework that\u2019s known for its flexibility and ease-of-use. CUDA - Enables GPU acceleration for computational tasks through APIs to simplify GPU-based parallel processing for HPC, data science and AI. NCCL - Implements multi-GPU and multi-node communication that is optimized for NVIDIA GPUs and networking. 4.6. Training AI models on Cyclone This section demonstrates how to run Python scripts with multiple environment configurations on Cyclone, focusing on scaling Python workloads by leveraging Pytorch, CUDA and NVIDIA's NCCL backend. 4.6.1. Running Python Scripts on CPU The script below demonstrates how to sructure a training pipeline for deep learning using PyTorch on a CPU. Firstly, the necessary libraries that handle tasks such as data loading and model training are imported. The main workflow includes downloading the dataset, as well as defining and training a Convolutional Neural Network (CNN) . import os import argparse import torch import torch.nn as nn import torch.optim as optim from torchvision import datasets, transforms from torch.utils.data import DataLoader from model import CNN_classifier import time def train(model, dataloader: DataLoader, args): print(\"Entering training loop...\") criterion = nn.NLLLoss() optimizer = optim.Adam(params=model.parameters(), lr = args.lr) model.train() for epoch in range(1, args.epochs + 1): epoch_loss: float = 0.0 for batch_idx, (data,target) in enumerate(dataloader): optimizer.zero_grad() output = model(data) loss = criterion(output, target) loss.backward() optimizer.step() epoch_loss += loss.item() if batch_idx % 100 == 0: print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(dataloader.dataset)} ' f'({100. * batch_idx / len(dataloader):.0f}%)]\\tLoss: {loss.item():.6f}') print(\"Exiting training loop...\") def main(): parser = argparse.ArgumentParser(prog=\"Pytorch on HPC\") parser.add_argument(\"--batch_size\", type=int, default=16) parser.add_argument(\"--epochs\", type=int, default=5) parser.add_argument(\"--lr\", type=float, default=0.001) args = parser.parse_args() transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ]) train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform) train_loader = DataLoader( train_dataset, batch_size = args.batch_size, shuffle=False, ) model = CNN_classifier() time_start = time.time() train(model=model, dataloader= train_loader, args=args) time_stop = time.time() print(f\"Training time = {time_stop-time_start}\" ) if __name__ == \"__main__\" : main() To execute the above script, first navigate to your $HOME directory, create and enter the new directory for this tutorial. [smillas@front02 ~]$ cd $HOME [smillas@front02 ~]$ mkdir tutorial_04 [smillas@front02 ~]$ cd tutorial_04 Next, create the source file cnn_cpu.py [smillas@front02 tutorial_04]$ touch cnn_cpu.py [smillas@front02 tutorial_04]$ nano cnn_cpu.py # copy the python code above and copy the code above. To execute cnn_cpu.py you need to create the SLUMR file run_cpu.SLURM [smillas@front02 tutorial_04]$ touch run_cpu.SLURM [smillas@front02 tutorial_04]$ chmod +x run_cpu.SLURM # make the file executable [smillas@front02 tutorial_04]$ nano run_cpu.SLURM # copy the SLURM script below and copy the following: #!/bin/bash #SBATCH --job-name=pytorch_cpu # Job name #SBATCH --nodes=1 # Number of nodes #SBATCH --cpus-per-task=10 # CPUs per task #SBATCH --time=02:00:00 # Maximum runtime (HH:MM:SS) #SBATCH --partition=cpu # Partition name #SBATCH --output=logs/cpu_%j.out # Standard output log #SBATCH --error=logs/cpu_%j.err # Standard error log module load PyTorch/1.12.0-foss-2022a-CUDA-11.7.0 module load torchvision/0.13.1-foss-2022a-CUDA-11.7.0 # Run Python script srun python cnn_cpu.py \\ --batch_size 16 \\ --epochs 5 \\ --lr 0.001 The run_cpu.SLURM script is designed to execute the cnn_cpu.py on Cyclone using SLURM. In the script, the number of nodes, CPU cores, runtime and partition are specified, which instruct SLURM to allocate 10 CPU cores from a single node for 2:00:00 hours for this job. Next, the environment is setup using Cyclones available modules for Pytorch and Torchvision ensuring all necessary libraries and dependencies are available. Finally, the Python script is launched using srun , which executes the specified script with the allocated resources and given runtime arguments. To run the script: sbatch run_cpu.SLURM After executing the above script, two seperate output logs will be generated in the logs/ directory. cpu_<jobid>.out cpu_<jobid>.err Navigate to the logs/ directory with the terminal interface using cd logs/ or by using VScode's file explorer. Next, view the contents of the cpu_ .out . To use the terminal, first execute module load nano to load a Linux text editor and enter the command nano cpu_ .out . If navigating using VScode, simply double click the output file. Entering training loop... Train Epoch: 1 [0/60000 (0%)] Loss: 2.300653 Train Epoch: 1 [1600/60000 (3%)] Loss: 0.546140 . . Train Epoch: 5 [57600/60000 (96%)] Loss: 0.000070 Train Epoch: 5 [59200/60000 (99%)] Loss: 0.000981 Exiting training loop... Training time = 192.2544162273407 Training this simple CNN classifier for 5 epochs on the relatively small MNIST dataset took a total of 192 seconds. This process can be made significantly more efficient by utilizing Cyclones GPU cores, rather than the CPU cores. 4.6.2. Running Python Scripts on Single GPU To train the AI model using GPUs on Cyclone, some basic modifications must be made on both the Python and SLURM scripts. Beginning with the cnn_gpu.py Python script, in the main function, the following code block is added. if torch.cuda.is_available(): print(\"Utilizing GPU\") device = torch.device(\"cuda\") else: print(\"Utilizing CPU\") device = torch.device('cpu') The above code initializes the device variable as the GPU, by first checking if there is one available. If not, the device defaults to CPU. The next changes to the script must be made before the model begins training using the code below model.to(device) and during training using the following data = data.to(device) target = target.to(device) It is important to have both the data and the model on the same device (CPU or GPU), otherwise a runtime error will occur. PyTorch operations require the tensors involved to be on the same device. \u2139\ufe0f The complete cnn_gpu.py can be found here . The run_gpu.SLURM script is designed to execute the cnn_gpu.py on a single GPU. This configuration is specified by instructing SLURM to allocate a GPU on a single node using the following SLURM directives. When launching gpu jobs on Cyclone, it is important to specify the correct --partition as SLURM defaults to CPU, which will cause an error. #SBATCH --nodes=1 # Number of nodes #SBATCH --ntasks-per-node=1 # Tasks per node (GPUs per node) #SBATCH --gpus-per-node=1 # GPUs per node #SBATCH --partition=gpu # Partition name Next, some additional GPU-related modules must be loaded from Cyclone's library. The cuDNN and CUDA modules provide the tools and drivers required to enable GPU optimized deep learning operations. module load PyTorch/1.12.0-foss-2022a-CUDA-11.7.0 module load cuDNN/8.4.1.50-CUDA-11.7.0 module load torchvision/0.13.1-foss-2022a-CUDA-11.7.0 module load CUDA/11.7.0 When loading modules from Cyclone's library, it is important to load compatible versions of these modules to ensure smooth interaction between the hardware (e.g., CUDA), and the deep learning framework (e.g., PyTorch), avoiding errors or performance issues. \u2139\ufe0f The complete run_gpu.SLURM can be found here . To run this example, first we need to create the cnn_gpu.py and run_gpu.SLURM as follows: [smillas@front02 ~]$ cd $HOME/tutorial_04 [smillas@front02 tutorial_04]$ touch cnn_gpu.py [smillas@front02 tutorial_04]$ nano cnn_gpu.py # copy the code from source Python file [smillas@front02 tutorial_04]$ touch run_gpu.SLURM [smillas@front02 tutorial_04]$ chmod +x run_gpu.SLURM # make the file executable [smillas@front02 tutorial_04]$ nano run_gpu.SLURM # copy the source SLURM script where the source files can be found from cnn_gpu.py and run_gpu.SLURM . Once the two files are ready, submit the job using: [smillas@front02 tutorial_04]$ sbatch run_gpu.SLURM After the job finishes, navigate to the logs/ directory and open the gpu_<job_id>.out file and observe the difference. Utilizing GPU Entering training loop... Train Epoch: 1 [0/60000 (0%)] Loss: 2.332518 Train Epoch: 1 [1600/60000 (3%)] Loss: 0.376552 Train Epoch: 1 [3200/60000 (5%)] Loss: 0.127207 . . . Train Epoch: 5 [59200/60000 (99%)] Loss: 0.082046 Exiting training loop... Training time = 73.48071932792664 Already there is a substantial decrease in training time by utilizing a GPU over a CPU . However, Cyclone offers much more GPU resources per node, meaning that there are further gains to training efficiency left on the table by utilizing only a single GPU. 4.6.3. Running Python Scripts on Multi-GPU (Single Node) using DDP To leverage multiple GPUs per node on Cyclone, workloads must be scaled using parallelization techniques. While there are many times of parallelism, Data parallelism will be used to scale model training in this tutorial. When optimizing AI training using data parallelism, a copy of the model is loaded on all GPUs available, and the dataset is split amongst them . Each GPU processes a different subset of the data in parallel. During the forward pass, each GPU processes a different batch of the data and the gradients are communicated between the devices so as to ensure the model parameters are appropriately updated during backpropagation. To implement this efficiently, PyTorch provides the Distributed Data Parallel (DDP) module, which automates the process of distributing data, synchronizing gradients, and ensuring consistent parameter updates across GPUs. DDP leverages NCCL (NVIDIA Collective Communications Library) as its backend to optimize GPU communication, enabling seamless gradient sharing and synchronization with minimal overhead. To train our AI model using DDP, some changes must be made to the Python and SLURM scripts. Firstly, the concepts of Ranks , Processes and the World are introduced to the workflow. A rank is the unique id given to a process , and is used for communication purposes. One GPU corresponds to one process. The World is a group that contains all the processes , thus the size of the World is equal to the number of GPUs. Firstly, in the run_multigpu.SLURM script, changes are being made to the directives to instruct SLURM to allocate more GPUs per node. For this section of the tutorial, 2 GPUs on a single node are utilized. #SBATCH --nodes=1 # Number of nodes #SBATCH --ntasks-per-node=2 # Tasks per node (GPUs per node) #SBATCH --gpus-per-node=2 # GPUs per node Further changes to the SLURM script include lines to retrieve environment variables set by the SLURM scheduler to define the nodes' address and a random port which are used to establish communication between processes during training . This communication will be done using the NCCL backend, which must be also loaded. World size can be directly calculated in the SLURM script using the environment variables, which as stated before is the total number of GPUs available. Finally to the srun command, add --export=ALL to ensure the environment variables are passed to the srun job. A snapshot of these changes is shown below. \u2139\ufe0f The full run_multigpu.SLURM can be found here . module load CUDA/11.7.0 module load NCCL/2.12.12-GCCcore-11.3.0-CUDA-11.7.0 module load PyTorch/1.12.0-foss-2022a-CUDA-11.7.0 module load cuDNN/8.4.1.50-CUDA-11.7.0 module load torchvision/0.13.1-foss-2022a-CUDA-11.7.0 export MASTER_ADDR=$(scontrol show hostname $SLURM_NODELIST | head -n 1) export MASTER_PORT=$(shuf -i 29500-65535 -n 1) export WORLD_SIZE=$(($SLURM_NNODES * $SLURM_NTASKS_PER_NODE)) srun --export=ALL python HPC_tutorial/multigpu_example.py \\ --batch_size 16 \\ --epochs 5 \\ --lr 0.001 Moving on to the Python script, the environment variables ( MASTER_ADDR , MASTER_PORT and WORLD_SIZE ) are exported from the SLURM scheduler to be used by DDP, and are placed as global variables at the start of the script, after the import statements. import os import argparse import torch import torch.nn as nn import torch.optim as optim from torchvision import datasets, transforms from torch.utils.data import DataLoader, DistributedSampler from model import CNN_classifier import time MASTER_ADDR = os.environ[\"MASTER_ADDR\"] MASTER_PORT = os.environ[\"MASTER_PORT\"] WORLD_SIZE = int(os.environ[\"WORLD_SIZE\"]) The next adjustment to the script happens to the main function (which has been renamed to the worker function). Since SLURM will launch as many processes as there are GPUs, the rank of the process can be defined as the process ID. Next, the process group must be initialized with some key parameters. nccl is chosen as the backend, the world_size and rank parameters are added, and lastly specify the init_method = 'env://' to indicate the MASTER_ADDR and MASTER_PORT environment variables should be used to configure the communication. Finally, the current process is assigned to its corresponding GPU based on its rank . The device object is created with the following syntax cuda:<rank> , which is used send the model and data to the approprate GPU. def worker(args): rank = int(os.environ[\"SLURM_PROCID\"]) torch.distributed.init_process_group( backend='nccl', world_size=WORLD_SIZE, rank=rank, init_method='env://' torch.cuda.set_device(rank) device = torch.device(f\"cuda:{rank}\") ) When implementing data parallelism, it is unnecessary to download the entire dataset on all devices. One device can download the dataset and share it with the rest of the GPUs. To do this, the dataset download command is changed to only execute on rank 0 . To ensure device synchronization, torch.distributed.barrier() is called, which instructs the GPUs to wait until all other devices reach that same point in the script before continuing. Next the dataset is loaded on all other GPUs if rank == 0: train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform) torch.distributed.barrier() train_dataset = datasets.MNIST('./data', train=True, download=False, transform=transform) Next, a DistributedSampler object is defined, which ensures that workload is distributed across all GPUs that are apart of the world. To ensure the dataset is split into manageable batches, the sampler is combined with the Dataloader object. train_sampler = DistributedSampler( train_dataset, num_replicas=WORLD_SIZE, rank=rank ) train_loader = DataLoader( train_dataset, batch_size=args.batch_size, drop_last=True, sampler=train_sampler ) The model is then wrapped with DistributedDataParallel , which will handle the multi-GPU training, ensuring the gradients will be synchronized across all processes after the forward pass. Next, the device_ids = [rank] is specified to define the GPU on which the model will run for the current process. model = CNN_classifier().to(device) model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[rank]) Stepping into the train() function, torch.distributed.barrier() is called at the end of the epoch to ensure synchronization during training and to avoid runtime errors. Optionally, rank parameter is added to the function call to avoid duplicate printing in the output call by specifying one GPU to be the logging device. def train(model, dataloader: DataLoader, args, device,rank): criterion = nn.NLLLoss() optimizer = optim.Adam(params=model.parameters(), lr = args.lr) model.train() if rank == 0: print(\"Entering training loop...\") for epoch in range(1, args.epochs + 1): epoch_loss: float = 0.0 for batch_idx, (data,target) in enumerate(dataloader): data = data.to(device) target = target.to(device) output = model(data) loss = criterion(output, target) optimizer.zero_grad() loss.backward() optimizer.step() epoch_loss += loss.item() #monitoring if rank == 0 and batch_idx % 100 == 0: print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(dataloader.dataset)} ' f'({100. * batch_idx / len(dataloader):.0f}%)]\\tLoss: {loss.item():.6f}') torch.distributed.barrier() if rank == 0: print(\"Exiting training loop...\") Last and most importantly, at the end of the worker function, torch.distributed.destroy_process_group() is called. This ensures all resources tied to distributed training are properly released. if rank == 0: print(f\"Training time = {time_stop - time_start}\") torch.distributed.destroy_process_group() if __name__ == \"__main__\" : worker(args=args) \u2139\ufe0f The full cnn_multigpu.py can be found here . To run this example, first we need to create the cnn_multigpu.py and run_multigpu.SLURM as follows: [smillas@front02 ~]$ cd $HOME/tutorial_04 [smillas@front02 tutorial_04]$ touch cnn_multigpu.py [smillas@front02 tutorial_04]$ nano cnn_multigpu.py # copy the code from source Python file [smillas@front02 tutorial_04]$ touch run_multigpu.SLURM [smillas@front02 tutorial_04]$ chmod +x run_multigpu.SLURM # make the file executable [smillas@front02 tutorial_04]$ nano run_multigpu.SLURM # copy the source SLURM script where the source files can be found from cnn_multigpu.py and run_multigpu.SLURM . Once the two files are ready, submit the job using: [smillas@front02 tutorial_04]$ sbatch run_multigpu.SLURM After the job finishes, navigate to the logs/ directory and open the multigpu_<job_id>.out file and observe the difference. MASTER_ADDR: gpu06 MASTER_PORT: 31315 WORLD_SIZE: 2 Entering training loop... Train Epoch: 1 [0/60000 (0%)] Loss: 2.319880 . . . Train Epoch: 5 [27200/60000 (91%)] Loss: 0.000118 Train Epoch: 5 [28800/60000 (96%)] Loss: 0.000066 Exiting training loop... Training time = 42.064579248428345 By utilizing multiple GPUs, we achieve a much faster training time . Cyclone offers 4 GPUs per node . But multiple nodes can be used to further speed up training with minimal changes to the code base. 4.6.4. Running Python Scripts on Multi-GPU (Multi-Node) with DDP Using the same parallelization technique and some simple changes to the previous SLURM and Python scripts, more compute resources can be leveraged to further speed up the AI models' training by utilizing multiple nodes. To do this, some changes must first be made to the SLURM script. These changes instruct the SLURM scheduler to allocate two GPUs on two nodes , for a total of four GPUs. #SBATCH --nodes=2 # Number of nodes #SBATCH --ntasks-per-node=2 # Tasks per node (GPUs per node) #SBATCH --gpus-per-node=2 # Number of GPUs per node Next change is the addition of the NODE_RANK environment variable which will equal the unique identifier of the current node in the distributed setup. This can be obtained through the $SLURM_NODEID environment variable, provided by SLURM. export MASTER_ADDR=$(scontrol show hostname $SLURM_NODELIST | head -n 1) export MASTER_PORT=$(shuf -i 29500-65535 -n 1) export WORLD_SIZE=$(($SLURM_NNODES * $SLURM_NTASKS_PER_NODE)) export NODE_RANK=$SLURM_NODEID \u2139\ufe0f The full run_multinode.SLURM can be found here . In the Python script, the first change that must be made is the addition of the NODE_RANK global variable, which will be used in the worker function for logging purposes. MASTER_ADDR = os.environ[\"MASTER_ADDR\"] MASTER_PORT = os.environ[\"MASTER_PORT\"] WORLD_SIZE = int(os.environ[\"WORLD_SIZE\"]) NODE_RANK = int(os.environ[\"NODE_RANK\"]) In the worker function, the concepts of local_rank and global_rank are introduced and defined using the SLURM scheduler environment variables. global_rank = int(os.environ[\"SLURM_PROCID\"]) local_rank = int(os.environ[\"SLURM_LOCALID\"]) The global_rank is the unique identity assigned to each process (or GPU) as part of the general world and is used for inter-process communication and coordination across the entire cluster . The local_rank is the unique identifier assigned to a process as part of a node and is used to assign and manage GPU usage within a specific node in DDP . Moving on, a few changes must be made to the rank assignment on the various function calls. In the torch.distributed.init_process_group() the global_rank is used to uniquely identify each process in the distributed training across all nodes. It ensures proper coordination and communication in the entire distributed world. torch.distributed.init_process_group( backend='nccl', world_size=WORLD_SIZE, rank=global_rank, init_method='env://' ) Next, when setting the device, the local_rank is used to specify which GPU on the current node the process will use. Each process must operate on a separate GPU within the same node. torch.cuda.set_device(local_rank) device = torch.device(f\"cuda:{local_rank}\") Afterwards, when downloading the dataset, local_rank is used to share the dataset with all other GPUs on the same node . if local_rank == 0: train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform) Furthermore, when wrapping the model with DDP, The local_rank is used to bind the DDP instance to the specific GPU that the process is operating on, ensuring the process handles only its assigned device. model = nn.parallel.DistributedDataParallel(model, device_ids=[local_rank]) Lastly, when logging during training, the global_rank is used to avoid redundant log statements in the output. train(model=model, dataloader=train_loader, args=args, device=device, rank=global_rank) \u2139\ufe0f The full cnn_multinode.py can be found here . To run this example, first we need to create the cnn_multinode.py and run_multinode.SLURM as follows: [smillas@front02 ~]$ cd $HOME/tutorial_04 [smillas@front02 tutorial_04]$ touch cnn_multinode.py [smillas@front02 tutorial_04]$ nano cnn_multinode.py # copy the code from source Python file [smillas@front02 tutorial_04]$ touch run_multinode.SLURM [smillas@front02 tutorial_04]$ chmod +x run_multinode.SLURM # make the file executable [smillas@front02 tutorial_04]$ nano run_multinode.SLURM # copy the source SLURM script where the source files can be found from cnn_multinode.py and run_multinode.SLURM . Once the two files are ready, submit the job using: [smillas@front02 tutorial_04]$ sbatch run_multigpu.SLURM After the job finishes, navigate to the logs/ directory and open the multinode_<job_id>.out file and observe the difference. MASTER_ADDR: gpu06 MASTER_PORT: 57049 WORLD_SIZE: 4 NODE_RANK: 0 Rank 1 on Node 0: Initializing process group. Rank 3 on Node 1: Initializing process group. Rank 2 on Node 1: Initializing process group. Rank 0 on Node 0: Initializing process group. Entering training loop... . . . Train Epoch: 5 [12800/60000 (85%)] Loss: 0.000021 Train Epoch: 5 [14400/60000 (96%)] Loss: 0.000576 Exiting training loop... Training time = 25.612423181533813 4.6.5. Runtime Results A comparison of each version with respect to a baseline setup. The baseline here is selected to be the single GPU version. Version Number of GPUs Runtime (s) SpeedUp (times) CPU-only 0 192.25 0.38 (or 2.63x slower) Single GPU 1 73.48 1 Single-node GPU 2 42.06 1.75 Multi-node GPU 4 25.61 2.86 4.7. Recap and Troubleshooting In this tutorial, we covered the process of scaling Python workloads on Cyclone, focusing on the following key concepts: Resource Allocation with SLURM Setting up SLURM directives for various configurations, from single CPU to multi-node GPU training. Using environment variables such as MASTER_ADDR, MASTER_PORT, and WORLD_SIZE for distributed computing. Python Script Modifications Adapting scripts for different hardware configurations (CPU, single GPU, multi-GPU, and multi-node setups). Leveraging DistributedDataParallel to automate data parallelism across GPUs. While Using Cyclone for distributed training and complex computations, some issues may be encountered with regards to job submission or RuntimeErrors SLURM job fails to launch Problem: SLURM job fails with an error indicating incorrect directives. Solution: Double-check SLURM script parameters ( --nodes , --gpus-per-node , --partition ). Ensure they match the resources available in the partition. Runtime errors Problem: Tensor or model mismatch errors during training. Solution: Ensure both the model and data tensors are moved to the correct device using model.to(device) and data.to(device) . Problem: Processes fail to communicate due to incorrect master address or port. Solution: Verify that MASTER_ADDR and MASTER_PORT are correctly set in the SLURM script and ensure network connectivity between nodes. Problem: Training does not scale as expected. Solution: Ensure efficient resource utilization by setting appropriate batch sizes and verifying GPU utilization using monitoring tools (e.g., nvidia-smi ).","title":"Scaling Python Workloads"},{"location":"tutorials/t04_scaling_python_workloads/#4-scaling-python-workloads","text":"","title":"4. Scaling Python Workloads"},{"location":"tutorials/t04_scaling_python_workloads/#41-overview","text":"This tutorial provides a comprehensive guide on scaling Python workloads using Cyclone, covering frameworks for deep learning and hardware acceleration. The tutorial begins by executing Python scripts on CPUs, gradually progressing to single GPU, multi GPU, and multi-node distributed training using PyTorch's Distributed Data Parallel framework. Each section includes modifications to SLURM scripts for resource allocation and Python code adaptations to leverage the targeted hardware configurations. The tutorial concludes with troubleshooting tips and a recap of practices mentioned.","title":"4.1. Overview"},{"location":"tutorials/t04_scaling_python_workloads/#42-learning-objectives","text":"By the end of this tutorial, participants will be able to: Write and execute SLURM scripts to run Python scripts on Cyclone in different configurations (CPU, single GPU, multi-GPU, multi-node). Understand the resource allocation process using SLURM for Python workloads. Optimize Python workflows for HPC by leveraging distributed computing frameworks like PyTorch DDP. Troubleshoot common issues when running Python jobs on Cyclone.","title":"4.2. Learning Objectives"},{"location":"tutorials/t04_scaling_python_workloads/#43-prerequisites","text":"T01 - Introduction to HPC Systems : This tutorial will give you some basic knowledge on HPC systems and basic terminologies. T02 - Accessing and Navigating Cyclone: This tutorial will give you some basic knowledge on how to connect, copy files and navigate the HPC system.","title":"4.3. Prerequisites"},{"location":"tutorials/t04_scaling_python_workloads/#44-why-use-cyclone-for-python-workloads","text":"Scalability: Using Cyclone, workloads can be scaled across multiple CPUs and GPUs, which enables faster processing of large datasets and complex computations. Performance Gains: Utilizing advanced hardware/software such as GPUs, high-speed interconnects and SLURM, Python workflows can be executed more efficiently.","title":"4.4. Why Use Cyclone for Python Workloads?"},{"location":"tutorials/t04_scaling_python_workloads/#45-tools-and-frameworks","text":"SLURM - Used for job scheduling, job monitoring and environment setup PyTorch - Open-source deep learning framework that\u2019s known for its flexibility and ease-of-use. CUDA - Enables GPU acceleration for computational tasks through APIs to simplify GPU-based parallel processing for HPC, data science and AI. NCCL - Implements multi-GPU and multi-node communication that is optimized for NVIDIA GPUs and networking.","title":"4.5. Tools and Frameworks"},{"location":"tutorials/t04_scaling_python_workloads/#46-training-ai-models-on-cyclone","text":"This section demonstrates how to run Python scripts with multiple environment configurations on Cyclone, focusing on scaling Python workloads by leveraging Pytorch, CUDA and NVIDIA's NCCL backend.","title":"4.6. Training AI models on Cyclone"},{"location":"tutorials/t04_scaling_python_workloads/#461-running-python-scripts-on-cpu","text":"The script below demonstrates how to sructure a training pipeline for deep learning using PyTorch on a CPU. Firstly, the necessary libraries that handle tasks such as data loading and model training are imported. The main workflow includes downloading the dataset, as well as defining and training a Convolutional Neural Network (CNN) . import os import argparse import torch import torch.nn as nn import torch.optim as optim from torchvision import datasets, transforms from torch.utils.data import DataLoader from model import CNN_classifier import time def train(model, dataloader: DataLoader, args): print(\"Entering training loop...\") criterion = nn.NLLLoss() optimizer = optim.Adam(params=model.parameters(), lr = args.lr) model.train() for epoch in range(1, args.epochs + 1): epoch_loss: float = 0.0 for batch_idx, (data,target) in enumerate(dataloader): optimizer.zero_grad() output = model(data) loss = criterion(output, target) loss.backward() optimizer.step() epoch_loss += loss.item() if batch_idx % 100 == 0: print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(dataloader.dataset)} ' f'({100. * batch_idx / len(dataloader):.0f}%)]\\tLoss: {loss.item():.6f}') print(\"Exiting training loop...\") def main(): parser = argparse.ArgumentParser(prog=\"Pytorch on HPC\") parser.add_argument(\"--batch_size\", type=int, default=16) parser.add_argument(\"--epochs\", type=int, default=5) parser.add_argument(\"--lr\", type=float, default=0.001) args = parser.parse_args() transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ]) train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform) train_loader = DataLoader( train_dataset, batch_size = args.batch_size, shuffle=False, ) model = CNN_classifier() time_start = time.time() train(model=model, dataloader= train_loader, args=args) time_stop = time.time() print(f\"Training time = {time_stop-time_start}\" ) if __name__ == \"__main__\" : main() To execute the above script, first navigate to your $HOME directory, create and enter the new directory for this tutorial. [smillas@front02 ~]$ cd $HOME [smillas@front02 ~]$ mkdir tutorial_04 [smillas@front02 ~]$ cd tutorial_04 Next, create the source file cnn_cpu.py [smillas@front02 tutorial_04]$ touch cnn_cpu.py [smillas@front02 tutorial_04]$ nano cnn_cpu.py # copy the python code above and copy the code above. To execute cnn_cpu.py you need to create the SLUMR file run_cpu.SLURM [smillas@front02 tutorial_04]$ touch run_cpu.SLURM [smillas@front02 tutorial_04]$ chmod +x run_cpu.SLURM # make the file executable [smillas@front02 tutorial_04]$ nano run_cpu.SLURM # copy the SLURM script below and copy the following: #!/bin/bash #SBATCH --job-name=pytorch_cpu # Job name #SBATCH --nodes=1 # Number of nodes #SBATCH --cpus-per-task=10 # CPUs per task #SBATCH --time=02:00:00 # Maximum runtime (HH:MM:SS) #SBATCH --partition=cpu # Partition name #SBATCH --output=logs/cpu_%j.out # Standard output log #SBATCH --error=logs/cpu_%j.err # Standard error log module load PyTorch/1.12.0-foss-2022a-CUDA-11.7.0 module load torchvision/0.13.1-foss-2022a-CUDA-11.7.0 # Run Python script srun python cnn_cpu.py \\ --batch_size 16 \\ --epochs 5 \\ --lr 0.001 The run_cpu.SLURM script is designed to execute the cnn_cpu.py on Cyclone using SLURM. In the script, the number of nodes, CPU cores, runtime and partition are specified, which instruct SLURM to allocate 10 CPU cores from a single node for 2:00:00 hours for this job. Next, the environment is setup using Cyclones available modules for Pytorch and Torchvision ensuring all necessary libraries and dependencies are available. Finally, the Python script is launched using srun , which executes the specified script with the allocated resources and given runtime arguments. To run the script: sbatch run_cpu.SLURM After executing the above script, two seperate output logs will be generated in the logs/ directory. cpu_<jobid>.out cpu_<jobid>.err Navigate to the logs/ directory with the terminal interface using cd logs/ or by using VScode's file explorer. Next, view the contents of the cpu_ .out . To use the terminal, first execute module load nano to load a Linux text editor and enter the command nano cpu_ .out . If navigating using VScode, simply double click the output file. Entering training loop... Train Epoch: 1 [0/60000 (0%)] Loss: 2.300653 Train Epoch: 1 [1600/60000 (3%)] Loss: 0.546140 . . Train Epoch: 5 [57600/60000 (96%)] Loss: 0.000070 Train Epoch: 5 [59200/60000 (99%)] Loss: 0.000981 Exiting training loop... Training time = 192.2544162273407 Training this simple CNN classifier for 5 epochs on the relatively small MNIST dataset took a total of 192 seconds. This process can be made significantly more efficient by utilizing Cyclones GPU cores, rather than the CPU cores.","title":"4.6.1. Running Python Scripts on CPU"},{"location":"tutorials/t04_scaling_python_workloads/#462-running-python-scripts-on-single-gpu","text":"To train the AI model using GPUs on Cyclone, some basic modifications must be made on both the Python and SLURM scripts. Beginning with the cnn_gpu.py Python script, in the main function, the following code block is added. if torch.cuda.is_available(): print(\"Utilizing GPU\") device = torch.device(\"cuda\") else: print(\"Utilizing CPU\") device = torch.device('cpu') The above code initializes the device variable as the GPU, by first checking if there is one available. If not, the device defaults to CPU. The next changes to the script must be made before the model begins training using the code below model.to(device) and during training using the following data = data.to(device) target = target.to(device) It is important to have both the data and the model on the same device (CPU or GPU), otherwise a runtime error will occur. PyTorch operations require the tensors involved to be on the same device. \u2139\ufe0f The complete cnn_gpu.py can be found here . The run_gpu.SLURM script is designed to execute the cnn_gpu.py on a single GPU. This configuration is specified by instructing SLURM to allocate a GPU on a single node using the following SLURM directives. When launching gpu jobs on Cyclone, it is important to specify the correct --partition as SLURM defaults to CPU, which will cause an error. #SBATCH --nodes=1 # Number of nodes #SBATCH --ntasks-per-node=1 # Tasks per node (GPUs per node) #SBATCH --gpus-per-node=1 # GPUs per node #SBATCH --partition=gpu # Partition name Next, some additional GPU-related modules must be loaded from Cyclone's library. The cuDNN and CUDA modules provide the tools and drivers required to enable GPU optimized deep learning operations. module load PyTorch/1.12.0-foss-2022a-CUDA-11.7.0 module load cuDNN/8.4.1.50-CUDA-11.7.0 module load torchvision/0.13.1-foss-2022a-CUDA-11.7.0 module load CUDA/11.7.0 When loading modules from Cyclone's library, it is important to load compatible versions of these modules to ensure smooth interaction between the hardware (e.g., CUDA), and the deep learning framework (e.g., PyTorch), avoiding errors or performance issues. \u2139\ufe0f The complete run_gpu.SLURM can be found here . To run this example, first we need to create the cnn_gpu.py and run_gpu.SLURM as follows: [smillas@front02 ~]$ cd $HOME/tutorial_04 [smillas@front02 tutorial_04]$ touch cnn_gpu.py [smillas@front02 tutorial_04]$ nano cnn_gpu.py # copy the code from source Python file [smillas@front02 tutorial_04]$ touch run_gpu.SLURM [smillas@front02 tutorial_04]$ chmod +x run_gpu.SLURM # make the file executable [smillas@front02 tutorial_04]$ nano run_gpu.SLURM # copy the source SLURM script where the source files can be found from cnn_gpu.py and run_gpu.SLURM . Once the two files are ready, submit the job using: [smillas@front02 tutorial_04]$ sbatch run_gpu.SLURM After the job finishes, navigate to the logs/ directory and open the gpu_<job_id>.out file and observe the difference. Utilizing GPU Entering training loop... Train Epoch: 1 [0/60000 (0%)] Loss: 2.332518 Train Epoch: 1 [1600/60000 (3%)] Loss: 0.376552 Train Epoch: 1 [3200/60000 (5%)] Loss: 0.127207 . . . Train Epoch: 5 [59200/60000 (99%)] Loss: 0.082046 Exiting training loop... Training time = 73.48071932792664 Already there is a substantial decrease in training time by utilizing a GPU over a CPU . However, Cyclone offers much more GPU resources per node, meaning that there are further gains to training efficiency left on the table by utilizing only a single GPU.","title":"4.6.2. Running Python Scripts on Single GPU"},{"location":"tutorials/t04_scaling_python_workloads/#463-running-python-scripts-on-multi-gpu-single-node-using-ddp","text":"To leverage multiple GPUs per node on Cyclone, workloads must be scaled using parallelization techniques. While there are many times of parallelism, Data parallelism will be used to scale model training in this tutorial. When optimizing AI training using data parallelism, a copy of the model is loaded on all GPUs available, and the dataset is split amongst them . Each GPU processes a different subset of the data in parallel. During the forward pass, each GPU processes a different batch of the data and the gradients are communicated between the devices so as to ensure the model parameters are appropriately updated during backpropagation. To implement this efficiently, PyTorch provides the Distributed Data Parallel (DDP) module, which automates the process of distributing data, synchronizing gradients, and ensuring consistent parameter updates across GPUs. DDP leverages NCCL (NVIDIA Collective Communications Library) as its backend to optimize GPU communication, enabling seamless gradient sharing and synchronization with minimal overhead. To train our AI model using DDP, some changes must be made to the Python and SLURM scripts. Firstly, the concepts of Ranks , Processes and the World are introduced to the workflow. A rank is the unique id given to a process , and is used for communication purposes. One GPU corresponds to one process. The World is a group that contains all the processes , thus the size of the World is equal to the number of GPUs. Firstly, in the run_multigpu.SLURM script, changes are being made to the directives to instruct SLURM to allocate more GPUs per node. For this section of the tutorial, 2 GPUs on a single node are utilized. #SBATCH --nodes=1 # Number of nodes #SBATCH --ntasks-per-node=2 # Tasks per node (GPUs per node) #SBATCH --gpus-per-node=2 # GPUs per node Further changes to the SLURM script include lines to retrieve environment variables set by the SLURM scheduler to define the nodes' address and a random port which are used to establish communication between processes during training . This communication will be done using the NCCL backend, which must be also loaded. World size can be directly calculated in the SLURM script using the environment variables, which as stated before is the total number of GPUs available. Finally to the srun command, add --export=ALL to ensure the environment variables are passed to the srun job. A snapshot of these changes is shown below. \u2139\ufe0f The full run_multigpu.SLURM can be found here . module load CUDA/11.7.0 module load NCCL/2.12.12-GCCcore-11.3.0-CUDA-11.7.0 module load PyTorch/1.12.0-foss-2022a-CUDA-11.7.0 module load cuDNN/8.4.1.50-CUDA-11.7.0 module load torchvision/0.13.1-foss-2022a-CUDA-11.7.0 export MASTER_ADDR=$(scontrol show hostname $SLURM_NODELIST | head -n 1) export MASTER_PORT=$(shuf -i 29500-65535 -n 1) export WORLD_SIZE=$(($SLURM_NNODES * $SLURM_NTASKS_PER_NODE)) srun --export=ALL python HPC_tutorial/multigpu_example.py \\ --batch_size 16 \\ --epochs 5 \\ --lr 0.001 Moving on to the Python script, the environment variables ( MASTER_ADDR , MASTER_PORT and WORLD_SIZE ) are exported from the SLURM scheduler to be used by DDP, and are placed as global variables at the start of the script, after the import statements. import os import argparse import torch import torch.nn as nn import torch.optim as optim from torchvision import datasets, transforms from torch.utils.data import DataLoader, DistributedSampler from model import CNN_classifier import time MASTER_ADDR = os.environ[\"MASTER_ADDR\"] MASTER_PORT = os.environ[\"MASTER_PORT\"] WORLD_SIZE = int(os.environ[\"WORLD_SIZE\"]) The next adjustment to the script happens to the main function (which has been renamed to the worker function). Since SLURM will launch as many processes as there are GPUs, the rank of the process can be defined as the process ID. Next, the process group must be initialized with some key parameters. nccl is chosen as the backend, the world_size and rank parameters are added, and lastly specify the init_method = 'env://' to indicate the MASTER_ADDR and MASTER_PORT environment variables should be used to configure the communication. Finally, the current process is assigned to its corresponding GPU based on its rank . The device object is created with the following syntax cuda:<rank> , which is used send the model and data to the approprate GPU. def worker(args): rank = int(os.environ[\"SLURM_PROCID\"]) torch.distributed.init_process_group( backend='nccl', world_size=WORLD_SIZE, rank=rank, init_method='env://' torch.cuda.set_device(rank) device = torch.device(f\"cuda:{rank}\") ) When implementing data parallelism, it is unnecessary to download the entire dataset on all devices. One device can download the dataset and share it with the rest of the GPUs. To do this, the dataset download command is changed to only execute on rank 0 . To ensure device synchronization, torch.distributed.barrier() is called, which instructs the GPUs to wait until all other devices reach that same point in the script before continuing. Next the dataset is loaded on all other GPUs if rank == 0: train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform) torch.distributed.barrier() train_dataset = datasets.MNIST('./data', train=True, download=False, transform=transform) Next, a DistributedSampler object is defined, which ensures that workload is distributed across all GPUs that are apart of the world. To ensure the dataset is split into manageable batches, the sampler is combined with the Dataloader object. train_sampler = DistributedSampler( train_dataset, num_replicas=WORLD_SIZE, rank=rank ) train_loader = DataLoader( train_dataset, batch_size=args.batch_size, drop_last=True, sampler=train_sampler ) The model is then wrapped with DistributedDataParallel , which will handle the multi-GPU training, ensuring the gradients will be synchronized across all processes after the forward pass. Next, the device_ids = [rank] is specified to define the GPU on which the model will run for the current process. model = CNN_classifier().to(device) model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[rank]) Stepping into the train() function, torch.distributed.barrier() is called at the end of the epoch to ensure synchronization during training and to avoid runtime errors. Optionally, rank parameter is added to the function call to avoid duplicate printing in the output call by specifying one GPU to be the logging device. def train(model, dataloader: DataLoader, args, device,rank): criterion = nn.NLLLoss() optimizer = optim.Adam(params=model.parameters(), lr = args.lr) model.train() if rank == 0: print(\"Entering training loop...\") for epoch in range(1, args.epochs + 1): epoch_loss: float = 0.0 for batch_idx, (data,target) in enumerate(dataloader): data = data.to(device) target = target.to(device) output = model(data) loss = criterion(output, target) optimizer.zero_grad() loss.backward() optimizer.step() epoch_loss += loss.item() #monitoring if rank == 0 and batch_idx % 100 == 0: print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(dataloader.dataset)} ' f'({100. * batch_idx / len(dataloader):.0f}%)]\\tLoss: {loss.item():.6f}') torch.distributed.barrier() if rank == 0: print(\"Exiting training loop...\") Last and most importantly, at the end of the worker function, torch.distributed.destroy_process_group() is called. This ensures all resources tied to distributed training are properly released. if rank == 0: print(f\"Training time = {time_stop - time_start}\") torch.distributed.destroy_process_group() if __name__ == \"__main__\" : worker(args=args) \u2139\ufe0f The full cnn_multigpu.py can be found here . To run this example, first we need to create the cnn_multigpu.py and run_multigpu.SLURM as follows: [smillas@front02 ~]$ cd $HOME/tutorial_04 [smillas@front02 tutorial_04]$ touch cnn_multigpu.py [smillas@front02 tutorial_04]$ nano cnn_multigpu.py # copy the code from source Python file [smillas@front02 tutorial_04]$ touch run_multigpu.SLURM [smillas@front02 tutorial_04]$ chmod +x run_multigpu.SLURM # make the file executable [smillas@front02 tutorial_04]$ nano run_multigpu.SLURM # copy the source SLURM script where the source files can be found from cnn_multigpu.py and run_multigpu.SLURM . Once the two files are ready, submit the job using: [smillas@front02 tutorial_04]$ sbatch run_multigpu.SLURM After the job finishes, navigate to the logs/ directory and open the multigpu_<job_id>.out file and observe the difference. MASTER_ADDR: gpu06 MASTER_PORT: 31315 WORLD_SIZE: 2 Entering training loop... Train Epoch: 1 [0/60000 (0%)] Loss: 2.319880 . . . Train Epoch: 5 [27200/60000 (91%)] Loss: 0.000118 Train Epoch: 5 [28800/60000 (96%)] Loss: 0.000066 Exiting training loop... Training time = 42.064579248428345 By utilizing multiple GPUs, we achieve a much faster training time . Cyclone offers 4 GPUs per node . But multiple nodes can be used to further speed up training with minimal changes to the code base.","title":"4.6.3. Running Python Scripts on Multi-GPU (Single Node) using DDP"},{"location":"tutorials/t04_scaling_python_workloads/#464-running-python-scripts-on-multi-gpu-multi-node-with-ddp","text":"Using the same parallelization technique and some simple changes to the previous SLURM and Python scripts, more compute resources can be leveraged to further speed up the AI models' training by utilizing multiple nodes. To do this, some changes must first be made to the SLURM script. These changes instruct the SLURM scheduler to allocate two GPUs on two nodes , for a total of four GPUs. #SBATCH --nodes=2 # Number of nodes #SBATCH --ntasks-per-node=2 # Tasks per node (GPUs per node) #SBATCH --gpus-per-node=2 # Number of GPUs per node Next change is the addition of the NODE_RANK environment variable which will equal the unique identifier of the current node in the distributed setup. This can be obtained through the $SLURM_NODEID environment variable, provided by SLURM. export MASTER_ADDR=$(scontrol show hostname $SLURM_NODELIST | head -n 1) export MASTER_PORT=$(shuf -i 29500-65535 -n 1) export WORLD_SIZE=$(($SLURM_NNODES * $SLURM_NTASKS_PER_NODE)) export NODE_RANK=$SLURM_NODEID \u2139\ufe0f The full run_multinode.SLURM can be found here . In the Python script, the first change that must be made is the addition of the NODE_RANK global variable, which will be used in the worker function for logging purposes. MASTER_ADDR = os.environ[\"MASTER_ADDR\"] MASTER_PORT = os.environ[\"MASTER_PORT\"] WORLD_SIZE = int(os.environ[\"WORLD_SIZE\"]) NODE_RANK = int(os.environ[\"NODE_RANK\"]) In the worker function, the concepts of local_rank and global_rank are introduced and defined using the SLURM scheduler environment variables. global_rank = int(os.environ[\"SLURM_PROCID\"]) local_rank = int(os.environ[\"SLURM_LOCALID\"]) The global_rank is the unique identity assigned to each process (or GPU) as part of the general world and is used for inter-process communication and coordination across the entire cluster . The local_rank is the unique identifier assigned to a process as part of a node and is used to assign and manage GPU usage within a specific node in DDP . Moving on, a few changes must be made to the rank assignment on the various function calls. In the torch.distributed.init_process_group() the global_rank is used to uniquely identify each process in the distributed training across all nodes. It ensures proper coordination and communication in the entire distributed world. torch.distributed.init_process_group( backend='nccl', world_size=WORLD_SIZE, rank=global_rank, init_method='env://' ) Next, when setting the device, the local_rank is used to specify which GPU on the current node the process will use. Each process must operate on a separate GPU within the same node. torch.cuda.set_device(local_rank) device = torch.device(f\"cuda:{local_rank}\") Afterwards, when downloading the dataset, local_rank is used to share the dataset with all other GPUs on the same node . if local_rank == 0: train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform) Furthermore, when wrapping the model with DDP, The local_rank is used to bind the DDP instance to the specific GPU that the process is operating on, ensuring the process handles only its assigned device. model = nn.parallel.DistributedDataParallel(model, device_ids=[local_rank]) Lastly, when logging during training, the global_rank is used to avoid redundant log statements in the output. train(model=model, dataloader=train_loader, args=args, device=device, rank=global_rank) \u2139\ufe0f The full cnn_multinode.py can be found here . To run this example, first we need to create the cnn_multinode.py and run_multinode.SLURM as follows: [smillas@front02 ~]$ cd $HOME/tutorial_04 [smillas@front02 tutorial_04]$ touch cnn_multinode.py [smillas@front02 tutorial_04]$ nano cnn_multinode.py # copy the code from source Python file [smillas@front02 tutorial_04]$ touch run_multinode.SLURM [smillas@front02 tutorial_04]$ chmod +x run_multinode.SLURM # make the file executable [smillas@front02 tutorial_04]$ nano run_multinode.SLURM # copy the source SLURM script where the source files can be found from cnn_multinode.py and run_multinode.SLURM . Once the two files are ready, submit the job using: [smillas@front02 tutorial_04]$ sbatch run_multigpu.SLURM After the job finishes, navigate to the logs/ directory and open the multinode_<job_id>.out file and observe the difference. MASTER_ADDR: gpu06 MASTER_PORT: 57049 WORLD_SIZE: 4 NODE_RANK: 0 Rank 1 on Node 0: Initializing process group. Rank 3 on Node 1: Initializing process group. Rank 2 on Node 1: Initializing process group. Rank 0 on Node 0: Initializing process group. Entering training loop... . . . Train Epoch: 5 [12800/60000 (85%)] Loss: 0.000021 Train Epoch: 5 [14400/60000 (96%)] Loss: 0.000576 Exiting training loop... Training time = 25.612423181533813","title":"4.6.4. Running Python Scripts on Multi-GPU (Multi-Node) with DDP"},{"location":"tutorials/t04_scaling_python_workloads/#465-runtime-results","text":"A comparison of each version with respect to a baseline setup. The baseline here is selected to be the single GPU version. Version Number of GPUs Runtime (s) SpeedUp (times) CPU-only 0 192.25 0.38 (or 2.63x slower) Single GPU 1 73.48 1 Single-node GPU 2 42.06 1.75 Multi-node GPU 4 25.61 2.86","title":"4.6.5. Runtime Results"},{"location":"tutorials/t04_scaling_python_workloads/#47-recap-and-troubleshooting","text":"In this tutorial, we covered the process of scaling Python workloads on Cyclone, focusing on the following key concepts: Resource Allocation with SLURM Setting up SLURM directives for various configurations, from single CPU to multi-node GPU training. Using environment variables such as MASTER_ADDR, MASTER_PORT, and WORLD_SIZE for distributed computing. Python Script Modifications Adapting scripts for different hardware configurations (CPU, single GPU, multi-GPU, and multi-node setups). Leveraging DistributedDataParallel to automate data parallelism across GPUs. While Using Cyclone for distributed training and complex computations, some issues may be encountered with regards to job submission or RuntimeErrors SLURM job fails to launch Problem: SLURM job fails with an error indicating incorrect directives. Solution: Double-check SLURM script parameters ( --nodes , --gpus-per-node , --partition ). Ensure they match the resources available in the partition. Runtime errors Problem: Tensor or model mismatch errors during training. Solution: Ensure both the model and data tensors are moved to the correct device using model.to(device) and data.to(device) . Problem: Processes fail to communicate due to incorrect master address or port. Solution: Verify that MASTER_ADDR and MASTER_PORT are correctly set in the SLURM script and ensure network connectivity between nodes. Problem: Training does not scale as expected. Solution: Ensure efficient resource utilization by setting appropriate batch sizes and verifying GPU utilization using monitoring tools (e.g., nvidia-smi ).","title":"4.7. Recap and Troubleshooting"},{"location":"tutorials/t05_compiling_and_running_code_with_slurm/","text":"5. Compiling and Running C/C++ Code on Cyclone with SLURM 5.1. Overview This tutorial teaches participants how to compile and run C/C++ programs on Cyclone using SLURM in various configurations. Participants will start with simple \"Hello, World\" programs and progress through advanced setups such as multi-threaded , GPU-accelerated , and distributed (MPI) programs. Note that the purpose of this tutorial is not to teach how to write parallel application, rather to demonstrate how SLURM enables effective resource utilization for C/C++ applications in HPC. 5.2. Learning Objectives By the end of this tutorial, participants will be able to: Compile and execute C/C++ programs on Cyclone in various configurations (serial, multi-threaded, GPU, and distributed). Write SLURM scripts to run C/C++ programs using appropriate resource allocations. Understand how to utilize MPI, OpenMP, and CUDA for scaling C/C++ workloads. Identify and address common errors when compiling and running C/C++ code on Cyclone. 5.3. Prerequisites T01 - Introduction to HPC Systems: This tutorial will give you some basic knowledge on HPC systems and basic terminologies. T02 - Accessing and Navigating Cyclone: This tutorial will give you some basic knowledge on how to connect, copy files and navigate the HPC system. T03 - Setting Up and Using Development Tools: This tutorial will guide you through creating python environments which will be used as notebook kernels throughout this tutorial. 5.4. Introduction to Compiling and Running C/C++ on Cyclone The tutorial begins by introducing Cyclone\u2019s compiler environment. Participants will gain an understanding of the available compilers, such as gcc for general C/C++ compilation, nvcc for GPU programming, and mpicc for distributed MPI applications. This section provides a high-level overview of the workflow, preparing participants for hands-on implementation. 5.4.1. High-Level Overview of the Workflow for Compiling and Running C/C++ Code on Cyclone The process of compiling and running C/C++ programs on Cyclone involves several key steps to ensure efficient execution on the HPC system. Both compilation and code execution should take place on the compute nodes of Cyclone. However, for small compilation times (e.g., a few seconds) the compilation can also take place on the login nodes. Here\u2019s an overview of the workflow that assumes compilation on the login nodes: Write Your C/C++ Code: First, you will write your C or C++ source code using your preferred text editor or integrated development environment (IDE). This code could range from simple serial programs to complex multi-threaded or GPU-accelerated applications. Select the Appropriate Compiler: Depending on your application\u2019s needs, choose the appropriate compiler. Cyclone offers a variety of compilers, including gcc (GNU Compiler Collection), Intel compilers , LLVM (Clang), and nvcc for CUDA-based programs. You can use modules to load the desired compiler version based on your program's requirements. Compile the Code: Using the selected compiler, you will compile your C/C++ code. This step converts the source code into an executable program. If necessary, you can add flags to enable optimizations (please see here ) or link with external libraries (e.g., for parallel processing or GPU computation). Note that for lightweight compilation (i.e., few files), this can take place on the login nodes. Otherwise if a library (or a big codebase) is compiled, this must be done in the compute nodes and by submiting a job via SLURM. Write a SLURM Job Script: To run your code on Cyclone, you will write a SLURM job script . This script specifies how many CPU cores, how much memory, and which partition to use for your job. It also includes the commands to run your C/C++ compiled program. SLURM helps manage the allocation of resources and ensures fair access to the system. Note that if you are compiling on the compute nodes through SLURM, you can create two different scripts -one for compiling the code, and one for running it- to avoid recompiling the code every time. Submit the Job to SLURM: After compiling the program, you will submit your job to SLURM using the sbatch command. SLURM will handle resource allocation and job execution based on your job script\u2019s configuration. Monitor and Debug: While your job is running, you can monitor its status using commands like squeue (to see running jobs) or sacct (to check completed jobs). If issues arise, you can debug your program by adjusting the SLURM script, compiling with different flags, or checking for errors in the job's output. Post-Execution: Once the job finishes, you can review the output and results. If there are issues or further optimization is needed, you may need to modify your code, recompile, and adjust your SLURM job script accordingly. This workflow ensures that your C/C++ applications are compiled, run, and optimized efficiently on the Cyclone HPC system, leveraging SLURM for resource management and scalability. 5.4.2. Available Compilers on Cyclone Cyclone offers several compilers, each suited for different types of C/C++ workloads. The table below summarizes the compilers available and their corresponding C and C++ compilers: Compiler Collection Description C Compiler C++ Compiler Compiling Example GNU Compiler Collection The standard compiler for general-purpose C/C++ programs. Optimized for general tasks. gcc g++ gcc -o my_program my_program.c Intel C/C++ Compilers High-performance compilers optimized for Intel CPUs. Offers better optimization for Intel hardware. icc icpc icc -o my_program my_program.c NVIDIA CUDA Compiler NVIDIA\u2019s compiler for CUDA, used for compiling GPU-accelerated programs. Specifically for GPU workloads. nvcc nvcc nvcc -o my_gpu_program my_gpu_program.cu MPI Compiler Compiler for distributed programs using MPI, which enables communication across multiple nodes. mpicc mpicxx mpicc -o my_mpi_program my_mpi_program.c LLVM A compiler infrastructure that supports various programming languages. Known for advanced optimization capabilities. clang clang++ clang -o my_program my_program.c 5.4.3 How to Change Compilers Using Modules To view the available compilers on Cyclone, use the following command: module avail $COMPILER_NAME where $COMPILER_NAME can be GCC , intel-compilers , LLVM , CUDA or OpenMPI , representing all available versions for each compiler vendor. An example output at the time of writing this tutorial can be: [cstyl@front02 ~]$ module spider GCC ---------------------------------------------------------------------------- GCC: ---------------------------------------------------------------------------- Description: The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Java, and Ada, as well as libraries for these languages (libstdc++, libgcj,...). Versions: GCC/8.3.0 GCC/10.2.0 GCC/11.2.0 GCC/11.3.0 GCC/12.2.0 GCC/12.3.0 GCC/13.2.0 Other possible modules matches: GCCcore To use a specific compiler, you need to load the corresponding module. Following the previous example, to load v11.2.0 of GCC : [cstyl@front02 ~]$ gcc --version gcc (GCC) 8.5.0 20210514 (Red Hat 8.5.0-10) Copyright (C) 2018 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. [cstyl@front02 ~]$ module load GCC/11.2.0 [cstyl@front02 ~]$ gcc --version gcc (GCC) 11.2.0 Copyright (C) 2021 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. This command will load GCC version 11.2.0 , and all subsequent compilation commands will use this version. If you want to switch to a different compiler version, use: [cstyl@front02 ~]$ module swap GCC GCC/12.3.0 The following have been reloaded with a version change: 1) GCC/11.2.0 => GCC/12.3.0 2) GCCcore/11.2.0 => GCCcore/12.3.0 3) binutils/2.37-GCCcore-11.2.0 => binutils/2.40-GCCcore-12.3.0 4) zlib/1.2.11-GCCcore-11.2.0 => zlib/1.2.13-GCCcore-12.3.0 [cstyl@front02 ~]$ gcc --version gcc (GCC) 12.3.0 Copyright (C) 2022 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Similar process can be followed for all other available compilers. 5.4. Serial C/C++ Programs In this section, we will start with a basic \"Hello, World\" C program to understand the workflow for compiling and running serial programs on Cyclone. You will learn how to compile a C program using the gcc compiler, write a simple SLURM job script to run the program on a single CPU core, and submit the job for execution. 5.4.1. Write the \"Hello, World\" C Program First, create a simple C program that prints \"Hello, World from Cyclone!\" to the console. This program will serve as our example for compiling and running a serial C program. Create a file called hello.c with the following code: #include <stdio.h> int main() { printf(\"Hello, World from Cyclone!\\n\"); return 0; } 5.4.2. Write the SLURM Job Script Next, you will create a SLURM job script to compile and run your program on Cyclone. This script will request the necessary resources (a single CPU core in this case) and manage the execution of the program. Create a file called run_hello_c.slurm with the following contents: #!/bin/bash #SBATCH --job-name=hello_c # Name of the job #SBATCH --output=hello_c.out # Output file for standard output #SBATCH --error=hello_c.err # Output file for error messages #SBATCH --partition=cpu # Which partition to run the job on (CPU partition) #SBATCH --ntasks=1 # Number of tasks (1 task for serial) #SBATCH --time=00:10:00 # Maximum runtime (10 minutes) #SBATCH --account=<your_account> # Replace with your account name module load GCC/11.3.0 # Load the GCC compiler module # Compile the program gcc -o hello hello.c # Run the program using srun srun ./hello In this script: #SBATCH directives are used to specify job settings such as the job name, output and error file locations, partition (CPU), and the amount of time allowed for the job to run. The module load command loads the required GCC compiler module for compiling the program. gcc -o hello hello.c compiles the C program using the gcc compiler and generates an executable called hello. srun ./hello runs the compiled program using srun, which ensures that the job is executed according to SLURM\u2019s resource management system. It is worth pointing out that the compilation of such a small code could have been done directly on the login node, or on a separate SLURM script so that we avoid recompiling every time we want to run the example. 5.4.3. Submit the Job Now that you\u2019ve written both the C program and the SLURM job script, you can submit your job to SLURM using the sbatch command: sbatch run_hello_c.slurm This command submits your job script to SLURM, which will allocate the requested resources (in this case, one CPU core) and execute the program. SLURM will place the job in the queue, and once resources are available, it will run your program. 5.4.4. Monitor the Job After submitting the job, you can monitor its status using the squeue command: squeue -u <your_username> This will show the status of your job in the queue. Once the job is completed, you can check the output and error files to see the results and any messages generated during execution. cat hello_c.out # Check the output cat hello_c.err # Check for any error messages 5.4.5. Using Alternative Compilers Sometimes any of the alternative compilers available on Cyclone are desired to be used, such as Intel Compilers which can sometimes offer performance optimizations on Intel CPUs. If you wish to use the Intel compiler instead of GCC, you can modify your job script by replacing the GCC module with the Intel module and using the Intel compiler commands: Modify the job script to load the Intel module: module load intel-compilers/2022.2.1 # Load the Intel compiler module icc -o hello hello.c # Compile using Intel compiler (icc) The rest of the process remains the same. You can submit and monitor the job as you did before, using the Intel compiler for potential performance improvements. 5.4.6 Using Optimisation Flags General Optimization Levels: -O0 : No optimization (default). -O1 : Basic optimizations. -O2 : Moderate optimizations without impacting debugging. -O3 : Aggressive optimizations, often enabling vectorization and loop unrolling. Advanced Flags: -ffast-math : Enables faster floating-point calculations (may reduce precision). -funroll-loops : Unrolls loops to reduce overhead. -march=native : Optimizes code for the architecture of the system being used. -flto : Enables link-time optimization. -fopenmp : Enables OpenMP for parallel programming. Each compiler has its specific flags. Refer to its documentation for more options. Compile with Optimization: gcc -O3 -march=native -funroll-loops hello.c -o hello Compiling code with -O3 -march=native -funroll-loops will result in faster execution runtimes. 5.4.7 Additional Testing with Intel Compiler: If available, try compiling with Intel's compiler: module load intel icc -O3 -xHost -ipo hello.c -o hello_intel 5.5. Multi-Threaded (OpenMP) C/C++ Programs Modern HPC systems, such as Cyclone, provide multiple cores and threads, making parallelism an essential aspect of achieving high performance. OpenMP (Open Multi-Processing) is a widely used API for parallel programming in C, C++, and Fortran. It enables shared-memory parallelism by allowing programs to split tasks across multiple threads. OpenMP uses compiler directives (pragmas) to parallelize code without major modifications. By adding `#pragma` statements, loops or sections of code can be executed concurrently within a single node and on multiple cores. To compile programs with OpenMP support, you need to use the following flags: - **GCC**: `-fopenmp` - **Intel Compiler**: `-qopenmp` - **Clang**: `-fopenmp` (requires linking to the `libomp` library) For example, assuming we are compiling the source file `hello_openmp.c` that contains OpenMP code. Then the compilation step would look like: gcc -O3 -fopenmp hello_openmp.c -o hello_openmp 5.5.1. Write the OpenMP \"Hello, World\" C Program First, create a simple OpenMP program that prints \"Hello, World from thread $THREAD_ID out of $TOTAL_THREADS!\" to the console. Create a file called hello_openmp.c with the following code: #include <stdio.h> #include <omp.h> int main() { #pragma omp parallel { printf(\"Hello from thread %d out of %d\\n\", omp_get_thread_num(), omp_get_num_threads()); } return 0; } Notice the addition of `#pragma omp parallel` and the curly brackets, indicating that the enclosed code is code running in parallel across multiple threads. 5.5.2. Write the SLURM Job Script Next, you will create a SLURM job script to run your program on Cyclone. This script will request the necessary resources (a single CPU core in this case) and manage the execution of the program. Create a file called run_openmp.slurm with the following contents: #!/bin/bash #SBATCH --job-name=openmp #SBATCH --output=openmp.out #SBATCH --error=openmp.err #SBATCH --partition=cpu #SBATCH --ntasks=1 #SBATCH --cpus-per-task=8 # Runs on 8 threads #SBATCH --time=00:15:00 #SBATCH --account=<your_account> module load GCC/11.3.0 export OMP_NUM_THREADS=8 srun ./hello_openmp In this script, the main differences from the Serial case are: #SBATCH --cpus-per-task=8 launches a task that runs on 8 threads (CPU cores). In practise, this number can go up to 40 (i.e., the maximum number of CPU cores available on each node). export OMP_NUM_THREADS=8 sets the OMP_NUM_THREADS environment variable. This ensures that OpenMP's runtime will be correctly set to operate with 8 threads, where needed. 5.5.3. Submit the Job Now that you\u2019ve written both the OpenMP program and the SLURM job script, you can submit your job to SLURM using the sbatch command: sbatch run_openmp.slurm This command submits your job script to SLURM, which will allocate the requested resources (in this case, one CPU core) and execute the program. SLURM will place the job in the queue, and once resources are available, it will run your program. 5.5.4. Monitor the Job After submitting the job, you can monitor its status using the squeue command: squeue -u <your_username> This will show the status of your job in the queue. Once the job is completed, you can check the output and error files to see the results and any messages generated during execution. cat openmp.out # Check the output cat openmp.err # Check for any error messages 5.6. GPU-Accelerated C/C++ Programs In this section, the objective is to learn how to extend C/C++ programs to leverage the power of GPU acceleration using CUDA. The section begins with a basic \"Hello, World\" example, demonstrating how to write a CUDA kernel, compile and run it on the GPU. We will also learn how to write SLURM job scripts to request GPU resources, compile their programs using the nvcc compiler, and execute them on the available GPUs in Cyclone. This section serves as an introduction to CUDA programming, providing a foundation for more complex GPU-accelerated applications. 5.6.1. Write the GPU-Accelerated Program First, create a simple CUDA program that prints \"Hello, World from GPU thread $THREAD_ID!\" to the console. Create a file called hello_cuda.cu with the following code: #include <stdio.h> #include <cuda_runtime.h> __global__ void helloFromGPU() { printf(\"Hello, World from GPU thread %d\\n\", threadIdx.x); } int main() { helloFromGPU<<<1, 10>>>(); cudaDeviceSynchronize(); return 0; } This CUDA program demonstrates how to leverage the GPU for parallel execution. The `helloFromGPU` function is marked with the `__global__` keyword, indicating that it will run on the GPU-this function is also known as the kernel. The program launches 10 threads in a single block using the syntax `helloFromGPU < < < 1, 10>>>();`, where the first parameter specifies the number of blocks (1 in this case) and the second parameter specifies the number of threads per block (10 threads). Each thread runs the same function, but since each thread has a unique `threadIdx.x`, it prints a message with its own thread index (e.g., \"Hello, World from GPU thread 0\", \"Hello, World from GPU thread 1\", etc.). The program uses `cudaDeviceSynchronize();` to ensure that the CPU waits for all GPU threads to finish before the program exits, which is important for ensuring that all output is printed correctly. The key GPU-specific concepts in this program include **thread management** and **parallel execution**. CUDA allows for running thousands of threads in parallel, and in this case, 10 threads run concurrently in one block. Each thread executes the `helloFromGPU` function and prints its thread index. This program serves as a basic example of how to use CUDA for GPU-accelerated tasks, illustrating how thread indices can be used to manage parallel work. The `__global__` function is executed on the GPU, and the threads work in parallel to output their unique results, making this an effective demonstration of how to perform parallel processing on the GPU in high-performance computing environments. A detailed explanation on the CUDA programming model can be found on NVIDIA's CUDA Programming Guide . 5.6.2. SLURM Job Script for GPU Next, you will create a SLURM job script to run your program on Cyclone. This script will request the necessary resources and manage the execution of the program. Create a file called run_cuda.slurm with the following contents: #!/bin/bash #SBATCH --job-name=hello_gpu #SBATCH --output=hello_gpu.out #SBATCH --error=hello_gpu.err #SBATCH --partition=gpu # Use GPU partition #SBATCH --ntasks=1 # Single task #SBATCH --gres=gpu:1 # Request one GPU #SBATCH --time=00:10:00 # Maximum runtime (10 minutes) #SBATCH --account=<your_account> # Replace with your account module load CUDA/12.1.1 # Load CUDA module # Compile the program nvcc -o hello_gpu hello_gpu.cu # Run the program srun ./hello_gpu In this script: --gres=gpu:1 specifies the request for one GPU. Maximum number of GPUs allowed per node is 4 . --partition=gpu is required to be able to use the part of the system with the GPUs. module load CUDA/12.1.1 loads the CUDA compiler and relevant libraries for GPU programming. 5.6.3. Submit and Monitor the Job Submit the job using: sbatch hello_gpu.sbatch Monitor the job with squeue and check the output: cat hello_gpu.out # View output cat hello_gpu.err # View error messages 5.7 MPI C/C++ Programs In this section, participants will learn how to write and run MPI (Message Passing Interface) programs, enabling parallel processing across multiple nodes and processors. MPI is a widely used standard for distributed computing, allowing different parts of a program to run concurrently on separate nodes in a cluster, sharing data and coordinating execution. The section begins with a basic MPI \"Hello, World\" program, where participants will use mpicc, the MPI compiler, to compile and run a simple parallel program across multiple tasks. As they progress, they will gain a deeper understanding of how to structure MPI programs for distributed computation, set up communication between processes, and utilize SLURM to manage resource allocation for MPI-based jobs on Cyclone. 5.7.1. Write an MPI \"Hello, World\" Program First, create a simple MPI program that prints \"Hello, World from rank $PROCESS_ID of $NUMBER_OF_PROCESSES!\" to the console. Create a file called hello_mpi.c with the following code: #include <stdio.h> #include <mpi.h> int main(int argc, char *argv[]) { int rank, size; MPI_Init(&argc, &argv); MPI_Comm_rank(MPI_COMM_WORLD, &rank); MPI_Comm_size(MPI_COMM_WORLD, &size); printf(\"Hello, World from rank %d of %d!\\n\", rank, size); MPI_Finalize(); return 0; } The MPI \"Hello, World\" program is a simple example that demonstrates how to use the Message Passing Interface (MPI) to run a program across multiple processes. The program begins by initializing MPI with the MPI_Init function and then retrieves the rank (unique identifier) of each process using MPI_Comm_rank . It also determines the total number of processes running by calling MPI_Comm_size . Each process then prints a message, including its rank and the total number of processes involved, using the printf function. This allows each process to output \"Hello, World\" from its own unique rank, showing how parallel processes can execute the same code but interact independently. The program ends by finalizing MPI with MPI_Finalize , which ensures proper shutdown of the MPI environment. The use of MPI allows the program to scale across multiple nodes and processors, where each process executes in parallel. This simple example forms the foundation for more complex parallel programs where processes can communicate with each other, pass data, and work together to solve larger problems. 5.7.2. SLURM Job Script for MPI Next, you will create a SLURM job script to run your program on Cyclone. This script will request the necessary resources and manage the execution of the program. Create a file called run_mpi.slurm with the following contents: #!/bin/bash #SBATCH --job-name=hello_mpi #SBATCH --output=hello_mpi.out #SBATCH --error=hello_mpi.err #SBATCH --partition=cpu #SBATCH --ntasks=4 # Runs on 4 Processes #SBATCH --time=00:10:00 #SBATCH --account=<your_account> module load OpenMPI/4.1.6-GCC-13.2.0 # Compile the program mpicc -o hello_mpi hello_mpi.c # Run the program srun ./hello_mpi In this script: The --ntasks=4 directive allocates 4 tasks (one per MPI process). The script loads the appropriate MPI module using module load OpenMPI/4.1.6-GCC-13.2.0 , ensuring that the MPI libraries are available. It then compiles the MPI program using mpicc compiler by OpenMPI and specifies the output executable hello_mpi . Finally, srun is used to run the MPI program across the allocated tasks. The script ensures that the job runs on 4 tasks and that each process executes the MPI program in parallel, distributing the workload across the compute nodes. 5.7.3. Submit and Monitor the Job Submit the job using: sbatch hello_mpi.sbatch Monitor the job with squeue and check the output: cat hello_mpi.out # View output cat hello_mpi.err # View error messages 5.8. Recap and Troubleshooting This tutorial covered various methods for compiling and running C/C++ programs on Cyclone, including serial, multi-threaded, GPU-accelerated, and distributed (MPI) applications. You learned how to use SLURM for resource management and job execution, and how to leverage multiple compilers (e.g., GCC, Intel, CUDA, and MPI) to optimize program performance. Common Issues and Troubleshooting: Compiler Not Found: Ensure the correct module is loaded (e.g., module load GCC/11.3.0). CUDA Errors: If CUDA programs fail to run on GPUs, verify the correct CUDA module is loaded and that GPU resources are requested properly (e.g., with --gres=gpu:1 ). MPI Job Failures: If MPI jobs do not execute, check the job status with squeue and ensure the correct number of tasks is allocated (--ntasks=4). General Debugging: Use the SLURM job script's output and error files to investigate issues during compilation and execution, adjusting flags or resource requests accordingly. By following these steps and utilizing the SLURM job scripts, you should be able to efficiently compile and execute a wide range of C/C++ programs on Cyclone.","title":"Compiling and Running C/C++ Code on Cyclone with SLURM"},{"location":"tutorials/t05_compiling_and_running_code_with_slurm/#5-compiling-and-running-cc-code-on-cyclone-with-slurm","text":"","title":"5. Compiling and Running C/C++ Code on Cyclone with SLURM"},{"location":"tutorials/t05_compiling_and_running_code_with_slurm/#51-overview","text":"This tutorial teaches participants how to compile and run C/C++ programs on Cyclone using SLURM in various configurations. Participants will start with simple \"Hello, World\" programs and progress through advanced setups such as multi-threaded , GPU-accelerated , and distributed (MPI) programs. Note that the purpose of this tutorial is not to teach how to write parallel application, rather to demonstrate how SLURM enables effective resource utilization for C/C++ applications in HPC.","title":"5.1. Overview"},{"location":"tutorials/t05_compiling_and_running_code_with_slurm/#52-learning-objectives","text":"By the end of this tutorial, participants will be able to: Compile and execute C/C++ programs on Cyclone in various configurations (serial, multi-threaded, GPU, and distributed). Write SLURM scripts to run C/C++ programs using appropriate resource allocations. Understand how to utilize MPI, OpenMP, and CUDA for scaling C/C++ workloads. Identify and address common errors when compiling and running C/C++ code on Cyclone.","title":"5.2. Learning Objectives"},{"location":"tutorials/t05_compiling_and_running_code_with_slurm/#53-prerequisites","text":"T01 - Introduction to HPC Systems: This tutorial will give you some basic knowledge on HPC systems and basic terminologies. T02 - Accessing and Navigating Cyclone: This tutorial will give you some basic knowledge on how to connect, copy files and navigate the HPC system. T03 - Setting Up and Using Development Tools: This tutorial will guide you through creating python environments which will be used as notebook kernels throughout this tutorial.","title":"5.3. Prerequisites"},{"location":"tutorials/t05_compiling_and_running_code_with_slurm/#54-introduction-to-compiling-and-running-cc-on-cyclone","text":"The tutorial begins by introducing Cyclone\u2019s compiler environment. Participants will gain an understanding of the available compilers, such as gcc for general C/C++ compilation, nvcc for GPU programming, and mpicc for distributed MPI applications. This section provides a high-level overview of the workflow, preparing participants for hands-on implementation.","title":"5.4. Introduction to Compiling and Running C/C++ on Cyclone"},{"location":"tutorials/t05_compiling_and_running_code_with_slurm/#541-high-level-overview-of-the-workflow-for-compiling-and-running-cc-code-on-cyclone","text":"The process of compiling and running C/C++ programs on Cyclone involves several key steps to ensure efficient execution on the HPC system. Both compilation and code execution should take place on the compute nodes of Cyclone. However, for small compilation times (e.g., a few seconds) the compilation can also take place on the login nodes. Here\u2019s an overview of the workflow that assumes compilation on the login nodes: Write Your C/C++ Code: First, you will write your C or C++ source code using your preferred text editor or integrated development environment (IDE). This code could range from simple serial programs to complex multi-threaded or GPU-accelerated applications. Select the Appropriate Compiler: Depending on your application\u2019s needs, choose the appropriate compiler. Cyclone offers a variety of compilers, including gcc (GNU Compiler Collection), Intel compilers , LLVM (Clang), and nvcc for CUDA-based programs. You can use modules to load the desired compiler version based on your program's requirements. Compile the Code: Using the selected compiler, you will compile your C/C++ code. This step converts the source code into an executable program. If necessary, you can add flags to enable optimizations (please see here ) or link with external libraries (e.g., for parallel processing or GPU computation). Note that for lightweight compilation (i.e., few files), this can take place on the login nodes. Otherwise if a library (or a big codebase) is compiled, this must be done in the compute nodes and by submiting a job via SLURM. Write a SLURM Job Script: To run your code on Cyclone, you will write a SLURM job script . This script specifies how many CPU cores, how much memory, and which partition to use for your job. It also includes the commands to run your C/C++ compiled program. SLURM helps manage the allocation of resources and ensures fair access to the system. Note that if you are compiling on the compute nodes through SLURM, you can create two different scripts -one for compiling the code, and one for running it- to avoid recompiling the code every time. Submit the Job to SLURM: After compiling the program, you will submit your job to SLURM using the sbatch command. SLURM will handle resource allocation and job execution based on your job script\u2019s configuration. Monitor and Debug: While your job is running, you can monitor its status using commands like squeue (to see running jobs) or sacct (to check completed jobs). If issues arise, you can debug your program by adjusting the SLURM script, compiling with different flags, or checking for errors in the job's output. Post-Execution: Once the job finishes, you can review the output and results. If there are issues or further optimization is needed, you may need to modify your code, recompile, and adjust your SLURM job script accordingly. This workflow ensures that your C/C++ applications are compiled, run, and optimized efficiently on the Cyclone HPC system, leveraging SLURM for resource management and scalability.","title":"5.4.1. High-Level Overview of the Workflow for Compiling and Running C/C++ Code on Cyclone"},{"location":"tutorials/t05_compiling_and_running_code_with_slurm/#542-available-compilers-on-cyclone","text":"Cyclone offers several compilers, each suited for different types of C/C++ workloads. The table below summarizes the compilers available and their corresponding C and C++ compilers: Compiler Collection Description C Compiler C++ Compiler Compiling Example GNU Compiler Collection The standard compiler for general-purpose C/C++ programs. Optimized for general tasks. gcc g++ gcc -o my_program my_program.c Intel C/C++ Compilers High-performance compilers optimized for Intel CPUs. Offers better optimization for Intel hardware. icc icpc icc -o my_program my_program.c NVIDIA CUDA Compiler NVIDIA\u2019s compiler for CUDA, used for compiling GPU-accelerated programs. Specifically for GPU workloads. nvcc nvcc nvcc -o my_gpu_program my_gpu_program.cu MPI Compiler Compiler for distributed programs using MPI, which enables communication across multiple nodes. mpicc mpicxx mpicc -o my_mpi_program my_mpi_program.c LLVM A compiler infrastructure that supports various programming languages. Known for advanced optimization capabilities. clang clang++ clang -o my_program my_program.c","title":"5.4.2. Available Compilers on Cyclone"},{"location":"tutorials/t05_compiling_and_running_code_with_slurm/#543-how-to-change-compilers-using-modules","text":"To view the available compilers on Cyclone, use the following command: module avail $COMPILER_NAME where $COMPILER_NAME can be GCC , intel-compilers , LLVM , CUDA or OpenMPI , representing all available versions for each compiler vendor. An example output at the time of writing this tutorial can be: [cstyl@front02 ~]$ module spider GCC ---------------------------------------------------------------------------- GCC: ---------------------------------------------------------------------------- Description: The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Java, and Ada, as well as libraries for these languages (libstdc++, libgcj,...). Versions: GCC/8.3.0 GCC/10.2.0 GCC/11.2.0 GCC/11.3.0 GCC/12.2.0 GCC/12.3.0 GCC/13.2.0 Other possible modules matches: GCCcore To use a specific compiler, you need to load the corresponding module. Following the previous example, to load v11.2.0 of GCC : [cstyl@front02 ~]$ gcc --version gcc (GCC) 8.5.0 20210514 (Red Hat 8.5.0-10) Copyright (C) 2018 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. [cstyl@front02 ~]$ module load GCC/11.2.0 [cstyl@front02 ~]$ gcc --version gcc (GCC) 11.2.0 Copyright (C) 2021 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. This command will load GCC version 11.2.0 , and all subsequent compilation commands will use this version. If you want to switch to a different compiler version, use: [cstyl@front02 ~]$ module swap GCC GCC/12.3.0 The following have been reloaded with a version change: 1) GCC/11.2.0 => GCC/12.3.0 2) GCCcore/11.2.0 => GCCcore/12.3.0 3) binutils/2.37-GCCcore-11.2.0 => binutils/2.40-GCCcore-12.3.0 4) zlib/1.2.11-GCCcore-11.2.0 => zlib/1.2.13-GCCcore-12.3.0 [cstyl@front02 ~]$ gcc --version gcc (GCC) 12.3.0 Copyright (C) 2022 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Similar process can be followed for all other available compilers.","title":"5.4.3 How to Change Compilers Using Modules"},{"location":"tutorials/t05_compiling_and_running_code_with_slurm/#54-serial-cc-programs","text":"In this section, we will start with a basic \"Hello, World\" C program to understand the workflow for compiling and running serial programs on Cyclone. You will learn how to compile a C program using the gcc compiler, write a simple SLURM job script to run the program on a single CPU core, and submit the job for execution.","title":"5.4. Serial C/C++ Programs"},{"location":"tutorials/t05_compiling_and_running_code_with_slurm/#541-write-the-hello-world-c-program","text":"First, create a simple C program that prints \"Hello, World from Cyclone!\" to the console. This program will serve as our example for compiling and running a serial C program. Create a file called hello.c with the following code: #include <stdio.h> int main() { printf(\"Hello, World from Cyclone!\\n\"); return 0; }","title":"5.4.1. Write the \"Hello, World\" C Program"},{"location":"tutorials/t05_compiling_and_running_code_with_slurm/#542-write-the-slurm-job-script","text":"Next, you will create a SLURM job script to compile and run your program on Cyclone. This script will request the necessary resources (a single CPU core in this case) and manage the execution of the program. Create a file called run_hello_c.slurm with the following contents: #!/bin/bash #SBATCH --job-name=hello_c # Name of the job #SBATCH --output=hello_c.out # Output file for standard output #SBATCH --error=hello_c.err # Output file for error messages #SBATCH --partition=cpu # Which partition to run the job on (CPU partition) #SBATCH --ntasks=1 # Number of tasks (1 task for serial) #SBATCH --time=00:10:00 # Maximum runtime (10 minutes) #SBATCH --account=<your_account> # Replace with your account name module load GCC/11.3.0 # Load the GCC compiler module # Compile the program gcc -o hello hello.c # Run the program using srun srun ./hello In this script: #SBATCH directives are used to specify job settings such as the job name, output and error file locations, partition (CPU), and the amount of time allowed for the job to run. The module load command loads the required GCC compiler module for compiling the program. gcc -o hello hello.c compiles the C program using the gcc compiler and generates an executable called hello. srun ./hello runs the compiled program using srun, which ensures that the job is executed according to SLURM\u2019s resource management system. It is worth pointing out that the compilation of such a small code could have been done directly on the login node, or on a separate SLURM script so that we avoid recompiling every time we want to run the example.","title":"5.4.2. Write the SLURM Job Script"},{"location":"tutorials/t05_compiling_and_running_code_with_slurm/#543-submit-the-job","text":"Now that you\u2019ve written both the C program and the SLURM job script, you can submit your job to SLURM using the sbatch command: sbatch run_hello_c.slurm This command submits your job script to SLURM, which will allocate the requested resources (in this case, one CPU core) and execute the program. SLURM will place the job in the queue, and once resources are available, it will run your program.","title":"5.4.3. Submit the Job"},{"location":"tutorials/t05_compiling_and_running_code_with_slurm/#544-monitor-the-job","text":"After submitting the job, you can monitor its status using the squeue command: squeue -u <your_username> This will show the status of your job in the queue. Once the job is completed, you can check the output and error files to see the results and any messages generated during execution. cat hello_c.out # Check the output cat hello_c.err # Check for any error messages","title":"5.4.4. Monitor the Job"},{"location":"tutorials/t05_compiling_and_running_code_with_slurm/#545-using-alternative-compilers","text":"Sometimes any of the alternative compilers available on Cyclone are desired to be used, such as Intel Compilers which can sometimes offer performance optimizations on Intel CPUs. If you wish to use the Intel compiler instead of GCC, you can modify your job script by replacing the GCC module with the Intel module and using the Intel compiler commands: Modify the job script to load the Intel module: module load intel-compilers/2022.2.1 # Load the Intel compiler module icc -o hello hello.c # Compile using Intel compiler (icc) The rest of the process remains the same. You can submit and monitor the job as you did before, using the Intel compiler for potential performance improvements.","title":"5.4.5. Using Alternative Compilers"},{"location":"tutorials/t05_compiling_and_running_code_with_slurm/#546-using-optimisation-flags","text":"","title":"5.4.6 Using Optimisation Flags"},{"location":"tutorials/t05_compiling_and_running_code_with_slurm/#general-optimization-levels","text":"-O0 : No optimization (default). -O1 : Basic optimizations. -O2 : Moderate optimizations without impacting debugging. -O3 : Aggressive optimizations, often enabling vectorization and loop unrolling.","title":"General Optimization Levels:"},{"location":"tutorials/t05_compiling_and_running_code_with_slurm/#advanced-flags","text":"-ffast-math : Enables faster floating-point calculations (may reduce precision). -funroll-loops : Unrolls loops to reduce overhead. -march=native : Optimizes code for the architecture of the system being used. -flto : Enables link-time optimization. -fopenmp : Enables OpenMP for parallel programming. Each compiler has its specific flags. Refer to its documentation for more options.","title":"Advanced Flags:"},{"location":"tutorials/t05_compiling_and_running_code_with_slurm/#compile-with-optimization","text":"gcc -O3 -march=native -funroll-loops hello.c -o hello Compiling code with -O3 -march=native -funroll-loops will result in faster execution runtimes.","title":"Compile with Optimization:"},{"location":"tutorials/t05_compiling_and_running_code_with_slurm/#547-additional-testing-with-intel-compiler","text":"If available, try compiling with Intel's compiler: module load intel icc -O3 -xHost -ipo hello.c -o hello_intel","title":"5.4.7 Additional Testing with Intel Compiler:"},{"location":"tutorials/t05_compiling_and_running_code_with_slurm/#55-multi-threaded-openmp-cc-programs","text":"Modern HPC systems, such as Cyclone, provide multiple cores and threads, making parallelism an essential aspect of achieving high performance. OpenMP (Open Multi-Processing) is a widely used API for parallel programming in C, C++, and Fortran. It enables shared-memory parallelism by allowing programs to split tasks across multiple threads. OpenMP uses compiler directives (pragmas) to parallelize code without major modifications. By adding `#pragma` statements, loops or sections of code can be executed concurrently within a single node and on multiple cores. To compile programs with OpenMP support, you need to use the following flags: - **GCC**: `-fopenmp` - **Intel Compiler**: `-qopenmp` - **Clang**: `-fopenmp` (requires linking to the `libomp` library) For example, assuming we are compiling the source file `hello_openmp.c` that contains OpenMP code. Then the compilation step would look like: gcc -O3 -fopenmp hello_openmp.c -o hello_openmp","title":"5.5. Multi-Threaded (OpenMP) C/C++ Programs"},{"location":"tutorials/t05_compiling_and_running_code_with_slurm/#551-write-the-openmp-hello-world-c-program","text":"First, create a simple OpenMP program that prints \"Hello, World from thread $THREAD_ID out of $TOTAL_THREADS!\" to the console. Create a file called hello_openmp.c with the following code: #include <stdio.h> #include <omp.h> int main() { #pragma omp parallel { printf(\"Hello from thread %d out of %d\\n\", omp_get_thread_num(), omp_get_num_threads()); } return 0; } Notice the addition of `#pragma omp parallel` and the curly brackets, indicating that the enclosed code is code running in parallel across multiple threads.","title":"5.5.1. Write the OpenMP \"Hello, World\" C Program"},{"location":"tutorials/t05_compiling_and_running_code_with_slurm/#552-write-the-slurm-job-script","text":"Next, you will create a SLURM job script to run your program on Cyclone. This script will request the necessary resources (a single CPU core in this case) and manage the execution of the program. Create a file called run_openmp.slurm with the following contents: #!/bin/bash #SBATCH --job-name=openmp #SBATCH --output=openmp.out #SBATCH --error=openmp.err #SBATCH --partition=cpu #SBATCH --ntasks=1 #SBATCH --cpus-per-task=8 # Runs on 8 threads #SBATCH --time=00:15:00 #SBATCH --account=<your_account> module load GCC/11.3.0 export OMP_NUM_THREADS=8 srun ./hello_openmp In this script, the main differences from the Serial case are: #SBATCH --cpus-per-task=8 launches a task that runs on 8 threads (CPU cores). In practise, this number can go up to 40 (i.e., the maximum number of CPU cores available on each node). export OMP_NUM_THREADS=8 sets the OMP_NUM_THREADS environment variable. This ensures that OpenMP's runtime will be correctly set to operate with 8 threads, where needed.","title":"5.5.2. Write the SLURM Job Script"},{"location":"tutorials/t05_compiling_and_running_code_with_slurm/#553-submit-the-job","text":"Now that you\u2019ve written both the OpenMP program and the SLURM job script, you can submit your job to SLURM using the sbatch command: sbatch run_openmp.slurm This command submits your job script to SLURM, which will allocate the requested resources (in this case, one CPU core) and execute the program. SLURM will place the job in the queue, and once resources are available, it will run your program.","title":"5.5.3. Submit the Job"},{"location":"tutorials/t05_compiling_and_running_code_with_slurm/#554-monitor-the-job","text":"After submitting the job, you can monitor its status using the squeue command: squeue -u <your_username> This will show the status of your job in the queue. Once the job is completed, you can check the output and error files to see the results and any messages generated during execution. cat openmp.out # Check the output cat openmp.err # Check for any error messages","title":"5.5.4. Monitor the Job"},{"location":"tutorials/t05_compiling_and_running_code_with_slurm/#56-gpu-accelerated-cc-programs","text":"In this section, the objective is to learn how to extend C/C++ programs to leverage the power of GPU acceleration using CUDA. The section begins with a basic \"Hello, World\" example, demonstrating how to write a CUDA kernel, compile and run it on the GPU. We will also learn how to write SLURM job scripts to request GPU resources, compile their programs using the nvcc compiler, and execute them on the available GPUs in Cyclone. This section serves as an introduction to CUDA programming, providing a foundation for more complex GPU-accelerated applications.","title":"5.6. GPU-Accelerated C/C++ Programs"},{"location":"tutorials/t05_compiling_and_running_code_with_slurm/#561-write-the-gpu-accelerated-program","text":"First, create a simple CUDA program that prints \"Hello, World from GPU thread $THREAD_ID!\" to the console. Create a file called hello_cuda.cu with the following code: #include <stdio.h> #include <cuda_runtime.h> __global__ void helloFromGPU() { printf(\"Hello, World from GPU thread %d\\n\", threadIdx.x); } int main() { helloFromGPU<<<1, 10>>>(); cudaDeviceSynchronize(); return 0; } This CUDA program demonstrates how to leverage the GPU for parallel execution. The `helloFromGPU` function is marked with the `__global__` keyword, indicating that it will run on the GPU-this function is also known as the kernel. The program launches 10 threads in a single block using the syntax `helloFromGPU < < < 1, 10>>>();`, where the first parameter specifies the number of blocks (1 in this case) and the second parameter specifies the number of threads per block (10 threads). Each thread runs the same function, but since each thread has a unique `threadIdx.x`, it prints a message with its own thread index (e.g., \"Hello, World from GPU thread 0\", \"Hello, World from GPU thread 1\", etc.). The program uses `cudaDeviceSynchronize();` to ensure that the CPU waits for all GPU threads to finish before the program exits, which is important for ensuring that all output is printed correctly. The key GPU-specific concepts in this program include **thread management** and **parallel execution**. CUDA allows for running thousands of threads in parallel, and in this case, 10 threads run concurrently in one block. Each thread executes the `helloFromGPU` function and prints its thread index. This program serves as a basic example of how to use CUDA for GPU-accelerated tasks, illustrating how thread indices can be used to manage parallel work. The `__global__` function is executed on the GPU, and the threads work in parallel to output their unique results, making this an effective demonstration of how to perform parallel processing on the GPU in high-performance computing environments. A detailed explanation on the CUDA programming model can be found on NVIDIA's CUDA Programming Guide .","title":"5.6.1. Write the GPU-Accelerated Program"},{"location":"tutorials/t05_compiling_and_running_code_with_slurm/#562-slurm-job-script-for-gpu","text":"Next, you will create a SLURM job script to run your program on Cyclone. This script will request the necessary resources and manage the execution of the program. Create a file called run_cuda.slurm with the following contents: #!/bin/bash #SBATCH --job-name=hello_gpu #SBATCH --output=hello_gpu.out #SBATCH --error=hello_gpu.err #SBATCH --partition=gpu # Use GPU partition #SBATCH --ntasks=1 # Single task #SBATCH --gres=gpu:1 # Request one GPU #SBATCH --time=00:10:00 # Maximum runtime (10 minutes) #SBATCH --account=<your_account> # Replace with your account module load CUDA/12.1.1 # Load CUDA module # Compile the program nvcc -o hello_gpu hello_gpu.cu # Run the program srun ./hello_gpu In this script: --gres=gpu:1 specifies the request for one GPU. Maximum number of GPUs allowed per node is 4 . --partition=gpu is required to be able to use the part of the system with the GPUs. module load CUDA/12.1.1 loads the CUDA compiler and relevant libraries for GPU programming.","title":"5.6.2. SLURM Job Script for GPU"},{"location":"tutorials/t05_compiling_and_running_code_with_slurm/#563-submit-and-monitor-the-job","text":"Submit the job using: sbatch hello_gpu.sbatch Monitor the job with squeue and check the output: cat hello_gpu.out # View output cat hello_gpu.err # View error messages","title":"5.6.3. Submit and Monitor the Job"},{"location":"tutorials/t05_compiling_and_running_code_with_slurm/#57-mpi-cc-programs","text":"In this section, participants will learn how to write and run MPI (Message Passing Interface) programs, enabling parallel processing across multiple nodes and processors. MPI is a widely used standard for distributed computing, allowing different parts of a program to run concurrently on separate nodes in a cluster, sharing data and coordinating execution. The section begins with a basic MPI \"Hello, World\" program, where participants will use mpicc, the MPI compiler, to compile and run a simple parallel program across multiple tasks. As they progress, they will gain a deeper understanding of how to structure MPI programs for distributed computation, set up communication between processes, and utilize SLURM to manage resource allocation for MPI-based jobs on Cyclone.","title":"5.7 MPI C/C++ Programs"},{"location":"tutorials/t05_compiling_and_running_code_with_slurm/#571-write-an-mpi-hello-world-program","text":"First, create a simple MPI program that prints \"Hello, World from rank $PROCESS_ID of $NUMBER_OF_PROCESSES!\" to the console. Create a file called hello_mpi.c with the following code: #include <stdio.h> #include <mpi.h> int main(int argc, char *argv[]) { int rank, size; MPI_Init(&argc, &argv); MPI_Comm_rank(MPI_COMM_WORLD, &rank); MPI_Comm_size(MPI_COMM_WORLD, &size); printf(\"Hello, World from rank %d of %d!\\n\", rank, size); MPI_Finalize(); return 0; } The MPI \"Hello, World\" program is a simple example that demonstrates how to use the Message Passing Interface (MPI) to run a program across multiple processes. The program begins by initializing MPI with the MPI_Init function and then retrieves the rank (unique identifier) of each process using MPI_Comm_rank . It also determines the total number of processes running by calling MPI_Comm_size . Each process then prints a message, including its rank and the total number of processes involved, using the printf function. This allows each process to output \"Hello, World\" from its own unique rank, showing how parallel processes can execute the same code but interact independently. The program ends by finalizing MPI with MPI_Finalize , which ensures proper shutdown of the MPI environment. The use of MPI allows the program to scale across multiple nodes and processors, where each process executes in parallel. This simple example forms the foundation for more complex parallel programs where processes can communicate with each other, pass data, and work together to solve larger problems.","title":"5.7.1. Write an MPI \"Hello, World\" Program"},{"location":"tutorials/t05_compiling_and_running_code_with_slurm/#572-slurm-job-script-for-mpi","text":"Next, you will create a SLURM job script to run your program on Cyclone. This script will request the necessary resources and manage the execution of the program. Create a file called run_mpi.slurm with the following contents: #!/bin/bash #SBATCH --job-name=hello_mpi #SBATCH --output=hello_mpi.out #SBATCH --error=hello_mpi.err #SBATCH --partition=cpu #SBATCH --ntasks=4 # Runs on 4 Processes #SBATCH --time=00:10:00 #SBATCH --account=<your_account> module load OpenMPI/4.1.6-GCC-13.2.0 # Compile the program mpicc -o hello_mpi hello_mpi.c # Run the program srun ./hello_mpi In this script: The --ntasks=4 directive allocates 4 tasks (one per MPI process). The script loads the appropriate MPI module using module load OpenMPI/4.1.6-GCC-13.2.0 , ensuring that the MPI libraries are available. It then compiles the MPI program using mpicc compiler by OpenMPI and specifies the output executable hello_mpi . Finally, srun is used to run the MPI program across the allocated tasks. The script ensures that the job runs on 4 tasks and that each process executes the MPI program in parallel, distributing the workload across the compute nodes.","title":"5.7.2. SLURM Job Script for MPI"},{"location":"tutorials/t05_compiling_and_running_code_with_slurm/#573-submit-and-monitor-the-job","text":"Submit the job using: sbatch hello_mpi.sbatch Monitor the job with squeue and check the output: cat hello_mpi.out # View output cat hello_mpi.err # View error messages","title":"5.7.3. Submit and Monitor the Job"},{"location":"tutorials/t05_compiling_and_running_code_with_slurm/#58-recap-and-troubleshooting","text":"This tutorial covered various methods for compiling and running C/C++ programs on Cyclone, including serial, multi-threaded, GPU-accelerated, and distributed (MPI) applications. You learned how to use SLURM for resource management and job execution, and how to leverage multiple compilers (e.g., GCC, Intel, CUDA, and MPI) to optimize program performance.","title":"5.8. Recap and Troubleshooting"},{"location":"tutorials/t05_compiling_and_running_code_with_slurm/#common-issues-and-troubleshooting","text":"Compiler Not Found: Ensure the correct module is loaded (e.g., module load GCC/11.3.0). CUDA Errors: If CUDA programs fail to run on GPUs, verify the correct CUDA module is loaded and that GPU resources are requested properly (e.g., with --gres=gpu:1 ). MPI Job Failures: If MPI jobs do not execute, check the job status with squeue and ensure the correct number of tasks is allocated (--ntasks=4). General Debugging: Use the SLURM job script's output and error files to investigate issues during compilation and execution, adjusting flags or resource requests accordingly. By following these steps and utilizing the SLURM job scripts, you should be able to efficiently compile and execute a wide range of C/C++ programs on Cyclone.","title":"Common Issues and Troubleshooting:"},{"location":"tutorials/t06_interactive_computing_with_jupyter_notebooks/","text":"6. Interactive Computing on Cyclone with Jupyter Notebooks 6.1 Overview This tutorial introduces participants to running Jupyter Notebooks directly on Cyclone\u2019s compute nodes, enabling interactive workflows for data analysis, AI model development, and other computational tasks. Participants will gain an understanding of the benefits of using Jupyter Notebooks in an HPC environment and learn the step-by-step process to launch and access them. By the end of the tutorial, users will be equipped with the knowledge to set up and interact with Jupyter Notebooks efficiently on Cyclone. 6.2 Learning Objectives By the end of this tutorial, participants will be able to: Understand the advantages of using Jupyter Notebooks on HPC systems for interactive computing. Follow the steps to configure and launch Jupyter Notebooks on Cyclone\u2019s compute nodes. Establish secure SSH tunnels to access notebooks from a local browser. Optimize resource allocation for Jupyter Notebook sessions using SLURM scripts. 6.3 Prerequisites Accessing and Navigating Cyclone . This tutorial will give you some basic knowledge on how to connect, copy files and navigate the HPC system Setting Up and Using Development Tools This tutorial will guide you through creating python environments which will be used as notebook kernels throughout this Tutorial 6.4 Detailed steps 6.4.1. Connect to Cyclone: First, establish a connection to Cyclone using SSH: ssh username@cyclone.hpcf.ac.cy \ud83d\udca1 Tip : Replace username with your actual Cyclone username. If you encounter connection issues, refer back to the Accessing and Navigating Cyclone tutorial from the beginning. 6.4.2. Create an environment with the necessary dependencies: During these steps you might see this in your terminal: Proceed ([y]/n)? Just type the letter y and then press Enter to continue. Step 1: Create a simple conda environment module load Anaconda3 conda create --name notebookEnv Your terminal should look something like this Step 2: Activate your environment conda activate notebookEnv You should see the name of the environment before your username now: Step 3: Install necessary dependencies conda install -c conda-forge notebook \u26a0\ufe0f Note : This installation might take a few minutes. Be patient and don't interrupt the process. When this command finishes, your terminal should look something like this: 6.5. Launching Jupyter on a Compute Node We'll use a pre-configured Slurm script to launch our Jupyter server. Let's break down the key components: Step 1: We setup the basic slurm environment variables so our job can be submitted using sbatch : #!/bin/bash -l #SBATCH --job-name=jupyter_test #SBATCH --partition=gpu # Partition #SBATCH --nodes=1 # Number of nodes #SBATCH --gres=gpu:1 # Number of GPUs #SBATCH --ntasks-per-node=1 # Number of tasks #SBATCH --cpus-per-task=10 # Number of cpu cores #SBATCH --mem=20G #Total memory per node #SBATCH --output=job.%j.out # Stdout (%j=jobId) #SBATCH --error=job.%j.err # Stderr (%j=jobId) #SBATCH --time=1:00:00 # Walltime #SBATCH --reservation=short # Walltime #SBATCH -A p166 # Accounting project In this instance, we're requesting resources - From 1 Node (--nodes=1) in the gpu partition (--partition=gpu) with: - 1 gpu (--gres=gpu:1) - 1 hour (--time=1:00:00) - 20GB of RAM (--mem=20G) - 10 cpu cores. (--cpus-per-task=10) We named the job name jupyter_test and our usage will be deducted from the p166 project. Then the actual script that's going to run on the compute node starts. This configuration requests: - 1 GPU node - 10 CPU cores - 1 hour of compute time - Resources from project p166 Step 2: We load the Anaconda3 module and activate our environment excatly like we did in section 2: # Load any necessary modules and activate environment module load Anaconda3 conda activate notebookEnv Step 3: Configure the jupyter server This piece of the slurm script initialises some basic variables so we can securely connect to our jupyter server: # Add our environment as a notebook kernel python -m ipykernel install --user --name=notebookEnv # Compute node hostname HOSTNAME=$(hostname) # Generate random ports for Jupyter JUPYTER_PORT=$(shuf -i 10000-60000 -n 1) # Generate a random password for Jupyter Notebook PASSWORD=$(openssl rand -base64 12) # Hash the password using Jupyter's built-in function HASHED_PASSWORD=$(python -c \"from jupyter_server.auth import passwd; print(passwd('$PASSWORD'))\") Let's look at it step by step: We start by adding our environment as a notebook kernel. This is done so we can effieciently manage our python packages. You can add more environments for different use cases. For example you can have a conda environment for pytorch and one for tensorflow. # Add our environment as a notebook kernel python -m ipykernel install --user --name=notebookEnv Then we retrieve the hostname or IP of the compute node: # Compute node hostname HOSTNAME=$(hostname) Generate random port numbers so we're less likely to use an already used port. # Generate random ports for Jupyter JUPYTER_PORT=$(shuf -i 10000-60000 -n 1) Generate a random password to avoid unauthorised usage of your jupyter server and HPC resources\" # Generate a random password for Jupyter Notebook PASSWORD=$(openssl rand -base64 12) # Hash the password using Jupyter's built-in function HASHED_PASSWORD=$(python -c \"from jupyter_server.auth import passwd; print(passwd('$PASSWORD'))\") Step 4: Launching the jupyter server We launch the jupyter server with the variables we just generated. Feel free to change the --notebook-dir option to point at whatever directory you want. # Run Jupyter notebook jupyter notebook --port=$JUPYTER_PORT --NotebookApp.password=\"$HASHED_PASSWORD\" --notebook-dir=\"$HOME\" --no-browser > jupyter.log 2>&1 & The jupyter command generates a blocking process, meaning it keeps control of our bash session until we end that process. So we redirect it's output to the jupyter.log file and leave it running as a background process. Step 5: Automatically generating the necessary connection commands Since we want to connect from our personal machine, laptop for example, to the jupyter server running on the compute node, we'll need an SSH tunnel. This tunnel will first create a jump connection from the front node to our assigned compute node, and then bind the port our server is running to our local machine's port. We've prepared a script which automatically generates this command for you: LOGIN_HOST=\"cyclone.hpcf.cyi.ac.cy\" # Prepare the message to be displayed and saved to a file CONNECTION_MESSAGE=$(cat <<EOF ================================================================== Run this command to connect on your jupyter notebooks remotely ssh -N -J ${USER}@${LOGIN_HOST} ${USER}@${HOSTNAME} -L ${JUPYTER_PORT}:localhost:${JUPYTER_PORT} Jupyter Notebook is running at: http://localhost:$JUPYTER_PORT Password to access the notebook: $PASSWORD ================================================================== EOF ) # Print the connection details to both the terminal and a txt file echo \"$CONNECTION_MESSAGE\" | tee ~/connection_info.txt wait Here is the complete script for your convenience: #!/bin/bash -l #SBATCH --job-name=jupyter_test #SBATCH --partition=gpu # Partition #SBATCH --nodes=1 # Number of nodes #SBATCH --gres=gpu:1 # Number of GPUs #SBATCH --ntasks-per-node=1 # Number of tasks #SBATCH --cpus-per-task=10 # Number of cpu cores #SBATCH --mem=20G #Total memory per node #SBATCH --output=job.%j.out # Stdout (%j=jobId) #SBATCH --error=job.%j.err # Stderr (%j=jobId) #SBATCH --time=1:00:00 # Walltime #SBATCH --reservation=short # Walltime #SBATCH -A p166 # Accounting project # Load any necessary modules and activate environment module load Anaconda3 conda activate notebookEnv # Add our environment as a notebook kernel python -m ipykernel install --user --name=notebookEnv # Compute node hostname HOSTNAME=$(hostname) # Generate random ports for Jupyter JUPYTER_PORT=$(shuf -i 10000-60000 -n 1) # Generate a random password for Jupyter Notebook PASSWORD=$(openssl rand -base64 12) # Hash the password using Jupyter's built-in function HASHED_PASSWORD=$(python -c \"from jupyter_server.auth import passwd; print(passwd('$PASSWORD'))\") # Run Jupyter notebook jupyter notebook --port=$JUPYTER_PORT --NotebookApp.password=\"$HASHED_PASSWORD\" --notebook-dir=\"$HOME\" --no-browser > jupyter.log 2>&1 & sleep 5 LOGIN_HOST=\"cyclone.hpcf.cyi.ac.cy\" # Prepare the message to be displayed and saved to a file CONNECTION_MESSAGE=$(cat <<EOF ================================================================== Run this command to connect on your jupyter notebooks remotely ssh -N -J ${USER}@${LOGIN_HOST} ${USER}@${HOSTNAME} -L ${JUPYTER_PORT}:localhost:${JUPYTER_PORT} Jupyter Notebook is running at: http://localhost:$JUPYTER_PORT Password to access the notebook: $PASSWORD ================================================================== EOF ) # Print the connection details to both the terminal and a txt file echo \"$CONNECTION_MESSAGE\" | tee ~/connection_info.txt wait Step 6: Now that everything is configured, let's submit this slurm script and see what it does. After successfully connecting to Cyclone through ssh, use this command: sbatch utils/launch_notebook.sh You might need to change the directory after sbatch . This example assumes you're in the root directory of this repository. You should see something like this in your terminal after using sbatch : Submitted batch job 1034638 In this instance 1034638 is your job id. To view the status of your job you can use the squeue command: squeue -u $USER The output will look like this: Under the ST column you can see the status of your job. In this case R means it's running. If you see CF then it means your node is in its configuration state, waiting 5 minutes should be enough for it to get ready and your have your job running. If you see PD then it means your job is Pending resource allocation, meaning there aren't enough resources and your job has been placed on a queue. When you're sure your job is running, you should also see some new files generated in your directory. job.1034638.out is your jobs stdout job.1034638.err is your jobs stderr jupyter.log is your jupyter server log output Unless you are debugging something these shouldn't concern you. 6.6. Connect to your jupyter server We'll look at two different options on how you can use notebooks running on the now running jupyter server. - Browser - VSCode Before we do that though, we need to create the SSH tunnel we mentioned in the previous section. The connection info was stored in a text file in your home directory with the name connection_info.txt . To view its content you can use your VSCode editor if you're following from Tutorial 03: Setting up and using development tools , or simply use the cat command: cat ~/connection_info.txt The output will look something like: Important: The SSH command should be run on a fresh local terminal! Not already connected to cyclone The ssh command is blocking, meaning nothing will be printed when it's run. You may be prompted for your key's passphrase. ``` 6.6.1 Browser With the SSH tunnel running, our local machine now is now connected to the compute node via the port 11083 in the above example. To launch the jupyter notebook in a browser, just head to the link printed from connection_info.txt , in our case it's http://localhost:11083. You should reach a page asking for the password looking like this: Once we input the password from connection_info.txt , in our case s23un9qxYjpenFnE , and press the Log In button, we're in! Let's create a new notebook! Click the New button Now we can see serveral options: - Notebook kernels - Both kernels have the same python interpreter, the one in our conda environment. - Python 3 (ipykernel) - Default Python kernel - notebookEnv - Our custom kernel we added Terminal - Launches a terminal session on the compute node, you can use this for running htop or nvidia-smi to view hardware utilisation Console - Launches a python interactive shell New File - Create a new file, this might be a text file, a python script or whatever you want. New Folder - Create a new folder If you click on any of the python kernel options, in this case notebookEnv, a new tab in your browser will open with a notebook: 6.6.2. VSCode To view and run notebooks in VSCode we need to have some extentions installed. Searching jupyter in the extensions tab of VSCode should show you something like this: Click install on the one circled and wait for it to be installed. Once that's done, open a folder on your local machine: For this example we used this repo: Right click inside a folder: Make sure to add the .ipyng extention at the end! Now the notebook should be open in your VSCode window. You are now ready to connect this notebook to the jupyter server running on the compute node. We do this by selecting a remote server by pressing the Select Kernel button at the top right of your screen: Then you will see this in the top middle of your screen: Select \"Existing Jupyter Server..\" Add the link that's inside your connection_info.txt Add the password, again found inside the connection_info.txt And finally a display name for your connection, this can be anything you want: Select the appropriate kernel: That's it. Now your notebook is running remotely on the compute node! Adding a couple of cells and calling nvidia-smi shows us the 1 gpu running on gpu01: 6.7. Troubleshooting the SSH tunnel: 6.7.1. Port Conflicts Symptom 1: Error message \"Address already in use\" or unable to connect to the specified port Solutions: Check if the port is already in use: bash lsof -i :PORT_NUMBER # On your local machine Kill any existing SSH tunnels: bash pkill -f \"ssh -N -J\" 6.7.2. Authentication issues Symptom 2: SSH key authentication failures Solutions: Verify your SSH key is properly added to Cyclone: bash ssh-add -l # List loaded keys ssh-add ~/.ssh/id_rsa # Add your key if needed Check key permissions: ```bash chmod 600 ~/.ssh/id_rsa chmod 700 ~/.ssh 6.7.3. General Debugging Tips Check the job output files for errors: bash cat job.[jobid].out cat job.[jobid].err > Replace [jobid] with the your Job ID These commands will print out the contents of the outputs of the job. They might contain some more information that will guide you to find the problem. Some examples: The conda environment name might be wrong. Package dependency issues inside your conda environment. The project you're requesting resources from might not have access to the partition you requested.","title":"Interactive Computing on Cyclone with Jupyter Notebooks"},{"location":"tutorials/t06_interactive_computing_with_jupyter_notebooks/#6-interactive-computing-on-cyclone-with-jupyter-notebooks","text":"","title":"6. Interactive Computing on Cyclone with Jupyter Notebooks"},{"location":"tutorials/t06_interactive_computing_with_jupyter_notebooks/#61-overview","text":"This tutorial introduces participants to running Jupyter Notebooks directly on Cyclone\u2019s compute nodes, enabling interactive workflows for data analysis, AI model development, and other computational tasks. Participants will gain an understanding of the benefits of using Jupyter Notebooks in an HPC environment and learn the step-by-step process to launch and access them. By the end of the tutorial, users will be equipped with the knowledge to set up and interact with Jupyter Notebooks efficiently on Cyclone.","title":"6.1 Overview"},{"location":"tutorials/t06_interactive_computing_with_jupyter_notebooks/#62-learning-objectives","text":"By the end of this tutorial, participants will be able to: Understand the advantages of using Jupyter Notebooks on HPC systems for interactive computing. Follow the steps to configure and launch Jupyter Notebooks on Cyclone\u2019s compute nodes. Establish secure SSH tunnels to access notebooks from a local browser. Optimize resource allocation for Jupyter Notebook sessions using SLURM scripts.","title":"6.2 Learning Objectives"},{"location":"tutorials/t06_interactive_computing_with_jupyter_notebooks/#63-prerequisites","text":"Accessing and Navigating Cyclone . This tutorial will give you some basic knowledge on how to connect, copy files and navigate the HPC system Setting Up and Using Development Tools This tutorial will guide you through creating python environments which will be used as notebook kernels throughout this Tutorial","title":"6.3 Prerequisites"},{"location":"tutorials/t06_interactive_computing_with_jupyter_notebooks/#64-detailed-steps","text":"","title":"6.4 Detailed steps"},{"location":"tutorials/t06_interactive_computing_with_jupyter_notebooks/#641-connect-to-cyclone","text":"First, establish a connection to Cyclone using SSH: ssh username@cyclone.hpcf.ac.cy \ud83d\udca1 Tip : Replace username with your actual Cyclone username. If you encounter connection issues, refer back to the Accessing and Navigating Cyclone tutorial from the beginning.","title":"6.4.1. Connect to Cyclone:"},{"location":"tutorials/t06_interactive_computing_with_jupyter_notebooks/#642-create-an-environment-with-the-necessary-dependencies","text":"During these steps you might see this in your terminal: Proceed ([y]/n)? Just type the letter y and then press Enter to continue.","title":"6.4.2. Create an environment with the necessary dependencies:"},{"location":"tutorials/t06_interactive_computing_with_jupyter_notebooks/#step-1-create-a-simple-conda-environment","text":"module load Anaconda3 conda create --name notebookEnv Your terminal should look something like this","title":"Step 1: Create a simple conda environment"},{"location":"tutorials/t06_interactive_computing_with_jupyter_notebooks/#step-2-activate-your-environment","text":"conda activate notebookEnv You should see the name of the environment before your username now:","title":"Step 2: Activate your environment"},{"location":"tutorials/t06_interactive_computing_with_jupyter_notebooks/#step-3-install-necessary-dependencies","text":"conda install -c conda-forge notebook \u26a0\ufe0f Note : This installation might take a few minutes. Be patient and don't interrupt the process. When this command finishes, your terminal should look something like this:","title":"Step 3: Install necessary dependencies"},{"location":"tutorials/t06_interactive_computing_with_jupyter_notebooks/#65-launching-jupyter-on-a-compute-node","text":"We'll use a pre-configured Slurm script to launch our Jupyter server. Let's break down the key components:","title":"6.5. Launching Jupyter on a Compute Node"},{"location":"tutorials/t06_interactive_computing_with_jupyter_notebooks/#step-1-we-setup-the-basic-slurm-environment-variables-so-our-job-can-be-submitted-using-sbatch","text":"#!/bin/bash -l #SBATCH --job-name=jupyter_test #SBATCH --partition=gpu # Partition #SBATCH --nodes=1 # Number of nodes #SBATCH --gres=gpu:1 # Number of GPUs #SBATCH --ntasks-per-node=1 # Number of tasks #SBATCH --cpus-per-task=10 # Number of cpu cores #SBATCH --mem=20G #Total memory per node #SBATCH --output=job.%j.out # Stdout (%j=jobId) #SBATCH --error=job.%j.err # Stderr (%j=jobId) #SBATCH --time=1:00:00 # Walltime #SBATCH --reservation=short # Walltime #SBATCH -A p166 # Accounting project In this instance, we're requesting resources - From 1 Node (--nodes=1) in the gpu partition (--partition=gpu) with: - 1 gpu (--gres=gpu:1) - 1 hour (--time=1:00:00) - 20GB of RAM (--mem=20G) - 10 cpu cores. (--cpus-per-task=10) We named the job name jupyter_test and our usage will be deducted from the p166 project. Then the actual script that's going to run on the compute node starts. This configuration requests: - 1 GPU node - 10 CPU cores - 1 hour of compute time - Resources from project p166","title":"Step 1: We setup the basic slurm environment variables so our job can be submitted using sbatch:"},{"location":"tutorials/t06_interactive_computing_with_jupyter_notebooks/#step-2-we-load-the-anaconda3-module-and-activate-our-environment-excatly-like-we-did-in-section-2","text":"# Load any necessary modules and activate environment module load Anaconda3 conda activate notebookEnv","title":"Step 2: We load the Anaconda3 module and activate our environment excatly like we did in section 2:"},{"location":"tutorials/t06_interactive_computing_with_jupyter_notebooks/#step-3-configure-the-jupyter-server","text":"This piece of the slurm script initialises some basic variables so we can securely connect to our jupyter server: # Add our environment as a notebook kernel python -m ipykernel install --user --name=notebookEnv # Compute node hostname HOSTNAME=$(hostname) # Generate random ports for Jupyter JUPYTER_PORT=$(shuf -i 10000-60000 -n 1) # Generate a random password for Jupyter Notebook PASSWORD=$(openssl rand -base64 12) # Hash the password using Jupyter's built-in function HASHED_PASSWORD=$(python -c \"from jupyter_server.auth import passwd; print(passwd('$PASSWORD'))\") Let's look at it step by step: We start by adding our environment as a notebook kernel. This is done so we can effieciently manage our python packages. You can add more environments for different use cases. For example you can have a conda environment for pytorch and one for tensorflow. # Add our environment as a notebook kernel python -m ipykernel install --user --name=notebookEnv Then we retrieve the hostname or IP of the compute node: # Compute node hostname HOSTNAME=$(hostname) Generate random port numbers so we're less likely to use an already used port. # Generate random ports for Jupyter JUPYTER_PORT=$(shuf -i 10000-60000 -n 1) Generate a random password to avoid unauthorised usage of your jupyter server and HPC resources\" # Generate a random password for Jupyter Notebook PASSWORD=$(openssl rand -base64 12) # Hash the password using Jupyter's built-in function HASHED_PASSWORD=$(python -c \"from jupyter_server.auth import passwd; print(passwd('$PASSWORD'))\")","title":"Step 3: Configure the jupyter server"},{"location":"tutorials/t06_interactive_computing_with_jupyter_notebooks/#step-4-launching-the-jupyter-server","text":"We launch the jupyter server with the variables we just generated. Feel free to change the --notebook-dir option to point at whatever directory you want. # Run Jupyter notebook jupyter notebook --port=$JUPYTER_PORT --NotebookApp.password=\"$HASHED_PASSWORD\" --notebook-dir=\"$HOME\" --no-browser > jupyter.log 2>&1 & The jupyter command generates a blocking process, meaning it keeps control of our bash session until we end that process. So we redirect it's output to the jupyter.log file and leave it running as a background process.","title":"Step 4: Launching the jupyter server"},{"location":"tutorials/t06_interactive_computing_with_jupyter_notebooks/#step-5-automatically-generating-the-necessary-connection-commands","text":"Since we want to connect from our personal machine, laptop for example, to the jupyter server running on the compute node, we'll need an SSH tunnel. This tunnel will first create a jump connection from the front node to our assigned compute node, and then bind the port our server is running to our local machine's port. We've prepared a script which automatically generates this command for you: LOGIN_HOST=\"cyclone.hpcf.cyi.ac.cy\" # Prepare the message to be displayed and saved to a file CONNECTION_MESSAGE=$(cat <<EOF ================================================================== Run this command to connect on your jupyter notebooks remotely ssh -N -J ${USER}@${LOGIN_HOST} ${USER}@${HOSTNAME} -L ${JUPYTER_PORT}:localhost:${JUPYTER_PORT} Jupyter Notebook is running at: http://localhost:$JUPYTER_PORT Password to access the notebook: $PASSWORD ================================================================== EOF ) # Print the connection details to both the terminal and a txt file echo \"$CONNECTION_MESSAGE\" | tee ~/connection_info.txt wait Here is the complete script for your convenience: #!/bin/bash -l #SBATCH --job-name=jupyter_test #SBATCH --partition=gpu # Partition #SBATCH --nodes=1 # Number of nodes #SBATCH --gres=gpu:1 # Number of GPUs #SBATCH --ntasks-per-node=1 # Number of tasks #SBATCH --cpus-per-task=10 # Number of cpu cores #SBATCH --mem=20G #Total memory per node #SBATCH --output=job.%j.out # Stdout (%j=jobId) #SBATCH --error=job.%j.err # Stderr (%j=jobId) #SBATCH --time=1:00:00 # Walltime #SBATCH --reservation=short # Walltime #SBATCH -A p166 # Accounting project # Load any necessary modules and activate environment module load Anaconda3 conda activate notebookEnv # Add our environment as a notebook kernel python -m ipykernel install --user --name=notebookEnv # Compute node hostname HOSTNAME=$(hostname) # Generate random ports for Jupyter JUPYTER_PORT=$(shuf -i 10000-60000 -n 1) # Generate a random password for Jupyter Notebook PASSWORD=$(openssl rand -base64 12) # Hash the password using Jupyter's built-in function HASHED_PASSWORD=$(python -c \"from jupyter_server.auth import passwd; print(passwd('$PASSWORD'))\") # Run Jupyter notebook jupyter notebook --port=$JUPYTER_PORT --NotebookApp.password=\"$HASHED_PASSWORD\" --notebook-dir=\"$HOME\" --no-browser > jupyter.log 2>&1 & sleep 5 LOGIN_HOST=\"cyclone.hpcf.cyi.ac.cy\" # Prepare the message to be displayed and saved to a file CONNECTION_MESSAGE=$(cat <<EOF ================================================================== Run this command to connect on your jupyter notebooks remotely ssh -N -J ${USER}@${LOGIN_HOST} ${USER}@${HOSTNAME} -L ${JUPYTER_PORT}:localhost:${JUPYTER_PORT} Jupyter Notebook is running at: http://localhost:$JUPYTER_PORT Password to access the notebook: $PASSWORD ================================================================== EOF ) # Print the connection details to both the terminal and a txt file echo \"$CONNECTION_MESSAGE\" | tee ~/connection_info.txt wait","title":"Step 5: Automatically generating the necessary connection commands"},{"location":"tutorials/t06_interactive_computing_with_jupyter_notebooks/#step-6","text":"Now that everything is configured, let's submit this slurm script and see what it does. After successfully connecting to Cyclone through ssh, use this command: sbatch utils/launch_notebook.sh You might need to change the directory after sbatch . This example assumes you're in the root directory of this repository. You should see something like this in your terminal after using sbatch : Submitted batch job 1034638 In this instance 1034638 is your job id. To view the status of your job you can use the squeue command: squeue -u $USER The output will look like this: Under the ST column you can see the status of your job. In this case R means it's running. If you see CF then it means your node is in its configuration state, waiting 5 minutes should be enough for it to get ready and your have your job running. If you see PD then it means your job is Pending resource allocation, meaning there aren't enough resources and your job has been placed on a queue. When you're sure your job is running, you should also see some new files generated in your directory. job.1034638.out is your jobs stdout job.1034638.err is your jobs stderr jupyter.log is your jupyter server log output Unless you are debugging something these shouldn't concern you.","title":"Step 6:"},{"location":"tutorials/t06_interactive_computing_with_jupyter_notebooks/#66-connect-to-your-jupyter-server","text":"We'll look at two different options on how you can use notebooks running on the now running jupyter server. - Browser - VSCode Before we do that though, we need to create the SSH tunnel we mentioned in the previous section. The connection info was stored in a text file in your home directory with the name connection_info.txt . To view its content you can use your VSCode editor if you're following from Tutorial 03: Setting up and using development tools , or simply use the cat command: cat ~/connection_info.txt The output will look something like: Important: The SSH command should be run on a fresh local terminal! Not already connected to cyclone The ssh command is blocking, meaning nothing will be printed when it's run. You may be prompted for your key's passphrase. ```","title":"6.6. Connect to your jupyter server"},{"location":"tutorials/t06_interactive_computing_with_jupyter_notebooks/#661-browser","text":"With the SSH tunnel running, our local machine now is now connected to the compute node via the port 11083 in the above example. To launch the jupyter notebook in a browser, just head to the link printed from connection_info.txt , in our case it's http://localhost:11083. You should reach a page asking for the password looking like this: Once we input the password from connection_info.txt , in our case s23un9qxYjpenFnE , and press the Log In button, we're in! Let's create a new notebook! Click the New button Now we can see serveral options: - Notebook kernels - Both kernels have the same python interpreter, the one in our conda environment. - Python 3 (ipykernel) - Default Python kernel - notebookEnv - Our custom kernel we added Terminal - Launches a terminal session on the compute node, you can use this for running htop or nvidia-smi to view hardware utilisation Console - Launches a python interactive shell New File - Create a new file, this might be a text file, a python script or whatever you want. New Folder - Create a new folder If you click on any of the python kernel options, in this case notebookEnv, a new tab in your browser will open with a notebook:","title":"6.6.1 Browser"},{"location":"tutorials/t06_interactive_computing_with_jupyter_notebooks/#662-vscode","text":"To view and run notebooks in VSCode we need to have some extentions installed. Searching jupyter in the extensions tab of VSCode should show you something like this: Click install on the one circled and wait for it to be installed. Once that's done, open a folder on your local machine: For this example we used this repo: Right click inside a folder: Make sure to add the .ipyng extention at the end! Now the notebook should be open in your VSCode window. You are now ready to connect this notebook to the jupyter server running on the compute node. We do this by selecting a remote server by pressing the Select Kernel button at the top right of your screen: Then you will see this in the top middle of your screen: Select \"Existing Jupyter Server..\" Add the link that's inside your connection_info.txt Add the password, again found inside the connection_info.txt And finally a display name for your connection, this can be anything you want: Select the appropriate kernel: That's it. Now your notebook is running remotely on the compute node! Adding a couple of cells and calling nvidia-smi shows us the 1 gpu running on gpu01:","title":"6.6.2. VSCode"},{"location":"tutorials/t06_interactive_computing_with_jupyter_notebooks/#67-troubleshooting-the-ssh-tunnel","text":"","title":"6.7. Troubleshooting the SSH tunnel:"},{"location":"tutorials/t06_interactive_computing_with_jupyter_notebooks/#671-port-conflicts","text":"Symptom 1: Error message \"Address already in use\" or unable to connect to the specified port Solutions: Check if the port is already in use: bash lsof -i :PORT_NUMBER # On your local machine Kill any existing SSH tunnels: bash pkill -f \"ssh -N -J\"","title":"6.7.1. Port Conflicts"},{"location":"tutorials/t06_interactive_computing_with_jupyter_notebooks/#672-authentication-issues","text":"Symptom 2: SSH key authentication failures Solutions: Verify your SSH key is properly added to Cyclone: bash ssh-add -l # List loaded keys ssh-add ~/.ssh/id_rsa # Add your key if needed Check key permissions: ```bash chmod 600 ~/.ssh/id_rsa chmod 700 ~/.ssh","title":"6.7.2. Authentication issues"},{"location":"tutorials/t06_interactive_computing_with_jupyter_notebooks/#673-general-debugging-tips","text":"Check the job output files for errors: bash cat job.[jobid].out cat job.[jobid].err > Replace [jobid] with the your Job ID These commands will print out the contents of the outputs of the job. They might contain some more information that will guide you to find the problem. Some examples: The conda environment name might be wrong. Package dependency issues inside your conda environment. The project you're requesting resources from might not have access to the partition you requested.","title":"6.7.3. General Debugging Tips"},{"location":"utils/Windows_SSH_Setup/","text":"SSH Manager Installation Guide for Windows 1. OpenSSH 1.1 Check if OpenSSH Client is installed: Open a Windows PowerShell as Administrator. Press Windows Key > Search \"Windows PowerShell\" > Right-click \"Run as Administrator\" Run the following command to see if the OpenSSH client is installed: powershell Get-WindowsCapability -Online | Where-Object Name -like 'OpenSSH.Client*' If installed, you should see something like: powershell Name: OpenSSH.Client~~~~0.0.1.0 State: Installed 1.2 Install OpenSSH Client (If not already Installed!) Open Settings Search for OpenSSH Client: Click on Add a feature and search for \"OpenSSH Client\" Select it and click Install . 1.3 Activate OpenSSH (Start Services) Open a Windows PowerShell as Administrator. Press Windows Key > Search \"Windows PowerShell\" > Right-click \"Run as Administrator\" Start the SSH Agent: powershell Start-Service ssh-agent 2. WSL Open a Windows PowerShell as Administrator. Press Windows Key > Search \"Windows PowerShell\" > Right-click \"Run as Administrator\" For Windows 10 (build 19041 and higher) and Windows 11, you can use the simplified command to install WSL: powershell wsl --install This will: - Enable the WSL feature. - Install the default Linux distribution (usually Ubuntu). - Install the necessary Virtual Machine Platform and Windows Subsystem for Linux components. 3. Restart Your Computer: If prompted, restart your computer to complete the installation. 3. Git Bash Install Git Bash: Install Git for Windows if you haven\u2019t already Download Git . Launch Git Bash Press Windows Key > Search \"Git Bash\"","title":"Windows SSH Setup"},{"location":"utils/Windows_SSH_Setup/#ssh-manager-installation-guide-for-windows","text":"","title":"SSH Manager Installation Guide for Windows"},{"location":"utils/Windows_SSH_Setup/#1-openssh","text":"","title":"1. OpenSSH"},{"location":"utils/Windows_SSH_Setup/#11-check-if-openssh-client-is-installed","text":"Open a Windows PowerShell as Administrator. Press Windows Key > Search \"Windows PowerShell\" > Right-click \"Run as Administrator\" Run the following command to see if the OpenSSH client is installed: powershell Get-WindowsCapability -Online | Where-Object Name -like 'OpenSSH.Client*' If installed, you should see something like: powershell Name: OpenSSH.Client~~~~0.0.1.0 State: Installed","title":"1.1 Check if OpenSSH Client is installed:"},{"location":"utils/Windows_SSH_Setup/#12-install-openssh-client-if-not-already-installed","text":"Open Settings Search for OpenSSH Client: Click on Add a feature and search for \"OpenSSH Client\" Select it and click Install .","title":"1.2 Install OpenSSH Client (If not already Installed!)"},{"location":"utils/Windows_SSH_Setup/#13-activate-openssh-start-services","text":"Open a Windows PowerShell as Administrator. Press Windows Key > Search \"Windows PowerShell\" > Right-click \"Run as Administrator\" Start the SSH Agent: powershell Start-Service ssh-agent","title":"1.3 Activate OpenSSH (Start Services)"},{"location":"utils/Windows_SSH_Setup/#2-wsl","text":"Open a Windows PowerShell as Administrator. Press Windows Key > Search \"Windows PowerShell\" > Right-click \"Run as Administrator\" For Windows 10 (build 19041 and higher) and Windows 11, you can use the simplified command to install WSL: powershell wsl --install This will: - Enable the WSL feature. - Install the default Linux distribution (usually Ubuntu). - Install the necessary Virtual Machine Platform and Windows Subsystem for Linux components. 3. Restart Your Computer: If prompted, restart your computer to complete the installation.","title":"2. WSL"},{"location":"utils/Windows_SSH_Setup/#3-git-bash","text":"Install Git Bash: Install Git for Windows if you haven\u2019t already Download Git . Launch Git Bash Press Windows Key > Search \"Git Bash\"","title":"3. Git Bash"}]}