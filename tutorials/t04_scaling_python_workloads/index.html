<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>Scaling Python Workloads - Cyclone Tutorials</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Scaling Python Workloads";
        var mkdocs_page_input_path = "tutorials/t04_scaling_python_workloads.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../..">
          <img src="../../images/cropped-NCC_Cyprus_Logo.png" class="logo" alt="Logo"/>
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../..">Home</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../t01_introduction_to_hpc_systems/">Introduction to HPC Systems</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../t02_accessing_and_navigating_cyclone/">Accessing and Navigating Cyclone</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../t03_setting_up_and_using_development_tools/">Setting up and Using Development tools</a>
                </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="#">Scaling Python Workloads</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#41-overview">4.1. Overview</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#42-introduction">4.2. Introduction</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#421-why-use-cyclone-for-python-workloads">4.2.1. Why Use Cyclone for Python Workloads?</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#2-performance-gains-utilizing-advanced-hardwaresoftware-such-as-gpus-high-speed-interconnects-and-slurm-python-workflows-can-be-executed-more-efficiently">2. Performance Gains: Utilizing advanced hardware/software such as GPUs, high-speed interconnects and SLURM, Python workflows can be executed more efficiently.</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#422-tools-and-frameworks">4.2.2. Tools and Frameworks</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#43-training-ai-models-on-cyclone">4.3. Training AI models on Cyclone</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#431-running-python-scripts-on-cpu">4.3.1. Running Python Scripts on CPU</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#432-running-python-scripts-on-single-gpu">4.3.2. Running Python Scripts on Single GPU</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#433-running-python-scripts-on-multi-gpu-single-node-using-ddp">4.3.3. Running Python Scripts on Multi-GPU (Single Node) using DDP</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#434-running-python-scripts-on-multi-gpu-multi-node-with-ddp">4.3.4. Running Python Scripts on Multi-GPU (Multi-Node) with DDP</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#44-recap-and-troubleshooting">4.4. Recap and Troubleshooting</a>
    </li>
    </ul>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../t05_compiling_and_running_code_with_slurm/">Compiling and Running C/C++ Code on Cyclone with SLURM</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../t06_interactive_computing_with_jupyter_notebooks/">Interactive Computing on Cyclone with Jupyter Notebooks</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../about/">About</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">Cyclone Tutorials</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Scaling Python Workloads</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <!--
 t04_scaling_python_workloads.md

 CaSToRC, The Cyprus Institute

 (c) 2024 The Cyprus Institute

 Contributing Authors:
 Spiros Millas (s.millas@cyi.ac.cy)
 Simone Bacchio (s.bacchio@cyi.ac.cy)
 Andreas Athenodorou (a.athenodorou@cyi.ac.cy)

 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
 You may obtain a copy of the License at

     https://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License.
-->

<h1 id="4-scaling-python-workloads">4. Scaling Python Workloads</h1>
<h2 id="41-overview">4.1. Overview</h2>
<p>This tutorial provides a comprehensive guide on scaling Python workloads using Cyclone, covering frameworks for deep learning and hardware acceleration. The tutorial begins by executing Python scripts on CPUs, gradually progressing to single GPU, multi GPU, and multi-node distributed training using PyTorch's Distributed Data Parallel framework. Each section includes modifications to SLURM scripts for resource allocation and Python code adaptations to leverage the targeted hardware configurations. The tutorial concludes with troubleshooting tips and a recap of practices mentioned.</p>
<h2 id="42-introduction">4.2. Introduction</h2>
<p>Cyclone offers substantial computational resources that can accelerate Python-based workflows, especially for AI and data-intensive applications. This tutorial offers an overview of how to leverage Cyclone for running Python and how to scale AI training on multiple GPUs. </p>
<hr />
<h3 id="421-why-use-cyclone-for-python-workloads">4.2.1. Why Use Cyclone for Python Workloads?</h3>
<ol>
<li><strong>Scalability</strong>: Using Cyclone, workloads can be scaled across multiple CPUs and GPUs, which enables faster processing of large datasets and complex computations</li>
</ol>
<h2 id="2-performance-gains-utilizing-advanced-hardwaresoftware-such-as-gpus-high-speed-interconnects-and-slurm-python-workflows-can-be-executed-more-efficiently">2. <strong>Performance Gains</strong>: Utilizing advanced hardware/software such as GPUs, high-speed interconnects and SLURM, Python workflows can be executed more efficiently.</h2>
<h3 id="422-tools-and-frameworks">4.2.2. Tools and Frameworks</h3>
<ul>
<li><strong>SLURM</strong> - Used for job scheduling, job monitoring and environment setup</li>
<li><strong>PyTorch</strong> - Open-source deep learning framework thatâ€™s known for its flexibility and ease-of-use.</li>
<li>
<p><strong>CUDA</strong> - Enables GPU acceleration for computational tasks through APIs to simplify GPU-based parallel processing for HPC, data science and AI.</p>
</li>
<li>
<p><strong>NCCL</strong> - Implements multi-GPU and multi-node communication that is optimized for NVIDIA GPUs and networking.</p>
</li>
</ul>
<h2 id="43-training-ai-models-on-cyclone">4.3. Training AI models on Cyclone</h2>
<p>This section demonstrates how to run Python scripts with multiple environment configurations on Cyclone, focusing on scaling Python workloads by leveraging Pytorch, CUDA and NVIDIA's NCCL backend.</p>
<hr />
<h3 id="431-running-python-scripts-on-cpu">4.3.1. Running Python Scripts on CPU</h3>
<hr />
<p>The script below demonstrates how to sructure a training pipeline for deep learning using pytorch on a CPU. Firstly, the necessary libraries that handle tasks such as data loading and model training are imported. The main workflow includes downloading the dataset, as well as defining and training a Convolutional Neural Network (CNN).</p>
<pre><code class="language-python">import os
import argparse
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
from model import CNN_classifier
import time

def train(model, dataloader: DataLoader, args):

    print(&quot;Entering training loop...&quot;)
    criterion = nn.NLLLoss()
    optimizer = optim.Adam(params=model.parameters(), lr = args.lr)
    model.train()
    for epoch in range(1, args.epochs + 1):
        epoch_loss: float =  0.0
        for batch_idx, (data,target) in enumerate(dataloader):    
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()
            if batch_idx % 100 == 0:
                    print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(dataloader.dataset)} '
                        f'({100. * batch_idx / len(dataloader):.0f}%)]\tLoss: {loss.item():.6f}')
    print(&quot;Exiting training loop...&quot;)            

def main():

    parser = argparse.ArgumentParser(prog=&quot;Pytorch on HPC&quot;)
    parser.add_argument(&quot;--batch_size&quot;, type=int, default=16)
    parser.add_argument(&quot;--epochs&quot;, type=int, default=5)
    parser.add_argument(&quot;--lr&quot;, type=float, default=0.001)

    args = parser.parse_args()

    transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
    ])

    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
    train_loader = DataLoader(
        train_dataset,
        batch_size = args.batch_size,
        shuffle=False,
    )
    model = CNN_classifier()

    time_start = time.time()
    train(model=model, dataloader= train_loader, args=args)
    time_stop = time.time()

    print(f&quot;Training time = {time_stop-time_start}&quot; )

if __name__ == &quot;__main__&quot; :
    main()
</code></pre>
<p>To execute the above script, navigate to 
<code>src/t04</code> and execute the <code>run_cpu.SLURM</code> file using  </p>
<pre><code class="language-bash">sbatch run_cpu.SLURM
</code></pre>
<pre><code class="language-bash">#!/bin/bash
#SBATCH --job-name=pytorch_cpu                  # Job name
#SBATCH --nodes=1                               # Number of nodes
#SBATCH --cpus-per-task=10                      # CPUs per task
#SBATCH --time=02:00:00                         # Maximum runtime (HH:MM:SS)
#SBATCH --partition=cpu                         # Partition name
#SBATCH --output=src/t04/logs/cpu_%j.out   # Standard output log
#SBATCH --error=src/t04/logs/cpu_%j.err    # Standard error log

module load PyTorch/1.12.0-foss-2022a-CUDA-11.7.0
module load torchvision/0.13.1-foss-2022a-CUDA-11.7.0

# Run Python script
srun python src/t04/cpu_example.py \
    --batch_size 16 \
    --epochs 5 \
    --lr 0.001
</code></pre>
<p>The <code>run_cpu.SLURM</code> script is designed to execute the <code>cpu_example.py</code> on Cyclone using SLURM. In the script, the number of nodes, CPU cores, runtime and partition are specified, which instruct SLURM to allocate 10 CPU cores from a single node for 2:00:00 hours for this job. </p>
<p>Next, the environment is setup using Cyclones available modules for Pytorch and Torchvision ensuring all necessary libraries and dependencies are available. </p>
<p>Finally, the Python script is launched using <code>srun</code>, which executes the specified script with the allocated resources and given runtime arguments.</p>
<hr />
<p>After executing the above script, two seperate output logs will be generated in the <code>logs/</code> directory. </p>
<pre><code>cpu_&lt;jobid&gt;.out
cpu_&lt;jobid&gt;.err
</code></pre>
<p>Navigate to the <code>logs/</code> directory with the terminal interface using <code>cd logs/</code> or by using VScode's file explorer.</p>
<p>Next, view the contents of the  <code>cpu_&lt;jobid&gt;.out</code>. To use the terminal, first execute <code>module load nano</code> to load a Linux text editor and enter the command <code>nano cpu_&lt;jobid&gt;.out</code>. If navigating using VScode, simply double click the output file. </p>
<pre><code class="language-python">Entering training loop...
Train Epoch: 1 [0/60000 (0%)]   Loss: 2.300653
Train Epoch: 1 [1600/60000 (3%)]    Loss: 0.546140
.
.
Train Epoch: 5 [57600/60000 (96%)]  Loss: 0.000070
Train Epoch: 5 [59200/60000 (99%)]  Loss: 0.000981
Exiting training loop...
Training time = 192.2544162273407
</code></pre>
<p>Training this simple CNN classifier for 5 epochs on the relatively small MNIST dataset took a total of 192 seconds. This process can be made significantly more efficient by utilizing Cyclones GPU cores, rather than the CPU cores.</p>
<hr />
<h3 id="432-running-python-scripts-on-single-gpu">4.3.2. Running Python Scripts on Single GPU</h3>
<hr />
<p>To train the AI model using GPUs on Cyclone, some basic modifications must be made on both the Python and SLURM scripts. Beginning with the <code>gpu_example.py</code> Python script, in the main function, the following code block is added.    </p>
<pre><code class="language-python"> if torch.cuda.is_available():
        print(&quot;Utilizing GPU&quot;)
        device = torch.device(&quot;cuda&quot;)

    else: 
        print(&quot;Utilizing CPU&quot;)
        device = torch.device('cpu')
</code></pre>
<p>The above code initializes the device variable as the GPU, by first checking if there is one available. If not, the device defaults to CPU. </p>
<p>The next changes to the script must be made before the model begins training using the code below </p>
<pre><code class="language-python">model.to(device)
</code></pre>
<p>and during training using the following </p>
<pre><code class="language-python">data = data.to(device)
target = target.to(device)
</code></pre>
<p>It is important to have both the data and the model on the same device (CPU or GPU), otherwise a runtime error will occur. PyTorch operations require the tensors involved to be on the same device.</p>
<p>The <code>run_gpu.SLURM</code> script is designed to execute the <code>gpu_example.py</code> on a single GPU. This configuration is specified by instructing SLURM to allocate a GPU on a single node using the following SLURM directives. When launching gpu jobs on Cyclone, it is important to specify the correct <code>--partition</code> as SLURM defaults to CPU, which will cause an error.</p>
<pre><code class="language-bash">#SBATCH --nodes=1                               # Number of nodes
#SBATCH --ntasks-per-node=1                     # Tasks per node (GPUs per node)
#SBATCH --gpus-per-node=1                       # GPUs per node
#SBATCH --partition=gpu                         # Partition name
</code></pre>
<p>Next, some additional GPU-related modules must be loaded from Cyclone's library. The cuDNN and CUDA modules provide the tools and drivers required to enable GPU optimized deep learning operations.</p>
<pre><code class="language-bash">module load PyTorch/1.12.0-foss-2022a-CUDA-11.7.0
module load cuDNN/8.4.1.50-CUDA-11.7.0
module load torchvision/0.13.1-foss-2022a-CUDA-11.7.0
module load CUDA/11.7.0
</code></pre>
<p>When loading modules from Cyclone's library, it is important to load compatible versions of these modules to ensure smooth interaction between the hardware, CUDA, and the deep learning framework (e.g., PyTorch), avoiding errors or performance issues.</p>
<p>To run this SLURM script, navigate to the script directory and submit the job using <code>sbatch run_gpu.SLURM</code>. After the job finishes, navigate to the <code>logs/</code> directory and open the <code>gpu_&lt;job_id&gt;.out</code> file and observe the difference.</p>
<pre><code class="language-python">Utilizing GPU
Entering training loop...
Train Epoch: 1 [0/60000 (0%)]   Loss: 2.332518
Train Epoch: 1 [1600/60000 (3%)]    Loss: 0.376552
Train Epoch: 1 [3200/60000 (5%)]    Loss: 0.127207
.
.
.
Train Epoch: 5 [59200/60000 (99%)]  Loss: 0.082046
Exiting training loop...
Training time = 73.48071932792664
</code></pre>
<p>Already there is a substantial decrease in training time by utilizing a GPU over a CPU. However, Cyclone offers much more GPU resources per node, meaning that there are further gains to training efficiency left on the table by utilizing only a single GPU.</p>
<hr />
<h3 id="433-running-python-scripts-on-multi-gpu-single-node-using-ddp">4.3.3. Running Python Scripts on Multi-GPU (Single Node) using DDP</h3>
<p>To leverage multiple GPUs per node on Cyclone, workloads must be scaled using parallelization techniques. While there are many times of parallelism, Data parallelism will be used to scale model training in this tutorial. </p>
<p><img alt="Data Parallelism diagram" src="../images/DD.png" /></p>
<p>When optimizing AI training using data parallelism, a copy model is loaded on all GPUs available, and the dataset is split amongst them. Each GPU processes a different subset of the data in parallel. During the forward pass, each GPU processes a different batch of the data and the gradients are communicated between the devices so as to ensure the model parameters are appropriately updated during backpropagation. To implement this efficiently, PyTorch provides the Distributed Data Parallel (DDP) module, which automates the process of distributing data, synchronizing gradients, and ensuring consistent parameter updates across GPUs. DDP leverages NCCL (NVIDIA Collective Communications Library) as its backend to optimize GPU communication, enabling seamless gradient sharing and synchronization with minimal overhead.</p>
<p><img alt="Ranks diagram" src="../images/singlenoderanks.png" /></p>
<p>To train our AI model using DDP, some changes must be made to the Python and SLURM scripts. Firstly, the concepts of <strong>Ranks</strong>, <strong>Processes</strong> and the <strong>World</strong> are introduced to the workflow. A rank is the unique id given to a process, and is used for communication purposes. One GPU corresponds to one process. The World is a group that contains all the processes, thus the size of the World is equal to the number of GPUs. </p>
<p>Firstly, in the <code>run_multigpu.SLURM</code> script, changes are being made to the directives to instruct SLURM to allocate more GPUs per node. For this section of the tutorial, 2 GPUs on a single node are utilized.   </p>
<pre><code class="language-bash">#SBATCH --nodes=1                                       # Number of nodes
#SBATCH --ntasks-per-node=2                             # Tasks per node (GPUs per node)
#SBATCH --gpus-per-node=2                               # GPUs per node
</code></pre>
<p>Further changes to the SLURM script include lines to retrieve environment variables set by the SLURM scheduler to define the nodes' address and a random port which are used to establish communication between processes during training. This communication will be done using the NCCL backend, which must be also loaded. World size can be directly calculated in the SLURM script using the environment variables, which as stated before are the total number of GPUs available. Finally to the srun command, add <code>--export=ALL</code> to ensure the environment variables are passed to the <code>srun</code> job</p>
<pre><code class="language-bash">module load CUDA/11.7.0
module load NCCL/2.12.12-GCCcore-11.3.0-CUDA-11.7.0
module load PyTorch/1.12.0-foss-2022a-CUDA-11.7.0
module load cuDNN/8.4.1.50-CUDA-11.7.0
module load torchvision/0.13.1-foss-2022a-CUDA-11.7.0

export MASTER_ADDR=$(scontrol show hostname $SLURM_NODELIST | head -n 1)
export MASTER_PORT=$(shuf -i 29500-65535 -n 1)
export WORLD_SIZE=$(($SLURM_NNODES * $SLURM_NTASKS_PER_NODE)) 

srun --export=ALL python HPC_tutorial/multigpu_example.py \
    --batch_size 16 \
    --epochs 5 \
    --lr 0.001 \
</code></pre>
<p>Moving on to the Python script, the environment variables are exported from the SLURM scheduler to be used by DDP, and are placed as global variables at the start of the script, after the import statements.</p>
<pre><code class="language-python">import os
import argparse
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, DistributedSampler
from model import CNN_classifier
import time

MASTER_ADDR = os.environ[&quot;MASTER_ADDR&quot;]
MASTER_PORT = os.environ[&quot;MASTER_PORT&quot;]
WORLD_SIZE = int(os.environ[&quot;WORLD_SIZE&quot;])
</code></pre>
<p>The next adjustment to the script happens to the main function (which has been renamed to the worker function). Since SLURM will launch as many processes as there are GPUs, the rank of the process can be defined as the process ID. Next, the process group must be initialized with some key parameters. <code>nccl</code> is chosen as the backend, the <code>world_size</code> and <code>rank</code> parameters are added, and lastly specify the <code>init_method = 'env://'</code> to indicate the <code>MASTER_ADDR</code> and <code>MASTER_PORT</code> environment variables should be used to configure the communication. Finally, the current process is assigned to its corresponding GPU based on its <code>rank</code>. The device object is created with the following syntax <code>cuda:&lt;rank&gt;</code>, which is used send the model and data to the approprate GPU. </p>
<pre><code class="language-python">def worker(args): 

    rank = int(os.environ[&quot;SLURM_PROCID&quot;])    

    torch.distributed.init_process_group(
        backend='nccl',
        world_size=WORLD_SIZE,
        rank=rank,
        init_method='env://'

    torch.cuda.set_device(rank)
    device = torch.device(f&quot;cuda:{rank}&quot;)

    )
</code></pre>
<p>When implementing data parallelism, it is unnecessary to download the entire dataset on all devices. One device can download the dataset and share it with the rest of the GPUs. To do this, the dataset download command is changed to only execute on <code>rank 0</code>. To ensure device synchronization, <code>torch.distributed.barrier()</code> is called, which instructs the GPUs to wait until all other devices reach that same point in the script before continuing. Next the dataset is loaded on all other GPUs</p>
<pre><code class="language-python">    if rank == 0:        
        train_dataset = datasets.MNIST('./data', 
                                       train=True, 
                                       download=True, 
                                       transform=transform)
    torch.distributed.barrier()

    train_dataset = datasets.MNIST('./data', 
                                   train=True, 
                                   download=False, 
                                   transform=transform)
</code></pre>
<p>Next, a DistributedSampler object is defined, which ensures that workload is distributed across all GPUs that are apart of the world. To ensure the dataset is split into manageable batches, the sampler is combined with the <code>Dataloader</code> object.</p>
<pre><code class="language-python">    train_sampler = DistributedSampler(
        train_dataset,
        num_replicas=WORLD_SIZE,
        rank=rank
    )

    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        drop_last=True,
        sampler=train_sampler
    )
</code></pre>
<p>The model is then wrapped with <code>DistributedDataParallel</code>, which will handle the multi-GPU training, ensuring the gradients will be synchronized across all processes after the forward pass. Next, the <code>device_ids = [rank]</code> is specified to define the GPU on which the model will run for the current process.</p>
<pre><code class="language-python">    model = CNN_classifier().to(device)
    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[rank])
</code></pre>
<p>Stepping into the <code>train()</code> function,  <code>torch.distributed.barrier()</code> is called at the end of the epoch to ensure synchronization during training and to avoid runtime errors. Optionally, rank parameter is added to the function call to avoid duplicate printing in the output call by specifying one GPU to be the logging device. </p>
<pre><code class="language-python">def train(model, dataloader: DataLoader, args, device,rank):
    criterion = nn.NLLLoss()
    optimizer = optim.Adam(params=model.parameters(), lr = args.lr)
    model.train()
    if rank == 0:    
        print(&quot;Entering training loop...&quot;)
    for epoch in range(1, args.epochs + 1):
        epoch_loss: float =  0.0
        for batch_idx, (data,target) in enumerate(dataloader):
            data = data.to(device)
            target = target.to(device)
            output = model(data)
            loss = criterion(output, target)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item() #monitoring
            if rank == 0 and batch_idx % 100 == 0:
                    print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(dataloader.dataset)} '
                        f'({100. * batch_idx / len(dataloader):.0f}%)]\tLoss: {loss.item():.6f}')

        torch.distributed.barrier()
    if rank == 0:    
        print(&quot;Exiting training loop...&quot;)            

</code></pre>
<p>Last and most importantly, at the end of the worker function, <code>torch.distributed.destroy_process_group()</code> is called. This ensures all resources tied to distributed training are properly released.</p>
<pre><code class="language-python">    if rank == 0:
        print(f&quot;Training time = {time_stop - time_start}&quot;)

    torch.distributed.destroy_process_group()

if __name__ == &quot;__main__&quot; :
   worker(args=args)
</code></pre>
<p>To execute this script, navigate to the <code>scr/t04/</code> directory and submit a job using <code>sbatch scr/t04/run_multigpu.SLURM</code>. After the job completes, navigate to the <code>logs/</code> directory, open the <code>multigpu_&lt;job_id&gt;.out</code> and observe the changes.</p>
<pre><code class="language-python">MASTER_ADDR: gpu06
MASTER_PORT: 31315
WORLD_SIZE: 2
Entering training loop...
Train Epoch: 1 [0/60000 (0%)]   Loss: 2.319880
.
.
.
Train Epoch: 5 [27200/60000 (91%)]  Loss: 0.000118
Train Epoch: 5 [28800/60000 (96%)]  Loss: 0.000066
Exiting training loop...
Training time = 42.064579248428345
</code></pre>
<p>By utilizing multiple GPUs, we achieve a much faster training time. Cyclone offers 4 GPUs per node. But multiple nodes can be used to further speed up training with minimal changes to the code base.</p>
<hr />
<h3 id="434-running-python-scripts-on-multi-gpu-multi-node-with-ddp">4.3.4. Running Python Scripts on Multi-GPU (Multi-Node) with DDP</h3>
<hr />
<p>Using the same parallelization technique and some simple changes to the SLURM and Python scripts, more compute resources can be leveraged to further speed up the AI models' training by utilizing multiple nodes.</p>
<p>To do this, some changes must first be made to the SLURM script. These changes instruct the SLURM scheduler to allocate two GPUs on two nodes, for a total of four GPUs.</p>
<pre><code class="language-python">#SBATCH --nodes=2                                       # Number of nodes
#SBATCH --ntasks-per-node=2                             # Tasks per node (GPUs per node)
#SBATCH --gpus-per-node=2   
</code></pre>
<p>Next change is the addition of the <code>NODE_RANK</code> environment variable which will equal the unique identifier of the current node in the distributed setup
.</p>
<pre><code class="language-python">export MASTER_ADDR=$(scontrol show hostname $SLURM_NODELIST | head -n 1)
export MASTER_PORT=$(shuf -i 29500-65535 -n 1)

export WORLD_SIZE=$(($SLURM_NNODES * $SLURM_NTASKS_PER_NODE))
export NODE_RANK=$SLURM_NODEID
</code></pre>
<p>In the python script, the first change that must be made is the addition of the <code>NODE_RANK</code> global variable, which will be used in the worker function for logging purposes. </p>
<pre><code class="language-python">MASTER_ADDR = os.environ[&quot;MASTER_ADDR&quot;]
MASTER_PORT = os.environ[&quot;MASTER_PORT&quot;]
WORLD_SIZE = int(os.environ[&quot;WORLD_SIZE&quot;])
NODE_RANK = int(os.environ[&quot;NODE_RANK&quot;])
</code></pre>
<p>In the worker function, the concepts of <code>local_rank</code> and <code>global_rank</code> are introduced and defined using the SLURM scheduler environment variables.</p>
<pre><code class="language-python">global_rank = int(os.environ[&quot;SLURM_PROCID&quot;])    
local_rank = int(os.environ[&quot;SLURM_LOCALID&quot;])
</code></pre>
<p>The  <code>global_rank</code> is the unique identity assigned to each process (GPU) as apart of the general <code>world</code> and is used for inter-process communication and coordination across the entire cluster. The <code>local_rank</code> is the unique identifier assigned to a process as a part of a node and is used to assign and manage GPU usage within a specific node in DDP.</p>
<p><img alt="Ranks diagram" src="../images/multinode.png" /></p>
<p>Moving on, a few changes must be made to the rank assignment on the various function calls. In the <code>torch.distributed.init_process_group()</code> the <code>global_rank</code> is used to uniquely identify each process in the distributed training across all nodes. It ensures proper coordination and communication in the entire distributed world.</p>
<pre><code class="language-python">torch.distributed.init_process_group(
    backend='nccl', 
    world_size=WORLD_SIZE, 
    rank=global_rank,
    init_method='env://'
)
</code></pre>
<p>Next, when setting the device, the <code>local_rank</code> is used to specify which GPU on the current node the process will use. Each process must operate on a separate GPU within the same node.</p>
<pre><code class="language-python">torch.cuda.set_device(local_rank)
device = torch.device(f&quot;cuda:{local_rank}&quot;)
</code></pre>
<p>Afterwards, When downloading the dataset, <code>local_rank</code> is used to share the dataset with all other GPUs on the same node</p>
<pre><code class="language-python"> if local_rank == 0:
        train_dataset = datasets.MNIST('./data',
                       train=True, 
                       download=True, 
                       transform=transform)
</code></pre>
<p>furthermore, When wrapping the model with DDP, The local_rank is used to bind the DDP instance to the specific GPU that the process is operating on, ensuring the process handles only its assigned device.</p>
<pre><code class="language-python">model = nn.parallel.DistributedDataParallel(model, device_ids=[local_rank])
</code></pre>
<p>Lastly, when logging during training, the <code>global_rank</code> is used to avoid redundant log statements in the output.</p>
<pre><code class="language-python">train(model=model, dataloader=train_loader, args=args, device=device, rank=global_rank)
</code></pre>
<p>To run this script and leverage multiple nodes on Cyclone, navigate to the <code>src/t04/</code> directory and launch the <code>run_multinode.SLURM</code> script. Wait until the job finishes, navigate to the <code>logs/</code> directory, open the <code>multinode_&lt;job_id&gt;.out</code> file and observe the results.</p>
<pre><code class="language-python">MASTER_ADDR: gpu06
MASTER_PORT: 57049
WORLD_SIZE: 4
NODE_RANK: 0
Rank 1 on Node 0: Initializing process group.
Rank 3 on Node 1: Initializing process group.
Rank 2 on Node 1: Initializing process group.
Rank 0 on Node 0: Initializing process group.
Entering training loop...
.
.
.
Train Epoch: 5 [12800/60000 (85%)]  Loss: 0.000021
Train Epoch: 5 [14400/60000 (96%)]  Loss: 0.000576
Exiting training loop...
Training time = 25.612423181533813
</code></pre>
<h2 id="44-recap-and-troubleshooting">4.4. Recap and Troubleshooting</h2>
<p>In this tutorial, we covered the process of scaling Python workloads on Cyclone, focusing on the following key concepts:</p>
<p>Resource Allocation with SLURM:
- Setting up SLURM directives for various configurations, from single CPU to multi-node GPU training.
 - Using environment variables such as MASTER_ADDR, MASTER_PORT, and WORLD_SIZE for distributed computing.</p>
<p>Python Script Modifications:</p>
<ul>
<li>Adapting scripts for different hardware configurations (CPU, single GPU, multi-GPU, and multi-node setups).</li>
<li>Leveraging DistributedDataParallel to automate data parallelism across GPUs.</li>
</ul>
<p>While Using Cyclone for distributed training and complex computations, some issues may be encountered with regards to job submission or <code>RuntimeErrors</code></p>
<p>SLURM job fails to launch:</p>
<hr />
<ul>
<li><strong>Problem</strong>: SLURM job fails with an error indicating incorrect directives.<br />
<strong>Solution</strong>: Double-check SLURM script parameters (<code>--nodes</code>, <code>--gpus-per-node</code>, <code>--partition</code>). Ensure they match the resources available in the partition.</li>
</ul>
<p>Runtime errors</p>
<hr />
<ul>
<li>
<p><strong>Problem</strong>: Tensor or model mismatch errors during training.<br />
<strong>Solution</strong>: Ensure both the model and data tensors are moved to the correct device using <code>model.to(device)</code> and <code>data.to(device)</code>.</p>
</li>
<li>
<p><strong>Problem</strong>: Processes fail to communicate due to incorrect master address or port.<br />
<strong>Solution</strong>: Verify that <code>MASTER_ADDR</code> and <code>MASTER_PORT</code> are correctly set in the SLURM script and ensure network connectivity between nodes.</p>
</li>
<li>
<p><strong>Problem</strong>: Training does not scale as expected.<br />
<strong>Solution</strong>: Ensure efficient resource utilization by setting appropriate batch sizes and verifying GPU utilization using monitoring tools (e.g., <code>nvidia-smi</code>).</p>
</li>
</ul>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../t03_setting_up_and_using_development_tools/" class="btn btn-neutral float-left" title="Setting up and Using Development tools"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../t05_compiling_and_running_code_with_slurm/" class="btn btn-neutral float-right" title="Compiling and Running C/C++ Code on Cyclone with SLURM">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../t03_setting_up_and_using_development_tools/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../t05_compiling_and_running_code_with_slurm/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../..";</script>
    <script src="../../js/theme_extra.js"></script>
    <script src="../../js/theme.js"></script>
      <script src="../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
